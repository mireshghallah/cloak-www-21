{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 \n",
    "import itertools\n",
    "\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from vgg16_2 import VGG_16_2\n",
    "\n",
    "\n",
    "import argparse\n",
    "from data_loader_withid_single_label import get_loader\n",
    "from data_loader_withid_single_label import get_dataset\n",
    "\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import time\n",
    "import shutil\n",
    "import os\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeba_image_dir = '/home/name/celeba/stargan/data/celeba/images'\n",
    "attr_path = '/home/name/celeba/stargan/data/celeba/list_attr_celeba.txt'\n",
    "id_path = '/home/name/celeba/stargan/data/celeba/identity_CelebA.txt'\n",
    "selected_attrs = ['Black_Hair', 'Male']\n",
    "celeba_crop_size = 224\n",
    "image_size = 224 \n",
    "batch_size = 40\n",
    "num_workers = 0\n",
    "mode1 = 'train' #test or train\n",
    "mode2 = 'test'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preprocessing the CelebA dataset...\n",
      "Finished preprocessing the CelebA dataset...\n"
     ]
    }
   ],
   "source": [
    "celeba_loader_train, data_train = get_loader(celeba_image_dir, attr_path, id_path, selected_attrs,celeba_crop_size, image_size, batch_size,'CelebA', mode1, num_workers)\n",
    "celeba_loader_test, data_test = get_loader(celeba_image_dir, attr_path, id_path, selected_attrs,celeba_crop_size, image_size, batch_size,'CelebA', mode2, num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182600\n",
      "19999\n"
     ]
    }
   ],
   "source": [
    "print(len(data_train))\n",
    "print(len(data_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_single_label(model):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    size =0\n",
    "    for im, labels in celeba_loader_test:\n",
    "        im, labels = im.to(device), labels.to(device)\n",
    "        labels = labels.long().squeeze(1)\n",
    "        preds, _ = model(im)\n",
    "        values, indices = preds.max(-1)\n",
    "        correct += indices.eq(labels).sum()\n",
    "\n",
    "        \n",
    "        del im\n",
    "        del preds\n",
    "        \n",
    "        del values\n",
    "        \n",
    "        print(correct)\n",
    "        size += len(labels)\n",
    "        del labels\n",
    "        print(float(correct)/float(size))\n",
    "        \n",
    "    print(size)\n",
    "    print(correct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_single_label_stoch(model):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    size =0\n",
    "    for im, labels in celeba_loader_test2:\n",
    "        im, labels = im.to(device), labels.to(device)\n",
    "        labels = labels.long().squeeze(1)\n",
    "        preds, _ = model(im)\n",
    "        values, indices = preds.max(-1)\n",
    "        correct += indices.eq(labels).sum()\n",
    "\n",
    "        \n",
    "        del im\n",
    "        del preds\n",
    "        \n",
    "        del values\n",
    "        \n",
    "        #print(correct)\n",
    "        size += len(labels)\n",
    "        del labels\n",
    "        print(float(correct)/float(size))\n",
    "        \n",
    "    print(size)\n",
    "    print(correct)\n",
    "    return size,correct\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demographic_parity(model):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    smiling_gen0 = 0\n",
    "    smiling_gen1=0\n",
    "    gen0 = 0\n",
    "    gen1= 0 \n",
    "    correct = 0\n",
    "    size = 0\n",
    "    for im, labels in celeba_loader_test:\n",
    "        im, labels = im.to(device), labels.to(device)\n",
    "        #print(labels.shape)\n",
    "        \n",
    "        labels_gender= labels[:,1]#.long().squeeze(1)\n",
    "        labels = labels[:,0] #.long().squeeze(1)\n",
    "        preds = model(im) # _\n",
    "        values, indices = preds.max(-1)\n",
    "        #print(labels)\n",
    "        #print(indices)\n",
    "        correct += indices.eq(labels).sum()\n",
    "\n",
    "        smiling_gen1 += (indices*labels_gender).sum()\n",
    "        smiling_gen0 += (indices*(1-labels_gender)).sum()\n",
    "        gen1 += labels_gender.sum()\n",
    "        gen0 += (len(labels_gender) - labels_gender.sum())\n",
    "        del im\n",
    "        del preds\n",
    "        \n",
    "        del values\n",
    "        \n",
    "        #print(correct)\n",
    "        size += len(labels)\n",
    "        del labels\n",
    "\n",
    "        \n",
    "    print(smiling_gen0/gen0, smiling_gen1/gen1)\n",
    "    print(smiling_gen0/gen0 - smiling_gen1/gen1)\n",
    "\n",
    "    print(float(correct)/float(size))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demographic_parity_mask(model, mask):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    smiling_gen0 = 0\n",
    "    smiling_gen1=0\n",
    "    gen0 = 0\n",
    "    gen1= 0 \n",
    "    correct = 0\n",
    "    size = 0\n",
    "    mask = torch.ones(model.intermed.scales().shape)\n",
    "    for im, labels in celeba_loader_test:\n",
    "        im, labels = im.to(device), labels.to(device)\n",
    "        #print(labels.shape)\n",
    "        im = im*mask\n",
    "        labels_gender= labels[:,1]#.long().squeeze(1)\n",
    "        labels = labels[:,0] #.long().squeeze(1)\n",
    "        preds = model(im) # _\n",
    "        values, indices = preds.max(-1)\n",
    "        #print(labels)\n",
    "        #print(indices)\n",
    "        correct += indices.eq(labels).sum()\n",
    "\n",
    "        smiling_gen1 += (indices*labels_gender).sum()\n",
    "        smiling_gen0 += (indices*(1-labels_gender)).sum()\n",
    "        gen1 += labels_gender.sum()\n",
    "        gen0 += (len(labels_gender) - labels_gender.sum())\n",
    "        del im\n",
    "        del preds\n",
    "        \n",
    "        del values\n",
    "        \n",
    "        #print(correct)\n",
    "        size += len(labels)\n",
    "        del labels\n",
    "        #print(float(correct)/float(size))\n",
    "        \n",
    "    print(smiling_gen0/gen0, smiling_gen1/gen1)\n",
    "    print(smiling_gen0/gen0 - smiling_gen1/gen1)\n",
    "\n",
    "    print(float(correct)/float(size))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal_odds(model):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    smiling_gen0 = 0\n",
    "    smiling_gen1=0\n",
    "    gen0 = 0\n",
    "    gen1= 0 \n",
    "    \n",
    "    smiling_gen0_not = 0\n",
    "    smiling_gen1_not =0\n",
    "    gen0_not = 0\n",
    "    gen1_not= 0 \n",
    "    \n",
    "    correct = 0\n",
    "    size = 0\n",
    "    for im, labels in celeba_loader_test:\n",
    "        im, labels = im.to(device), labels.to(device)\n",
    "        #print(labels.shape)\n",
    "        \n",
    "        labels_gender= labels[:,1]#.long().squeeze(1)\n",
    "        labels = labels[:,0] #.long().squeeze(1)\n",
    "        preds = model(im) # _\n",
    "        values, indices = preds.max(-1)\n",
    "        #print(labels)\n",
    "        #print(indices)\n",
    "        correct += indices.eq(labels).sum()\n",
    "        smiling_gen1 += ((indices*labels)*labels_gender).sum()\n",
    "        smiling_gen0 += ((indices*labels)*(1-labels_gender)).sum()\n",
    "        gen1 += (labels_gender*labels).sum()\n",
    "        gen0 += ((1 - labels_gender)*labels).sum()\n",
    "        ##########\n",
    "        smiling_gen1_not += ((indices*(1-labels))*labels_gender).sum()\n",
    "        smiling_gen0_not += ((indices*(1-labels))*(1-labels_gender)).sum()\n",
    "        \n",
    "        gen1_not += (labels_gender*(1-labels)).sum()\n",
    "        gen0_not += ((1 - labels_gender)*(1-labels)).sum()\n",
    "        \n",
    "        del im\n",
    "        del preds\n",
    "        \n",
    "        del values\n",
    "        \n",
    "        #print(correct)\n",
    "        size += len(labels)\n",
    "        del labels\n",
    "        #print(float(correct)/float(size))\n",
    "        \n",
    "    print(gen0, gen1, smiling_gen0, smiling_gen1)\n",
    "    print(smiling_gen0/gen0, smiling_gen1/gen1)\n",
    "    print((smiling_gen0/gen0 - smiling_gen1/gen1))\n",
    "    print(\"************\")\n",
    "    print(smiling_gen0_not/gen0_not, smiling_gen1_not/gen1_not)\n",
    "    print((smiling_gen0_not/gen0_not - smiling_gen1_not/gen1_not))    \n",
    "    \n",
    "    print(float(correct)/float(size))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal_odds_mask(model, mask):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    smiling_gen0 = 0\n",
    "    smiling_gen1=0\n",
    "    gen0 = 0\n",
    "    gen1= 0 \n",
    "    \n",
    "    smiling_gen0_not = 0\n",
    "    smiling_gen1_not =0\n",
    "    gen0_not = 0\n",
    "    gen1_not= 0 \n",
    "    \n",
    "    correct = 0\n",
    "    size = 0\n",
    "    mask = torch.ones(model.intermed.scales().shape)\n",
    "    for im, labels in celeba_loader_test:\n",
    "        im, labels = im.to(device), labels.to(device)\n",
    "        #print(labels.shape)\n",
    "        im = im*mask\n",
    "        labels_gender= labels[:,1]#.long().squeeze(1)\n",
    "        labels = labels[:,0] #.long().squeeze(1)\n",
    "        preds = model(im) # _\n",
    "        values, indices = preds.max(-1)\n",
    "        #print(labels)\n",
    "        #print(indices)\n",
    "        correct += indices.eq(labels).sum()\n",
    "        smiling_gen1 += ((indices*labels)*labels_gender).sum()\n",
    "        smiling_gen0 += ((indices*labels)*(1-labels_gender)).sum()\n",
    "        gen1 += (labels_gender*labels).sum()\n",
    "        gen0 += ((1 - labels_gender)*labels).sum()\n",
    "        ##########\n",
    "        smiling_gen1_not += ((indices*(1-labels))*labels_gender).sum()\n",
    "        smiling_gen0_not += ((indices*(1-labels))*(1-labels_gender)).sum()\n",
    "        \n",
    "        gen1_not += (labels_gender*(1-labels)).sum()\n",
    "        gen0_not += ((1 - labels_gender)*(1-labels)).sum()\n",
    "        \n",
    "        del im\n",
    "        del preds\n",
    "        \n",
    "        del values\n",
    "        \n",
    "        #print(correct)\n",
    "        size += len(labels)\n",
    "        del labels\n",
    "        #print(float(correct)/float(size))\n",
    "        \n",
    "    print(gen0, gen1, smiling_gen0, smiling_gen1)\n",
    "    print(smiling_gen0/gen0, smiling_gen1/gen1)\n",
    "    print((smiling_gen0/gen0 - smiling_gen1/gen1))\n",
    "    print(\"************\")\n",
    "    print(smiling_gen0_not/gen0_not, smiling_gen1_not/gen1_not)\n",
    "    print((smiling_gen0_not/gen0_not - smiling_gen1_not/gen1_not))    \n",
    "    \n",
    "    print(float(correct)/float(size))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal_odds_syn(model, mask):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    smiling_gen0 = 0\n",
    "    smiling_gen1=0\n",
    "    gen0 = 0\n",
    "    gen1= 0 \n",
    "    \n",
    "    smiling_gen0_not = 0\n",
    "    smiling_gen1_not =0\n",
    "    gen0_not = 0\n",
    "    gen1_not= 0 \n",
    "    \n",
    "    \n",
    "    correct = 0\n",
    "    size = 0\n",
    "    #mask = torch.ones(model.intermed.scales().shape)\n",
    "    for im, labels in celeba_loader_test:\n",
    "        im, labels = im.to(device), labels.to(device)\n",
    "        #print(labels.shape)\n",
    "        \n",
    "        labels_gender= labels[:,1]#.long().squeeze(1)\n",
    "        labels = labels[:,0] #.long().squeeze(1)\n",
    "        preds, _= model(im, mask) # _\n",
    "        values, indices = preds.max(-1)\n",
    "        #print(labels)\n",
    "        #print(indices)\n",
    "        correct += indices.eq(labels).sum()\n",
    "        smiling_gen1 += ((indices*labels)*labels_gender).sum()\n",
    "        smiling_gen0 += ((indices*labels)*(1-labels_gender)).sum()\n",
    "        gen1 += (labels_gender*labels).sum()\n",
    "        gen0 += ((1 - labels_gender)*labels).sum()\n",
    "        ##########\n",
    "        smiling_gen1_not += (((1-indices)*(1-labels))*labels_gender).sum()\n",
    "        smiling_gen0_not += (((1-indices)*(1-labels))*(1-labels_gender)).sum()\n",
    "        \n",
    "        gen1_not += (labels_gender*(1-labels)).sum()\n",
    "        gen0_not += ((1 - labels_gender)*(1-labels)).sum()\n",
    "        \n",
    "        del im\n",
    "        del preds\n",
    "        \n",
    "        del values\n",
    "        \n",
    "        #print(correct)\n",
    "        size += len(labels)\n",
    "        del labels\n",
    "        #print(float(correct)/float(size))\n",
    "        \n",
    "    print(gen0, gen1, smiling_gen0, smiling_gen1)\n",
    "    print(smiling_gen0/gen0, smiling_gen1/gen1)\n",
    "    print((smiling_gen0/gen0 - smiling_gen1/gen1))\n",
    "    print(\"************\")\n",
    "    print(smiling_gen0_not/gen0_not, smiling_gen1_not/gen1_not)\n",
    "    print((smiling_gen0_not/gen0_not - smiling_gen1_not/gen1_not))    \n",
    "    \n",
    "    print(float(correct)/float(size))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demographic_parity_syn(model, mask):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    smiling_gen0 = 0\n",
    "    smiling_gen1=0\n",
    "    smiling_gen0_not = 0\n",
    "    smiling_gen1_not=0\n",
    "    gen0 = 0\n",
    "    gen1= 0 \n",
    "    correct = 0\n",
    "    size = 0\n",
    "    #mask = torch.ones(model.intermed.scales().shape)\n",
    "    for im, labels in celeba_loader_test:\n",
    "        im, labels = im.to(device), labels.to(device)\n",
    "        #print(labels.shape)\n",
    "        \n",
    "        labels_gender= labels[:,1]#.long().squeeze(1)\n",
    "        labels = labels[:,0] #.long().squeeze(1)\n",
    "        preds , _ = model(im, mask) # _\n",
    "        values, indices = preds.max(-1)\n",
    "        #print(labels)\n",
    "        #print(indices)\n",
    "        correct += indices.eq(labels).sum()\n",
    "\n",
    "        smiling_gen1 += (indices*labels_gender).sum()\n",
    "        smiling_gen0 += (indices*(1-labels_gender)).sum()\n",
    "        \n",
    "        smiling_gen1_not += ((1-indices)*labels_gender).sum()\n",
    "        smiling_gen0_not += ((1-indices)*(1-labels_gender)).sum()\n",
    "        gen1 += labels_gender.sum()\n",
    "        gen0 += (len(labels_gender) - labels_gender.sum())\n",
    "        del im\n",
    "        del preds\n",
    "        \n",
    "        del values\n",
    "        \n",
    "        #print(correct)\n",
    "        size += len(labels)\n",
    "        del labels\n",
    "        #print(float(correct)/float(size))\n",
    "        \n",
    "    print(smiling_gen0/gen0, smiling_gen1/gen1)\n",
    "    print((smiling_gen0/gen0 - smiling_gen1/gen1))\n",
    "    print(\"***********************\")\n",
    "    print(smiling_gen0_not/gen0, smiling_gen1_not/gen1)\n",
    "    print((smiling_gen0_not/gen0 - smiling_gen1_not/gen1))\n",
    "    print(float(correct)/float(size))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noisy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NoisyActivation(nn.Module):\n",
    "    def __init__(self,  given_locs, given_scales, min_scale, max_scale):\n",
    "        super(NoisyActivation, self).__init__()\n",
    "        size = given_scales.shape\n",
    "        self.min_scale = min_scale\n",
    "        self.max_scale = max_scale\n",
    "        self.given_locs = given_locs \n",
    "        self.given_scales = given_scales\n",
    "        self.locs = nn.Parameter(torch.Tensor(size).copy_(self.given_locs))         \n",
    "        self.rhos = nn.Parameter(torch.ones(size)-5) #-inf\n",
    "\n",
    "\n",
    "        self.normal = torch.distributions.normal.Normal(0,1)\n",
    "        self.rhos.requires_grad = True\n",
    "        self.locs.requires_grad = True\n",
    "        \n",
    "    def scales(self):\n",
    "        return (1.0 +torch.tanh(self.rhos))/2*(self.max_scale-self.min_scale) +self.min_scale             \n",
    "    \n",
    "    def sample_noise(self, mask):\n",
    "        epsilon = self.normal.sample(self.rhos.shape)\n",
    "        return self.locs + self.scales() * epsilon * mask\n",
    "                                 \n",
    "                               \n",
    "                            \n",
    "    def forward(self, input, mask):\n",
    "        noise = self.sample_noise(mask)\n",
    "        return (input)*mask + noise\n",
    "\n",
    "\n",
    "\n",
    "class vgg_syn(nn.Module):\n",
    "\n",
    "    def __init__(self, model_features, model_classifier, min_scale,max_scale, given_locs, given_scale):\n",
    "        super(vgg_syn, self).__init__()\n",
    "        \n",
    "\n",
    "        self.intermed = NoisyActivation( given_locs, given_scale, min_scale, max_scale)\n",
    "        self.model_pt2 =  torch.nn.Sequential(*(list(model_features)))\n",
    "        self.model_pt3 = model_classifier\n",
    "        #self.components = components\n",
    "        for child in itertools.chain(self.model_pt2, self.model_pt3): #self.model_pt2 #(self.model_pt2, \n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "            if isinstance(child, nn.modules.batchnorm._BatchNorm):\n",
    "                child.eval()\n",
    "                child.affine = False\n",
    "                child.track_running_stats = False\n",
    "                \n",
    "        self.intermed.rhos.reuires_grad = True\n",
    "        self.intermed.locs.reuires_grad = True\n",
    "                                 \n",
    "    def forward(self, img, mask):\n",
    "                                 \n",
    "        \n",
    "        x = self.intermed(img, mask)\n",
    "        noisy = x.detach()\n",
    "         \n",
    "        x = self.model_pt2(x)                    \n",
    "        x = x.view(img.size(0), -1)\n",
    "        x = self.model_pt3(x)                                 \n",
    "\n",
    "        return x, noisy\n",
    "    \n",
    "                                 \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VGG_16_2()\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "mus = torch.zeros((3,224,224))\n",
    "scale = torch.ones((3,224,224))*0.001\n",
    "model_syn = vgg_syn(model.convnet, model.fc ,0.0,5 ,mus, scale )\n",
    "model_syn.load_state_dict(torch.load(\"celeba-hair-avg-cloak\", map_location=device))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mult_mask = torch.where(model_syn.intermed.scales()>4.5, torch.zeros(model_syn.intermed.scales().shape), torch.ones(model_syn.intermed.scales().shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9626)\n"
     ]
    }
   ],
   "source": [
    "size = model_syn.intermed.rhos.shape\n",
    "ones= torch.ones(size)\n",
    "mask = torch.where(model_syn.intermed.scales()>4.5, ones, torch.zeros(size))\n",
    "suppressed = torch.sum(mask)\n",
    "all_count = torch.sum(ones)\n",
    "print(suppressed/all_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3461) tensor(0.4298)\n",
      "tensor(-0.0837)\n",
      "***********************\n",
      "tensor(0.6539) tensor(0.5702)\n",
      "tensor(0.0837)\n",
      "0.7807890394519726\n"
     ]
    }
   ],
   "source": [
    "demographic_parity_syn(model_syn, mult_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2353.) tensor(2500.) tensor(1930.) tensor(2122.)\n",
      "tensor(0.8202) tensor(0.8488)\n",
      "tensor(-0.0286)\n",
      "************\n",
      "tensor(0.7746) tensor(0.7395)\n",
      "tensor(0.0352)\n",
      "0.7788389419470974\n"
     ]
    }
   ],
   "source": [
    "equal_odds_syn(model_syn, mult_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
