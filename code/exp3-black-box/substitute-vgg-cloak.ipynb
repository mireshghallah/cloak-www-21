{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 \n",
    "import itertools\n",
    "\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "from vgg16_2 import VGG_16_2\n",
    "\n",
    "\n",
    "import argparse\n",
    "from data_loader_half_2 import get_loader\n",
    "from data_loader_half_2 import get_dataset\n",
    "\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import time\n",
    "import shutil\n",
    "import os\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='2'\n",
    "celeba_image_dir = '/home/name/celeba/stargan/data/celeba/images'\n",
    "attr_path = '/home/name/celeba/stargan/data/celeba/list_attr_celeba.txt'\n",
    "id_path = '/home/name/celeba/stargan/data/celeba/identity_CelebA.txt'\n",
    "selected_attrs = ['Black_Hair']\n",
    "celeba_crop_size = 224\n",
    "image_size = 224 \n",
    "batch_size = 30\n",
    "num_workers = 0\n",
    "mode1 = 'train' #test or train\n",
    "mode2 = 'test'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #\"cuda:0\" if torch.cuda.is_available() else \n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preprocessing the CelebA dataset...\n",
      "Finished preprocessing the CelebA dataset...\n"
     ]
    }
   ],
   "source": [
    "celeba_loader_train, data_train = get_loader(celeba_image_dir, attr_path, id_path, selected_attrs,celeba_crop_size, image_size, batch_size,'CelebA', mode1, num_workers)\n",
    "celeba_loader_test, data_test = get_loader(celeba_image_dir, attr_path, id_path, selected_attrs,celeba_crop_size, image_size, batch_size,'CelebA', mode2, num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preprocessing the CelebA dataset...\n"
     ]
    }
   ],
   "source": [
    "celeba_loader_test2, data_test2 = get_loader(celeba_image_dir, attr_path, id_path, selected_attrs,celeba_crop_size, image_size, 1,'CelebA', mode2, num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    size =0\n",
    "    for im, labels in celeba_loader_test:\n",
    "        im, labels = im.to(device), labels.to(device)\n",
    "        labels = labels.long().squeeze(1)\n",
    "        preds, _ = model(im)\n",
    "        values, indices = preds.max(-1)\n",
    "        correct += indices.eq(labels).sum()\n",
    "        \n",
    "        del im\n",
    "        del preds\n",
    "        \n",
    "        del values\n",
    "        \n",
    "        print(correct)\n",
    "        size += len(labels)\n",
    "        del labels\n",
    "        print(float(correct)/float(size))\n",
    "        \n",
    "    print(size)\n",
    "    print(correct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_per_example(model):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    size =0\n",
    "    for im, labels in celeba_loader_test2:\n",
    "        im, labels = im.to(device), labels[:,0].to(device)\n",
    "        labels = labels.long()\n",
    "        preds, _ = model(im)\n",
    "        values, indices = preds.max(-1)\n",
    "        correct += indices.eq(labels).sum()\n",
    "\n",
    "        \n",
    "        del im\n",
    "        del preds\n",
    "        \n",
    "        del values\n",
    "        \n",
    "        size += len(labels)\n",
    "        del labels\n",
    "        print(float(correct)/float(size))\n",
    "        \n",
    "    print(size)\n",
    "    print(correct)\n",
    "    return size,correct\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_per_example_mask(model, mask):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    size =0\n",
    "    for im, labels in celeba_loader_test2:\n",
    "        im, labels = im.to(device), labels[:,0].to(device)\n",
    "        labels = labels.long()\n",
    "        im = im\n",
    "        preds, _ = model(im,mask)\n",
    "        values, indices = preds.max(-1)\n",
    "        correct += indices.eq(labels).sum()\n",
    "\n",
    "        \n",
    "        del im\n",
    "        del preds\n",
    "        \n",
    "        del values\n",
    "        \n",
    "        size += len(labels)\n",
    "        del labels\n",
    "        print(float(correct)/float(size))\n",
    "        \n",
    "    print(size)\n",
    "    print(correct)\n",
    "    return size,correct\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, increase= False, coef=1):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    size = 0\n",
    "    correcta = 0\n",
    "    for i, (im, labels) in enumerate(celeba_loader_train):\n",
    "        #if (i ==1):\n",
    "         #   print (labels)\n",
    "        print(\"iteration no\", i)\n",
    "        im, labels = im.to(device), labels.to(device)\n",
    "        labels = labels.long().squeeze(1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output,noisy = model(im)\n",
    "        \n",
    "        \n",
    "        print(torch.mean(model.intermed.scales()))\n",
    "        print(torch.max(model.intermed.scales()))\n",
    "        print(torch.min(model.intermed.scales()))\n",
    "        \n",
    "        if (increase):\n",
    "            #loss = criterion(output, labels) + coef* 1/(torch.mean(model.intermed.scales()) )\n",
    "            print(labels.shape)\n",
    "            print(labels)\n",
    "            loss = criterion(output, labels) - coef* torch.log(torch.mean(model.intermed.scales()) ) + 10\n",
    "            \n",
    "        else:\n",
    "            loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print (loss, \"loss\")\n",
    "        #preds = model(im)\n",
    "        values, indices = output.max(-1)\n",
    "        size += len(labels)\n",
    "        correcta += indices.eq(labels).sum()\n",
    "        correct = indices.eq(labels).sum()\n",
    "        del im\n",
    "        del labels\n",
    "        del output\n",
    "        del values\n",
    "        del indices\n",
    "        \n",
    "        \n",
    "        print(float(correcta)/float(size))\n",
    "            \n",
    "            \n",
    "    print(correcta, \"correct out of all\")\n",
    "    print(size)\n",
    "    return correct, size\n",
    "     \n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mask(model, criterion, optimizer, mask, increase= False, coef=1):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    size = 0\n",
    "    correcta = 0\n",
    "    for i, (im, labels) in enumerate(celeba_loader_train):\n",
    "        #if (i ==1):\n",
    "         #   print (labels)\n",
    "        print(\"iteration no\", i)\n",
    "        im, labels = im.to(device), labels[:,0].to(device)\n",
    "        labels = labels.long()\n",
    "        \n",
    "        \n",
    "        im = im \n",
    "        optimizer.zero_grad()\n",
    "        output,noisy = model(im,mask)\n",
    "        \n",
    "        \n",
    "        print(torch.mean(model.intermed.scales()))\n",
    "        print(torch.max(model.intermed.scales()))\n",
    "        print(torch.min(model.intermed.scales()))\n",
    "        \n",
    "        if (increase):\n",
    "            print(labels.shape)\n",
    "            print(labels)\n",
    "            #loss = criterion(output, labels) + coef* 1/(torch.mean(model.intermed.scales()) )\n",
    "            loss = criterion(output, labels) - coef* torch.log(torch.mean(model.intermed.scales()) ) + 10\n",
    "        else:\n",
    "            loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print (loss, \"loss\")\n",
    "        #preds = model(im)\n",
    "        values, indices = output.max(-1)\n",
    "        size += len(labels)\n",
    "        correcta += indices.eq(labels).sum()\n",
    "        correct = indices.eq(labels).sum()\n",
    "        del im\n",
    "        del labels\n",
    "        del output\n",
    "        del values\n",
    "        del indices\n",
    "        \n",
    "        \n",
    "        print(float(correcta)/float(size))\n",
    "            \n",
    "            \n",
    "    print(correcta, \"correct out of all\")\n",
    "    print(size)\n",
    "    return correct, size\n",
    "     \n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noisy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NoisyActivation(nn.Module):\n",
    "    def __init__(self,  given_locs, given_scales, min_scale, max_scale):\n",
    "        super(NoisyActivation, self).__init__()\n",
    "        size = given_scales.shape\n",
    "        self.min_scale = min_scale\n",
    "        self.max_scale = max_scale\n",
    "        self.given_locs = given_locs \n",
    "        self.given_scales = given_scales\n",
    "        self.locs = nn.Parameter(torch.Tensor(size).copy_(self.given_locs))         \n",
    "        self.rhos = nn.Parameter(torch.ones(size)-5) #-inf\n",
    "\n",
    "\n",
    "        self.normal = torch.distributions.normal.Normal(0,1)\n",
    "        #self.rhos.requires_grad = False\n",
    "        #self.locs.requires_grad = True\n",
    "        \n",
    "    def scales(self):\n",
    "        return (1.0 +torch.tanh(self.rhos))/2*(self.max_scale-self.min_scale) +self.min_scale             \n",
    "    \n",
    "    def sample_noise(self, mask):\n",
    "        epsilon = self.normal.sample(self.rhos.shape)*mask\n",
    "        return  self.locs + self.scales()*epsilon\n",
    "                                 \n",
    "                               \n",
    "                            \n",
    "    def forward(self, input, mask):\n",
    "        noise = self.sample_noise(mask)\n",
    "        return (input)*mask + noise\n",
    "\n",
    "\n",
    "\n",
    "class vgg_syn(nn.Module):\n",
    "\n",
    "    def __init__(self, model_features, model_classifier, min_scale,max_scale, given_locs, given_scale):\n",
    "        super(vgg_syn, self).__init__()\n",
    "        \n",
    "\n",
    "        self.intermed = NoisyActivation( given_locs, given_scale, min_scale, max_scale)\n",
    "        self.model_pt2 =  torch.nn.Sequential(*(list(model_features)))\n",
    "        self.model_pt3 = model_classifier\n",
    "        #self.components = components\n",
    "        for child in itertools.chain(self.model_pt2, self.model_pt3): #self.model_pt2 #(self.model_pt2, \n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "            if isinstance(child, nn.modules.batchnorm._BatchNorm):\n",
    "                child.eval()\n",
    "                child.affine = False\n",
    "                child.track_running_stats = False\n",
    "         \n",
    "                                 \n",
    "    def forward(self, img, mask):\n",
    "                                 \n",
    "        img = img\n",
    "        x = self.intermed(img, mask)\n",
    "        noisy = x.detach()\n",
    "       \n",
    "        x = self.model_pt2(x)                    \n",
    "        x = x.view(img.size(0), -1)\n",
    "        x = self.model_pt3(x)                                 \n",
    "\n",
    "        return x, noisy\n",
    "    \n",
    "                                 \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = VGG_16_2()\n",
    "\n",
    "model.load_state_dict(torch.load(\"vgg-hair-less-84l88\",  map_location=device))\n",
    "\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "mus = torch.zeros((3,224,224))\n",
    "scale = torch.ones((3,224,224))*0.001\n",
    "model_syn = vgg_syn(model.convnet, model.fc ,0.0,5 ,mus, scale )\n",
    "model_syn.load_state_dict(torch.load(\"celeba-vgg-hair-avg-cloak\", map_location=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mult_mask =  torch.ones(model_syn.intermed.rhos.shape)\n",
    "mult_mask = torch.where(model_syn.intermed.scales()>4.5, torch.zeros(model_syn.intermed.scales().shape), torch.ones(model_syn.intermed.scales().shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "d = list(filter(lambda p: p.requires_grad, model_syn.parameters()))\n",
    "print(len(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 0\n",
      "tensor(4.8030, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9007, grad_fn=<AddBackward0>) loss\n",
      "0.7666666666666667\n",
      "iteration no 1\n",
      "tensor(4.8030, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 1, 1, 0])\n",
      "tensor(9.9321, grad_fn=<AddBackward0>) loss\n",
      "0.7833333333333333\n",
      "iteration no 2\n",
      "tensor(4.8030, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0])\n",
      "tensor(9.9499, grad_fn=<AddBackward0>) loss\n",
      "0.7666666666666667\n",
      "iteration no 3\n",
      "tensor(4.8030, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 1, 0, 0, 0])\n",
      "tensor(9.9916, grad_fn=<AddBackward0>) loss\n",
      "0.7666666666666667\n",
      "iteration no 4\n",
      "tensor(4.8030, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8761, grad_fn=<AddBackward0>) loss\n",
      "0.7933333333333333\n",
      "iteration no 5\n",
      "tensor(4.8030, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9547, grad_fn=<AddBackward0>) loss\n",
      "0.8055555555555556\n",
      "iteration no 6\n",
      "tensor(4.8030, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 1, 0, 0])\n",
      "tensor(9.9277, grad_fn=<AddBackward0>) loss\n",
      "0.8142857142857143\n",
      "iteration no 7\n",
      "tensor(4.8029, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9214, grad_fn=<AddBackward0>) loss\n",
      "0.8166666666666667\n",
      "iteration no 8\n",
      "tensor(4.8029, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9539, grad_fn=<AddBackward0>) loss\n",
      "0.8111111111111111\n",
      "iteration no 9\n",
      "tensor(4.8029, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(10.0246, grad_fn=<AddBackward0>) loss\n",
      "0.8066666666666666\n",
      "iteration no 10\n",
      "tensor(4.8029, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
      "        1, 0, 0, 1, 1, 0])\n",
      "tensor(10.0658, grad_fn=<AddBackward0>) loss\n",
      "0.7909090909090909\n",
      "iteration no 11\n",
      "tensor(4.8029, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 1, 0, 0, 0, 1])\n",
      "tensor(9.9617, grad_fn=<AddBackward0>) loss\n",
      "0.7916666666666666\n",
      "iteration no 12\n",
      "tensor(4.8029, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0])\n",
      "tensor(9.9695, grad_fn=<AddBackward0>) loss\n",
      "0.7923076923076923\n",
      "iteration no 13\n",
      "tensor(4.8029, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.8963, grad_fn=<AddBackward0>) loss\n",
      "0.7952380952380952\n",
      "iteration no 14\n",
      "tensor(4.8029, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(10.1159, grad_fn=<AddBackward0>) loss\n",
      "0.7844444444444445\n",
      "iteration no 15\n",
      "tensor(4.8029, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8687, grad_fn=<AddBackward0>) loss\n",
      "0.7895833333333333\n",
      "iteration no 16\n",
      "tensor(4.8029, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
      "        0, 0, 1, 1, 0, 0])\n",
      "tensor(9.9738, grad_fn=<AddBackward0>) loss\n",
      "0.7843137254901961\n",
      "iteration no 17\n",
      "tensor(4.8029, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.8960, grad_fn=<AddBackward0>) loss\n",
      "0.7870370370370371\n",
      "iteration no 18\n",
      "tensor(4.8029, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.8776, grad_fn=<AddBackward0>) loss\n",
      "0.7894736842105263\n",
      "iteration no 19\n",
      "tensor(4.8028, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,\n",
      "        0, 1, 0, 0, 1, 1])\n",
      "tensor(9.9908, grad_fn=<AddBackward0>) loss\n",
      "0.7866666666666666\n",
      "iteration no 20\n",
      "tensor(4.8028, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
      "        1, 1, 0, 0, 0, 0])\n",
      "tensor(9.9397, grad_fn=<AddBackward0>) loss\n",
      "0.7857142857142857\n",
      "iteration no 21\n",
      "tensor(4.8028, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 0])\n",
      "tensor(9.9219, grad_fn=<AddBackward0>) loss\n",
      "0.7893939393939394\n",
      "iteration no 22\n",
      "tensor(4.8028, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8891, grad_fn=<AddBackward0>) loss\n",
      "0.7942028985507247\n",
      "iteration no 23\n",
      "tensor(4.8028, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9712, grad_fn=<AddBackward0>) loss\n",
      "0.7930555555555555\n",
      "iteration no 24\n",
      "tensor(4.8028, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9572, grad_fn=<AddBackward0>) loss\n",
      "0.792\n",
      "iteration no 25\n",
      "tensor(4.8028, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9576, grad_fn=<AddBackward0>) loss\n",
      "0.7897435897435897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 26\n",
      "tensor(4.8028, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 1, 0, 0])\n",
      "tensor(9.9732, grad_fn=<AddBackward0>) loss\n",
      "0.7888888888888889\n",
      "iteration no 27\n",
      "tensor(4.8028, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0])\n",
      "tensor(10.0378, grad_fn=<AddBackward0>) loss\n",
      "0.7857142857142857\n",
      "iteration no 28\n",
      "tensor(4.8028, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(10.0586, grad_fn=<AddBackward0>) loss\n",
      "0.7816091954022989\n",
      "iteration no 29\n",
      "tensor(4.8028, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.9598, grad_fn=<AddBackward0>) loss\n",
      "0.7811111111111111\n",
      "iteration no 30\n",
      "tensor(4.8028, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0])\n",
      "tensor(10.0238, grad_fn=<AddBackward0>) loss\n",
      "0.7784946236559139\n",
      "iteration no 31\n",
      "tensor(4.8027, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 1, 0, 0, 1, 0])\n",
      "tensor(10.1103, grad_fn=<AddBackward0>) loss\n",
      "0.7739583333333333\n",
      "iteration no 32\n",
      "tensor(4.8027, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 1, 0, 0])\n",
      "tensor(10.0758, grad_fn=<AddBackward0>) loss\n",
      "0.7717171717171717\n",
      "iteration no 33\n",
      "tensor(4.8027, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9954, grad_fn=<AddBackward0>) loss\n",
      "0.7715686274509804\n",
      "iteration no 34\n",
      "tensor(4.8027, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8766, grad_fn=<AddBackward0>) loss\n",
      "0.7761904761904762\n",
      "iteration no 35\n",
      "tensor(4.8027, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9201, grad_fn=<AddBackward0>) loss\n",
      "0.7777777777777778\n",
      "iteration no 36\n",
      "tensor(4.8027, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        0, 0, 1, 1, 0, 0])\n",
      "tensor(9.9490, grad_fn=<AddBackward0>) loss\n",
      "0.7774774774774775\n",
      "iteration no 37\n",
      "tensor(4.8027, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9125, grad_fn=<AddBackward0>) loss\n",
      "0.7789473684210526\n",
      "iteration no 38\n",
      "tensor(4.8027, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8586, grad_fn=<AddBackward0>) loss\n",
      "0.7803418803418803\n",
      "iteration no 39\n",
      "tensor(4.8027, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8771, grad_fn=<AddBackward0>) loss\n",
      "0.7816666666666666\n",
      "iteration no 40\n",
      "tensor(4.8027, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(10.0085, grad_fn=<AddBackward0>) loss\n",
      "0.7788617886178861\n",
      "iteration no 41\n",
      "tensor(4.8027, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9590, grad_fn=<AddBackward0>) loss\n",
      "0.776984126984127\n",
      "iteration no 42\n",
      "tensor(4.8027, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9381, grad_fn=<AddBackward0>) loss\n",
      "0.7767441860465116\n",
      "iteration no 43\n",
      "tensor(4.8027, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(10.1058, grad_fn=<AddBackward0>) loss\n",
      "0.7757575757575758\n",
      "iteration no 44\n",
      "tensor(4.8027, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 1])\n",
      "tensor(9.9633, grad_fn=<AddBackward0>) loss\n",
      "0.7755555555555556\n",
      "iteration no 45\n",
      "tensor(4.8027, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0])\n",
      "tensor(10.0714, grad_fn=<AddBackward0>) loss\n",
      "0.7731884057971015\n",
      "iteration no 46\n",
      "tensor(4.8027, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(10.0335, grad_fn=<AddBackward0>) loss\n",
      "0.7709219858156028\n",
      "iteration no 47\n",
      "tensor(4.8027, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,\n",
      "        0, 0, 1, 1, 0, 0])\n",
      "tensor(9.9994, grad_fn=<AddBackward0>) loss\n",
      "0.7694444444444445\n",
      "iteration no 48\n",
      "tensor(4.8027, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.8603, grad_fn=<AddBackward0>) loss\n",
      "0.7714285714285715\n",
      "iteration no 49\n",
      "tensor(4.8027, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(10.0296, grad_fn=<AddBackward0>) loss\n",
      "0.77\n",
      "iteration no 50\n",
      "tensor(4.8026, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
      "        0, 0, 0, 1, 0, 1])\n",
      "tensor(10.0053, grad_fn=<AddBackward0>) loss\n",
      "0.7699346405228759\n",
      "iteration no 51\n",
      "tensor(4.8026, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.9476, grad_fn=<AddBackward0>) loss\n",
      "0.7698717948717949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 52\n",
      "tensor(4.8026, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 1, 0, 1, 0, 0])\n",
      "tensor(9.8800, grad_fn=<AddBackward0>) loss\n",
      "0.7710691823899372\n",
      "iteration no 53\n",
      "tensor(4.8026, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.9486, grad_fn=<AddBackward0>) loss\n",
      "0.7709876543209877\n",
      "iteration no 54\n",
      "tensor(4.8026, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 1])\n",
      "tensor(9.9632, grad_fn=<AddBackward0>) loss\n",
      "0.7703030303030303\n",
      "iteration no 55\n",
      "tensor(4.8026, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 0, 0, 1, 0, 1])\n",
      "tensor(10.0618, grad_fn=<AddBackward0>) loss\n",
      "0.768452380952381\n",
      "iteration no 56\n",
      "tensor(4.8026, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9453, grad_fn=<AddBackward0>) loss\n",
      "0.7684210526315789\n",
      "iteration no 57\n",
      "tensor(4.8026, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9636, grad_fn=<AddBackward0>) loss\n",
      "0.7689655172413793\n",
      "iteration no 58\n",
      "tensor(4.8026, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9278, grad_fn=<AddBackward0>) loss\n",
      "0.7700564971751412\n",
      "iteration no 59\n",
      "tensor(4.8026, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.8092, grad_fn=<AddBackward0>) loss\n",
      "0.7727777777777778\n",
      "iteration no 60\n",
      "tensor(4.8026, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
      "        1, 0, 0, 1, 0, 0])\n",
      "tensor(10.0485, grad_fn=<AddBackward0>) loss\n",
      "0.7721311475409836\n",
      "iteration no 61\n",
      "tensor(4.8026, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1])\n",
      "tensor(9.9550, grad_fn=<AddBackward0>) loss\n",
      "0.7720430107526882\n",
      "iteration no 62\n",
      "tensor(4.8026, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.8484, grad_fn=<AddBackward0>) loss\n",
      "0.773015873015873\n",
      "iteration no 63\n",
      "tensor(4.8026, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        1, 0, 1, 1, 0, 1])\n",
      "tensor(9.9322, grad_fn=<AddBackward0>) loss\n",
      "0.7744791666666667\n",
      "iteration no 64\n",
      "tensor(4.8026, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.9216, grad_fn=<AddBackward0>) loss\n",
      "0.7748717948717949\n",
      "iteration no 65\n",
      "tensor(4.8026, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,\n",
      "        0, 1, 1, 1, 0, 1])\n",
      "tensor(10.0171, grad_fn=<AddBackward0>) loss\n",
      "0.7727272727272727\n",
      "iteration no 66\n",
      "tensor(4.8026, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 1, 0])\n",
      "tensor(9.8689, grad_fn=<AddBackward0>) loss\n",
      "0.7736318407960199\n",
      "iteration no 67\n",
      "tensor(4.8026, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1])\n",
      "tensor(9.9499, grad_fn=<AddBackward0>) loss\n",
      "0.7735294117647059\n",
      "iteration no 68\n",
      "tensor(4.8026, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.8883, grad_fn=<AddBackward0>) loss\n",
      "0.77487922705314\n",
      "iteration no 69\n",
      "tensor(4.8026, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(10.1009, grad_fn=<AddBackward0>) loss\n",
      "0.7728571428571429\n",
      "iteration no 70\n",
      "tensor(4.8026, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.7550, grad_fn=<AddBackward0>) loss\n",
      "0.7746478873239436\n",
      "iteration no 71\n",
      "tensor(4.8025, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        1, 0, 0, 0, 0, 1])\n",
      "tensor(9.9372, grad_fn=<AddBackward0>) loss\n",
      "0.775\n",
      "iteration no 72\n",
      "tensor(4.8025, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        1, 0, 1, 0, 0, 0])\n",
      "tensor(9.8733, grad_fn=<AddBackward0>) loss\n",
      "0.776255707762557\n",
      "iteration no 73\n",
      "tensor(4.8025, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.9965, grad_fn=<AddBackward0>) loss\n",
      "0.7761261261261261\n",
      "iteration no 74\n",
      "tensor(4.8025, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0])\n",
      "tensor(10.0484, grad_fn=<AddBackward0>) loss\n",
      "0.7751111111111111\n",
      "iteration no 75\n",
      "tensor(4.8025, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8548, grad_fn=<AddBackward0>) loss\n",
      "0.7767543859649123\n",
      "iteration no 76\n",
      "tensor(4.8025, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
      "        0, 1, 0, 0, 1, 0])\n",
      "tensor(9.9989, grad_fn=<AddBackward0>) loss\n",
      "0.7766233766233767\n",
      "iteration no 77\n",
      "tensor(4.8025, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 0, 1, 0, 0, 1])\n",
      "tensor(9.9793, grad_fn=<AddBackward0>) loss\n",
      "0.7756410256410257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 78\n",
      "tensor(4.8025, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 1, 1])\n",
      "tensor(10.0672, grad_fn=<AddBackward0>) loss\n",
      "0.7746835443037975\n",
      "iteration no 79\n",
      "tensor(4.8025, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(10.0377, grad_fn=<AddBackward0>) loss\n",
      "0.77375\n",
      "iteration no 80\n",
      "tensor(4.8025, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 1, 0])\n",
      "tensor(9.8591, grad_fn=<AddBackward0>) loss\n",
      "0.7744855967078189\n",
      "iteration no 81\n",
      "tensor(4.8025, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 1])\n",
      "tensor(9.9632, grad_fn=<AddBackward0>) loss\n",
      "0.7752032520325203\n",
      "iteration no 82\n",
      "tensor(4.8025, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 1])\n",
      "tensor(9.9426, grad_fn=<AddBackward0>) loss\n",
      "0.7751004016064257\n",
      "iteration no 83\n",
      "tensor(4.8025, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "        0, 1, 0, 0, 0, 1])\n",
      "tensor(10.0304, grad_fn=<AddBackward0>) loss\n",
      "0.775\n",
      "iteration no 84\n",
      "tensor(4.8025, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9600, grad_fn=<AddBackward0>) loss\n",
      "0.7752941176470588\n",
      "iteration no 85\n",
      "tensor(4.8025, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0])\n",
      "tensor(9.9140, grad_fn=<AddBackward0>) loss\n",
      "0.775968992248062\n",
      "iteration no 86\n",
      "tensor(4.8025, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,\n",
      "        1, 0, 0, 1, 1, 0])\n",
      "tensor(10.0275, grad_fn=<AddBackward0>) loss\n",
      "0.774712643678161\n",
      "iteration no 87\n",
      "tensor(4.8025, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8658, grad_fn=<AddBackward0>) loss\n",
      "0.7761363636363636\n",
      "iteration no 88\n",
      "tensor(4.8025, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n",
      "        0, 1, 1, 0, 1, 1])\n",
      "tensor(10.1050, grad_fn=<AddBackward0>) loss\n",
      "0.7749063670411985\n",
      "iteration no 89\n",
      "tensor(4.8025, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.9070, grad_fn=<AddBackward0>) loss\n",
      "0.7755555555555556\n",
      "iteration no 90\n",
      "tensor(4.8025, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.9533, grad_fn=<AddBackward0>) loss\n",
      "0.7754578754578755\n",
      "iteration no 91\n",
      "tensor(4.8025, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9888, grad_fn=<AddBackward0>) loss\n",
      "0.7757246376811594\n",
      "iteration no 92\n",
      "tensor(4.8025, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
      "        1, 1, 0, 0, 0, 0])\n",
      "tensor(9.9690, grad_fn=<AddBackward0>) loss\n",
      "0.7752688172043011\n",
      "iteration no 93\n",
      "tensor(4.8024, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.9581, grad_fn=<AddBackward0>) loss\n",
      "0.7755319148936171\n",
      "iteration no 94\n",
      "tensor(4.8024, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 1, 0])\n",
      "tensor(10.1477, grad_fn=<AddBackward0>) loss\n",
      "0.7747368421052632\n",
      "iteration no 95\n",
      "tensor(4.8024, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9333, grad_fn=<AddBackward0>) loss\n",
      "0.7746527777777777\n",
      "iteration no 96\n",
      "tensor(4.8024, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0])\n",
      "tensor(9.9704, grad_fn=<AddBackward0>) loss\n",
      "0.7745704467353952\n",
      "iteration no 97\n",
      "tensor(4.8024, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(10.0536, grad_fn=<AddBackward0>) loss\n",
      "0.7744897959183673\n",
      "iteration no 98\n",
      "tensor(4.8024, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9321, grad_fn=<AddBackward0>) loss\n",
      "0.774074074074074\n",
      "iteration no 99\n",
      "tensor(4.8024, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(10.0126, grad_fn=<AddBackward0>) loss\n",
      "0.7733333333333333\n",
      "iteration no 100\n",
      "tensor(4.8024, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9706, grad_fn=<AddBackward0>) loss\n",
      "0.7732673267326733\n",
      "iteration no 101\n",
      "tensor(4.8024, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.8560, grad_fn=<AddBackward0>) loss\n",
      "0.7738562091503268\n",
      "iteration no 102\n",
      "tensor(4.8024, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 1, 1])\n",
      "tensor(9.9305, grad_fn=<AddBackward0>) loss\n",
      "0.7737864077669903\n",
      "iteration no 103\n",
      "tensor(4.8024, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 1, 0])\n",
      "tensor(9.8328, grad_fn=<AddBackward0>) loss\n",
      "0.7746794871794872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 104\n",
      "tensor(4.8024, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.8981, grad_fn=<AddBackward0>) loss\n",
      "0.7746031746031746\n",
      "iteration no 105\n",
      "tensor(4.8024, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 1, 1, 0, 0])\n",
      "tensor(9.9816, grad_fn=<AddBackward0>) loss\n",
      "0.7738993710691824\n",
      "iteration no 106\n",
      "tensor(4.8024, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 1, 1, 0, 0])\n",
      "tensor(9.8799, grad_fn=<AddBackward0>) loss\n",
      "0.7744548286604361\n",
      "iteration no 107\n",
      "tensor(4.8024, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 1, 0, 0])\n",
      "tensor(10.0415, grad_fn=<AddBackward0>) loss\n",
      "0.7734567901234568\n",
      "iteration no 108\n",
      "tensor(4.8024, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 1, 1, 1])\n",
      "tensor(9.8522, grad_fn=<AddBackward0>) loss\n",
      "0.7740061162079511\n",
      "iteration no 109\n",
      "tensor(4.8024, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 1])\n",
      "tensor(9.9518, grad_fn=<AddBackward0>) loss\n",
      "0.7739393939393939\n",
      "iteration no 110\n",
      "tensor(4.8024, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9436, grad_fn=<AddBackward0>) loss\n",
      "0.7738738738738739\n",
      "iteration no 111\n",
      "tensor(4.8024, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        1, 0, 0, 0, 1, 1])\n",
      "tensor(9.9282, grad_fn=<AddBackward0>) loss\n",
      "0.7738095238095238\n",
      "iteration no 112\n",
      "tensor(4.8024, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 1, 0])\n",
      "tensor(9.9061, grad_fn=<AddBackward0>) loss\n",
      "0.7746312684365781\n",
      "iteration no 113\n",
      "tensor(4.8024, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 0])\n",
      "tensor(9.9023, grad_fn=<AddBackward0>) loss\n",
      "0.7751461988304094\n",
      "iteration no 114\n",
      "tensor(4.8024, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9123, grad_fn=<AddBackward0>) loss\n",
      "0.7747826086956522\n",
      "iteration no 115\n",
      "tensor(4.8024, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 1, 0, 0, 1, 0])\n",
      "tensor(9.8700, grad_fn=<AddBackward0>) loss\n",
      "0.7758620689655172\n",
      "iteration no 116\n",
      "tensor(4.8024, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9316, grad_fn=<AddBackward0>) loss\n",
      "0.7754985754985755\n",
      "iteration no 117\n",
      "tensor(4.8024, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9261, grad_fn=<AddBackward0>) loss\n",
      "0.7757062146892655\n",
      "iteration no 118\n",
      "tensor(4.8024, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n",
      "        0, 0, 1, 0, 1, 0])\n",
      "tensor(9.9652, grad_fn=<AddBackward0>) loss\n",
      "0.7756302521008404\n",
      "iteration no 119\n",
      "tensor(4.8023, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8495, grad_fn=<AddBackward0>) loss\n",
      "0.7763888888888889\n",
      "iteration no 120\n",
      "tensor(4.8023, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(10.0038, grad_fn=<AddBackward0>) loss\n",
      "0.7757575757575758\n",
      "iteration no 121\n",
      "tensor(4.8023, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9979, grad_fn=<AddBackward0>) loss\n",
      "0.7754098360655738\n",
      "iteration no 122\n",
      "tensor(4.8023, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 1])\n",
      "tensor(9.9284, grad_fn=<AddBackward0>) loss\n",
      "0.7758807588075881\n",
      "iteration no 123\n",
      "tensor(4.8023, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 1, 1, 0, 0, 0])\n",
      "tensor(10.0324, grad_fn=<AddBackward0>) loss\n",
      "0.7752688172043011\n",
      "iteration no 124\n",
      "tensor(4.8023, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 1, 1, 0])\n",
      "tensor(9.9697, grad_fn=<AddBackward0>) loss\n",
      "0.7744\n",
      "iteration no 125\n",
      "tensor(4.8023, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 1])\n",
      "tensor(9.9877, grad_fn=<AddBackward0>) loss\n",
      "0.7746031746031746\n",
      "iteration no 126\n",
      "tensor(4.8023, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 1, 0, 1, 0])\n",
      "tensor(9.8887, grad_fn=<AddBackward0>) loss\n",
      "0.7750656167979003\n",
      "iteration no 127\n",
      "tensor(4.8023, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 1])\n",
      "tensor(10.0059, grad_fn=<AddBackward0>) loss\n",
      "0.77421875\n",
      "iteration no 128\n",
      "tensor(4.8023, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 1, 1, 0, 0])\n",
      "tensor(9.9689, grad_fn=<AddBackward0>) loss\n",
      "0.7736434108527132\n",
      "iteration no 129\n",
      "tensor(4.8023, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.9234, grad_fn=<AddBackward0>) loss\n",
      "0.7733333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 130\n",
      "tensor(4.8023, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9179, grad_fn=<AddBackward0>) loss\n",
      "0.7732824427480917\n",
      "iteration no 131\n",
      "tensor(4.8023, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9197, grad_fn=<AddBackward0>) loss\n",
      "0.7732323232323233\n",
      "iteration no 132\n",
      "tensor(4.8023, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9629, grad_fn=<AddBackward0>) loss\n",
      "0.7736842105263158\n",
      "iteration no 133\n",
      "tensor(4.8023, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 1, 0])\n",
      "tensor(10.0548, grad_fn=<AddBackward0>) loss\n",
      "0.7728855721393035\n",
      "iteration no 134\n",
      "tensor(4.8023, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9207, grad_fn=<AddBackward0>) loss\n",
      "0.7730864197530865\n",
      "iteration no 135\n",
      "tensor(4.8023, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 0, 0])\n",
      "tensor(9.9004, grad_fn=<AddBackward0>) loss\n",
      "0.7735294117647059\n",
      "iteration no 136\n",
      "tensor(4.8023, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.9078, grad_fn=<AddBackward0>) loss\n",
      "0.7744525547445256\n",
      "iteration no 137\n",
      "tensor(4.8023, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9855, grad_fn=<AddBackward0>) loss\n",
      "0.7746376811594203\n",
      "iteration no 138\n",
      "tensor(4.8023, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 1, 1, 0])\n",
      "tensor(10.0126, grad_fn=<AddBackward0>) loss\n",
      "0.7743405275779377\n",
      "iteration no 139\n",
      "tensor(4.8023, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 0, 1, 0, 1])\n",
      "tensor(9.8703, grad_fn=<AddBackward0>) loss\n",
      "0.7742857142857142\n",
      "iteration no 140\n",
      "tensor(4.8023, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9669, grad_fn=<AddBackward0>) loss\n",
      "0.7742316784869976\n",
      "iteration no 141\n",
      "tensor(4.8023, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9912, grad_fn=<AddBackward0>) loss\n",
      "0.773943661971831\n",
      "iteration no 142\n",
      "tensor(4.8023, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.9761, grad_fn=<AddBackward0>) loss\n",
      "0.7734265734265734\n",
      "iteration no 143\n",
      "tensor(4.8023, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9365, grad_fn=<AddBackward0>) loss\n",
      "0.774074074074074\n",
      "iteration no 144\n",
      "tensor(4.8022, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 1, 0])\n",
      "tensor(9.8513, grad_fn=<AddBackward0>) loss\n",
      "0.7740229885057471\n",
      "iteration no 145\n",
      "tensor(4.8022, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 1, 0, 1])\n",
      "tensor(9.9373, grad_fn=<AddBackward0>) loss\n",
      "0.7735159817351598\n",
      "iteration no 146\n",
      "tensor(4.8022, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 1, 0])\n",
      "tensor(9.9417, grad_fn=<AddBackward0>) loss\n",
      "0.773922902494331\n",
      "iteration no 147\n",
      "tensor(4.8022, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.9716, grad_fn=<AddBackward0>) loss\n",
      "0.7736486486486487\n",
      "iteration no 148\n",
      "tensor(4.8022, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
      "        0, 0, 1, 1, 0, 0])\n",
      "tensor(9.8636, grad_fn=<AddBackward0>) loss\n",
      "0.7740492170022372\n",
      "iteration no 149\n",
      "tensor(4.8022, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1])\n",
      "tensor(9.8345, grad_fn=<AddBackward0>) loss\n",
      "0.7748888888888888\n",
      "iteration no 150\n",
      "tensor(4.8022, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(10.0207, grad_fn=<AddBackward0>) loss\n",
      "0.7746136865342164\n",
      "iteration no 151\n",
      "tensor(4.8022, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(10.0345, grad_fn=<AddBackward0>) loss\n",
      "0.7741228070175439\n",
      "iteration no 152\n",
      "tensor(4.8022, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(10.0543, grad_fn=<AddBackward0>) loss\n",
      "0.773202614379085\n",
      "iteration no 153\n",
      "tensor(4.8022, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 1, 0, 1])\n",
      "tensor(9.8729, grad_fn=<AddBackward0>) loss\n",
      "0.7733766233766234\n",
      "iteration no 154\n",
      "tensor(4.8022, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9429, grad_fn=<AddBackward0>) loss\n",
      "0.7735483870967742\n",
      "iteration no 155\n",
      "tensor(4.8022, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 1, 0, 1])\n",
      "tensor(10.0368, grad_fn=<AddBackward0>) loss\n",
      "0.7728632478632479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 156\n",
      "tensor(4.8022, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.8465, grad_fn=<AddBackward0>) loss\n",
      "0.7732484076433122\n",
      "iteration no 157\n",
      "tensor(4.8022, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.9477, grad_fn=<AddBackward0>) loss\n",
      "0.7729957805907173\n",
      "iteration no 158\n",
      "tensor(4.8022, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "        1, 1, 1, 0, 0, 1])\n",
      "tensor(9.9042, grad_fn=<AddBackward0>) loss\n",
      "0.7729559748427673\n",
      "iteration no 159\n",
      "tensor(4.8022, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(10.0022, grad_fn=<AddBackward0>) loss\n",
      "0.7722916666666667\n",
      "iteration no 160\n",
      "tensor(4.8022, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 0, 0])\n",
      "tensor(9.9541, grad_fn=<AddBackward0>) loss\n",
      "0.7724637681159421\n",
      "iteration no 161\n",
      "tensor(4.8022, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9803, grad_fn=<AddBackward0>) loss\n",
      "0.7724279835390947\n",
      "iteration no 162\n",
      "tensor(4.8022, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 1, 0, 0])\n",
      "tensor(9.8947, grad_fn=<AddBackward0>) loss\n",
      "0.7723926380368098\n",
      "iteration no 163\n",
      "tensor(4.8022, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n",
      "        0, 0, 0, 1, 1, 1])\n",
      "tensor(10.0262, grad_fn=<AddBackward0>) loss\n",
      "0.7723577235772358\n",
      "iteration no 164\n",
      "tensor(4.8022, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 1, 0, 1, 0])\n",
      "tensor(9.9981, grad_fn=<AddBackward0>) loss\n",
      "0.7717171717171717\n",
      "iteration no 165\n",
      "tensor(4.8022, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0])\n",
      "tensor(9.9205, grad_fn=<AddBackward0>) loss\n",
      "0.772289156626506\n",
      "iteration no 166\n",
      "tensor(4.8021, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9648, grad_fn=<AddBackward0>) loss\n",
      "0.7722554890219561\n",
      "iteration no 167\n",
      "tensor(4.8021, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9189, grad_fn=<AddBackward0>) loss\n",
      "0.772420634920635\n",
      "iteration no 168\n",
      "tensor(4.8021, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8385, grad_fn=<AddBackward0>) loss\n",
      "0.7727810650887574\n",
      "iteration no 169\n",
      "tensor(4.8021, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(10.0142, grad_fn=<AddBackward0>) loss\n",
      "0.7727450980392156\n",
      "iteration no 170\n",
      "tensor(4.8021, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.8971, grad_fn=<AddBackward0>) loss\n",
      "0.7725146198830409\n",
      "iteration no 171\n",
      "tensor(4.8021, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 0, 1, 0, 1])\n",
      "tensor(9.9139, grad_fn=<AddBackward0>) loss\n",
      "0.7724806201550387\n",
      "iteration no 172\n",
      "tensor(4.8021, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.8412, grad_fn=<AddBackward0>) loss\n",
      "0.7730250481695569\n",
      "iteration no 173\n",
      "tensor(4.8021, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8191, grad_fn=<AddBackward0>) loss\n",
      "0.7735632183908046\n",
      "iteration no 174\n",
      "tensor(4.8021, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 1, 1, 0, 1, 0])\n",
      "tensor(10.0212, grad_fn=<AddBackward0>) loss\n",
      "0.7731428571428571\n",
      "iteration no 175\n",
      "tensor(4.8021, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
      "        0, 1, 0, 1, 0, 0])\n",
      "tensor(9.9391, grad_fn=<AddBackward0>) loss\n",
      "0.7729166666666667\n",
      "iteration no 176\n",
      "tensor(4.8021, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(10.0023, grad_fn=<AddBackward0>) loss\n",
      "0.7730696798493408\n",
      "iteration no 177\n",
      "tensor(4.8021, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,\n",
      "        0, 0, 1, 1, 0, 0])\n",
      "tensor(9.9866, grad_fn=<AddBackward0>) loss\n",
      "0.7732209737827715\n",
      "iteration no 178\n",
      "tensor(4.8021, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9520, grad_fn=<AddBackward0>) loss\n",
      "0.7731843575418994\n",
      "iteration no 179\n",
      "tensor(4.8021, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9137, grad_fn=<AddBackward0>) loss\n",
      "0.7731481481481481\n",
      "iteration no 180\n",
      "tensor(4.8021, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 1])\n",
      "tensor(9.9542, grad_fn=<AddBackward0>) loss\n",
      "0.7731123388581952\n",
      "iteration no 181\n",
      "tensor(4.8021, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 1, 0, 0, 1, 0])\n",
      "tensor(9.9727, grad_fn=<AddBackward0>) loss\n",
      "0.7730769230769231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 182\n",
      "tensor(4.8021, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9090, grad_fn=<AddBackward0>) loss\n",
      "0.7735883424408014\n",
      "iteration no 183\n",
      "tensor(4.8021, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 0, 0])\n",
      "tensor(9.9512, grad_fn=<AddBackward0>) loss\n",
      "0.7735507246376812\n",
      "iteration no 184\n",
      "tensor(4.8021, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(10.0233, grad_fn=<AddBackward0>) loss\n",
      "0.7735135135135135\n",
      "iteration no 185\n",
      "tensor(4.8020, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 1])\n",
      "tensor(9.9235, grad_fn=<AddBackward0>) loss\n",
      "0.7736559139784946\n",
      "iteration no 186\n",
      "tensor(4.8020, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
      "        1, 1, 0, 0, 0, 1])\n",
      "tensor(9.9671, grad_fn=<AddBackward0>) loss\n",
      "0.7736185383244206\n",
      "iteration no 187\n",
      "tensor(4.8020, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.9126, grad_fn=<AddBackward0>) loss\n",
      "0.773936170212766\n",
      "iteration no 188\n",
      "tensor(4.8020, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9796, grad_fn=<AddBackward0>) loss\n",
      "0.7735449735449735\n",
      "iteration no 189\n",
      "tensor(4.8020, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.8971, grad_fn=<AddBackward0>) loss\n",
      "0.7735087719298246\n",
      "iteration no 190\n",
      "tensor(4.8020, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(10.0150, grad_fn=<AddBackward0>) loss\n",
      "0.7729493891797556\n",
      "iteration no 191\n",
      "tensor(4.8020, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(10.0008, grad_fn=<AddBackward0>) loss\n",
      "0.7730902777777777\n",
      "iteration no 192\n",
      "tensor(4.8020, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 1, 1, 0])\n",
      "tensor(9.9099, grad_fn=<AddBackward0>) loss\n",
      "0.7735751295336788\n",
      "iteration no 193\n",
      "tensor(4.8020, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.8993, grad_fn=<AddBackward0>) loss\n",
      "0.7737113402061856\n",
      "iteration no 194\n",
      "tensor(4.8020, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(10.0366, grad_fn=<AddBackward0>) loss\n",
      "0.7731623931623932\n",
      "iteration no 195\n",
      "tensor(4.8020, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 0, 0, 0, 1, 1])\n",
      "tensor(9.9955, grad_fn=<AddBackward0>) loss\n",
      "0.7726190476190476\n",
      "iteration no 196\n",
      "tensor(4.8020, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.9137, grad_fn=<AddBackward0>) loss\n",
      "0.7722504230118443\n",
      "iteration no 197\n",
      "tensor(4.8020, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9101, grad_fn=<AddBackward0>) loss\n",
      "0.7725589225589226\n",
      "iteration no 198\n",
      "tensor(4.8020, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9349, grad_fn=<AddBackward0>) loss\n",
      "0.7725293132328308\n",
      "iteration no 199\n",
      "tensor(4.8020, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
      "        0, 0, 1, 1, 1, 1])\n",
      "tensor(9.9316, grad_fn=<AddBackward0>) loss\n",
      "0.7723333333333333\n",
      "iteration no 200\n",
      "tensor(4.8020, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 1, 0, 0, 1, 1])\n",
      "tensor(9.9593, grad_fn=<AddBackward0>) loss\n",
      "0.7724709784411277\n",
      "iteration no 201\n",
      "tensor(4.8020, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 1])\n",
      "tensor(9.9013, grad_fn=<AddBackward0>) loss\n",
      "0.7724422442244224\n",
      "iteration no 202\n",
      "tensor(4.8020, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 1, 0, 0])\n",
      "tensor(10.2188, grad_fn=<AddBackward0>) loss\n",
      "0.7714285714285715\n",
      "iteration no 203\n",
      "tensor(4.8020, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 1, 0, 1, 0])\n",
      "tensor(9.9556, grad_fn=<AddBackward0>) loss\n",
      "0.7717320261437909\n",
      "iteration no 204\n",
      "tensor(4.8020, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 0, 1])\n",
      "tensor(9.8671, grad_fn=<AddBackward0>) loss\n",
      "0.7720325203252032\n",
      "iteration no 205\n",
      "tensor(4.8020, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0])\n",
      "tensor(9.9927, grad_fn=<AddBackward0>) loss\n",
      "0.7718446601941747\n",
      "iteration no 206\n",
      "tensor(4.8020, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 1, 1])\n",
      "tensor(10.0448, grad_fn=<AddBackward0>) loss\n",
      "0.7714975845410628\n",
      "iteration no 207\n",
      "tensor(4.8020, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9184, grad_fn=<AddBackward0>) loss\n",
      "0.771474358974359\n",
      "iteration no 208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.8020, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.8585, grad_fn=<AddBackward0>) loss\n",
      "0.7719298245614035\n",
      "iteration no 209\n",
      "tensor(4.8020, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0])\n",
      "tensor(9.9239, grad_fn=<AddBackward0>) loss\n",
      "0.7725396825396825\n",
      "iteration no 210\n",
      "tensor(4.8020, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 0])\n",
      "tensor(9.8991, grad_fn=<AddBackward0>) loss\n",
      "0.7726698262243286\n",
      "iteration no 211\n",
      "tensor(4.8020, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 1, 1])\n",
      "tensor(9.9489, grad_fn=<AddBackward0>) loss\n",
      "0.7727987421383647\n",
      "iteration no 212\n",
      "tensor(4.8020, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
      "        1, 0, 1, 1, 1, 1])\n",
      "tensor(9.9614, grad_fn=<AddBackward0>) loss\n",
      "0.7724569640062597\n",
      "iteration no 213\n",
      "tensor(4.8020, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0])\n",
      "tensor(9.9582, grad_fn=<AddBackward0>) loss\n",
      "0.7721183800623053\n",
      "iteration no 214\n",
      "tensor(4.8020, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9570, grad_fn=<AddBackward0>) loss\n",
      "0.7717829457364341\n",
      "iteration no 215\n",
      "tensor(4.8020, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 1, 0, 0])\n",
      "tensor(9.8889, grad_fn=<AddBackward0>) loss\n",
      "0.7720679012345679\n",
      "iteration no 216\n",
      "tensor(4.8020, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9605, grad_fn=<AddBackward0>) loss\n",
      "0.771889400921659\n",
      "iteration no 217\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 1, 0, 1])\n",
      "tensor(9.9787, grad_fn=<AddBackward0>) loss\n",
      "0.77217125382263\n",
      "iteration no 218\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9099, grad_fn=<AddBackward0>) loss\n",
      "0.7726027397260274\n",
      "iteration no 219\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
      "        1, 0, 0, 1, 0, 0])\n",
      "tensor(10.0831, grad_fn=<AddBackward0>) loss\n",
      "0.7718181818181818\n",
      "iteration no 220\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(10.0459, grad_fn=<AddBackward0>) loss\n",
      "0.771342383107089\n",
      "iteration no 221\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(10.1169, grad_fn=<AddBackward0>) loss\n",
      "0.7708708708708709\n",
      "iteration no 222\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9565, grad_fn=<AddBackward0>) loss\n",
      "0.7707025411061286\n",
      "iteration no 223\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.8057, grad_fn=<AddBackward0>) loss\n",
      "0.7709821428571428\n",
      "iteration no 224\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.9461, grad_fn=<AddBackward0>) loss\n",
      "0.7711111111111111\n",
      "iteration no 225\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
      "        0, 0, 1, 0, 1, 0])\n",
      "tensor(10.0818, grad_fn=<AddBackward0>) loss\n",
      "0.7707964601769911\n",
      "iteration no 226\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.8661, grad_fn=<AddBackward0>) loss\n",
      "0.7713656387665199\n",
      "iteration no 227\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
      "        0, 0, 0, 1, 1, 0])\n",
      "tensor(10.0144, grad_fn=<AddBackward0>) loss\n",
      "0.7707602339181286\n",
      "iteration no 228\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 1])\n",
      "tensor(9.8547, grad_fn=<AddBackward0>) loss\n",
      "0.77117903930131\n",
      "iteration no 229\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(10.0223, grad_fn=<AddBackward0>) loss\n",
      "0.7711594202898551\n",
      "iteration no 230\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9216, grad_fn=<AddBackward0>) loss\n",
      "0.7708513708513709\n",
      "iteration no 231\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8575, grad_fn=<AddBackward0>) loss\n",
      "0.771264367816092\n",
      "iteration no 232\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8992, grad_fn=<AddBackward0>) loss\n",
      "0.771244635193133\n",
      "iteration no 233\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9974, grad_fn=<AddBackward0>) loss\n",
      "0.771082621082621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 234\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 1, 0, 0])\n",
      "tensor(9.9138, grad_fn=<AddBackward0>) loss\n",
      "0.7710638297872341\n",
      "iteration no 235\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9757, grad_fn=<AddBackward0>) loss\n",
      "0.771045197740113\n",
      "iteration no 236\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 1, 1, 0, 0])\n",
      "tensor(10.0995, grad_fn=<AddBackward0>) loss\n",
      "0.7706047819971871\n",
      "iteration no 237\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 1, 0, 0, 1, 0])\n",
      "tensor(9.9846, grad_fn=<AddBackward0>) loss\n",
      "0.7704481792717087\n",
      "iteration no 238\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9464, grad_fn=<AddBackward0>) loss\n",
      "0.7705718270571827\n",
      "iteration no 239\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 0, 1, 1, 1])\n",
      "tensor(10.1158, grad_fn=<AddBackward0>) loss\n",
      "0.77\n",
      "iteration no 240\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(10.0668, grad_fn=<AddBackward0>) loss\n",
      "0.769432918395574\n",
      "iteration no 241\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8945, grad_fn=<AddBackward0>) loss\n",
      "0.7696969696969697\n",
      "iteration no 242\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(10.0108, grad_fn=<AddBackward0>) loss\n",
      "0.769684499314129\n",
      "iteration no 243\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9110, grad_fn=<AddBackward0>) loss\n",
      "0.7699453551912568\n",
      "iteration no 244\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(10.0049, grad_fn=<AddBackward0>) loss\n",
      "0.7696598639455783\n",
      "iteration no 245\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 1])\n",
      "tensor(10.0279, grad_fn=<AddBackward0>) loss\n",
      "0.7691056910569106\n",
      "iteration no 246\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(10.0175, grad_fn=<AddBackward0>) loss\n",
      "0.7685560053981106\n",
      "iteration no 247\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9089, grad_fn=<AddBackward0>) loss\n",
      "0.7688172043010753\n",
      "iteration no 248\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,\n",
      "        0, 1, 0, 1, 0, 0])\n",
      "tensor(10.0724, grad_fn=<AddBackward0>) loss\n",
      "0.7686746987951807\n",
      "iteration no 249\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.8909, grad_fn=<AddBackward0>) loss\n",
      "0.7689333333333334\n",
      "iteration no 250\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 1, 1, 1])\n",
      "tensor(9.9783, grad_fn=<AddBackward0>) loss\n",
      "0.7689243027888446\n",
      "iteration no 251\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9627, grad_fn=<AddBackward0>) loss\n",
      "0.7687830687830688\n",
      "iteration no 252\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.8463, grad_fn=<AddBackward0>) loss\n",
      "0.769038208168643\n",
      "iteration no 253\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.8947, grad_fn=<AddBackward0>) loss\n",
      "0.7694225721784776\n",
      "iteration no 254\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 1, 1, 0, 0, 0])\n",
      "tensor(9.9364, grad_fn=<AddBackward0>) loss\n",
      "0.7694117647058824\n",
      "iteration no 255\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8780, grad_fn=<AddBackward0>) loss\n",
      "0.7696614583333333\n",
      "iteration no 256\n",
      "tensor(4.8019, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "        0, 1, 0, 0, 0, 1])\n",
      "tensor(9.9415, grad_fn=<AddBackward0>) loss\n",
      "0.7697795071335928\n",
      "iteration no 257\n",
      "tensor(4.8018, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9337, grad_fn=<AddBackward0>) loss\n",
      "0.7700258397932817\n",
      "iteration no 258\n",
      "tensor(4.8018, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        1, 1, 0, 1, 0, 0])\n",
      "tensor(9.9108, grad_fn=<AddBackward0>) loss\n",
      "0.7701415701415701\n",
      "iteration no 259\n",
      "tensor(4.8018, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 1, 0, 0])\n",
      "tensor(9.9503, grad_fn=<AddBackward0>) loss\n",
      "0.77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 260\n",
      "tensor(4.8018, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(10.0286, grad_fn=<AddBackward0>) loss\n",
      "0.7694763729246488\n",
      "iteration no 261\n",
      "tensor(4.8018, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9858, grad_fn=<AddBackward0>) loss\n",
      "0.7695928753180662\n",
      "iteration no 262\n",
      "tensor(4.8018, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0])\n",
      "tensor(10.0143, grad_fn=<AddBackward0>) loss\n",
      "0.7697084917617237\n",
      "iteration no 263\n",
      "tensor(4.8018, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 0, 1, 0, 0, 1])\n",
      "tensor(10.0128, grad_fn=<AddBackward0>) loss\n",
      "0.7693181818181818\n",
      "iteration no 264\n",
      "tensor(4.8018, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
      "        1, 1, 0, 0, 1, 0])\n",
      "tensor(10.0569, grad_fn=<AddBackward0>) loss\n",
      "0.7690566037735849\n",
      "iteration no 265\n",
      "tensor(4.8018, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 1, 0])\n",
      "tensor(9.9639, grad_fn=<AddBackward0>) loss\n",
      "0.7690476190476191\n",
      "iteration no 266\n",
      "tensor(4.8018, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0])\n",
      "tensor(9.9771, grad_fn=<AddBackward0>) loss\n",
      "0.7690387016229713\n",
      "iteration no 267\n",
      "tensor(4.8018, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.8773, grad_fn=<AddBackward0>) loss\n",
      "0.7694029850746269\n",
      "iteration no 268\n",
      "tensor(4.8018, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 1, 1])\n",
      "tensor(9.9115, grad_fn=<AddBackward0>) loss\n",
      "0.7692688971499381\n",
      "iteration no 269\n",
      "tensor(4.8018, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        1, 1, 0, 1, 0, 0])\n",
      "tensor(9.9350, grad_fn=<AddBackward0>) loss\n",
      "0.7692592592592593\n",
      "iteration no 270\n",
      "tensor(4.8018, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "        1, 0, 1, 1, 0, 1])\n",
      "tensor(10.0175, grad_fn=<AddBackward0>) loss\n",
      "0.7690036900369004\n",
      "iteration no 271\n",
      "tensor(4.8018, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 1, 0, 0, 0])\n",
      "tensor(9.9679, grad_fn=<AddBackward0>) loss\n",
      "0.7692401960784314\n",
      "iteration no 272\n",
      "tensor(4.8018, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9647, grad_fn=<AddBackward0>) loss\n",
      "0.7693528693528694\n",
      "iteration no 273\n",
      "tensor(4.8018, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1, 0, 1, 0, 0])\n",
      "tensor(9.8106, grad_fn=<AddBackward0>) loss\n",
      "0.7698296836982969\n",
      "iteration no 274\n",
      "tensor(4.8018, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 1, 1, 1])\n",
      "tensor(9.9446, grad_fn=<AddBackward0>) loss\n",
      "0.7696969696969697\n",
      "iteration no 275\n",
      "tensor(4.8018, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(10.0197, grad_fn=<AddBackward0>) loss\n",
      "0.7694444444444445\n",
      "iteration no 276\n",
      "tensor(4.8018, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 1, 0, 1, 1, 0])\n",
      "tensor(9.9781, grad_fn=<AddBackward0>) loss\n",
      "0.7696750902527075\n",
      "iteration no 277\n",
      "tensor(4.8018, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        1, 1, 0, 1, 1, 0])\n",
      "tensor(10.1700, grad_fn=<AddBackward0>) loss\n",
      "0.7690647482014389\n",
      "iteration no 278\n",
      "tensor(4.8018, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.8932, grad_fn=<AddBackward0>) loss\n",
      "0.769295101553166\n",
      "iteration no 279\n",
      "tensor(4.8018, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 0])\n",
      "tensor(9.8655, grad_fn=<AddBackward0>) loss\n",
      "0.7694047619047619\n",
      "iteration no 280\n",
      "tensor(4.8017, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 1, 1])\n",
      "tensor(9.8764, grad_fn=<AddBackward0>) loss\n",
      "0.7697508896797153\n",
      "iteration no 281\n",
      "tensor(4.8017, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0])\n",
      "tensor(9.9994, grad_fn=<AddBackward0>) loss\n",
      "0.7698581560283688\n",
      "iteration no 282\n",
      "tensor(4.8017, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 1, 0, 0, 0, 1])\n",
      "tensor(9.9389, grad_fn=<AddBackward0>) loss\n",
      "0.7698468786808009\n",
      "iteration no 283\n",
      "tensor(4.8017, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 1])\n",
      "tensor(10.0977, grad_fn=<AddBackward0>) loss\n",
      "0.7694835680751174\n",
      "iteration no 284\n",
      "tensor(4.8017, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9991, grad_fn=<AddBackward0>) loss\n",
      "0.7693567251461988\n",
      "iteration no 285\n",
      "tensor(4.8017, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 1])\n",
      "tensor(10.0576, grad_fn=<AddBackward0>) loss\n",
      "0.7691142191142191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 286\n",
      "tensor(4.8017, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9247, grad_fn=<AddBackward0>) loss\n",
      "0.7694541231126597\n",
      "iteration no 287\n",
      "tensor(4.8017, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.9161, grad_fn=<AddBackward0>) loss\n",
      "0.7694444444444445\n",
      "iteration no 288\n",
      "tensor(4.8017, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
      "        1, 1, 1, 1, 0, 0])\n",
      "tensor(9.9772, grad_fn=<AddBackward0>) loss\n",
      "0.769434832756632\n",
      "iteration no 289\n",
      "tensor(4.8017, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 1, 1, 0, 0, 0])\n",
      "tensor(10.0069, grad_fn=<AddBackward0>) loss\n",
      "0.7691954022988505\n",
      "iteration no 290\n",
      "tensor(4.8017, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(10.0690, grad_fn=<AddBackward0>) loss\n",
      "0.7688430698739978\n",
      "iteration no 291\n",
      "tensor(4.8017, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9524, grad_fn=<AddBackward0>) loss\n",
      "0.7689497716894977\n",
      "iteration no 292\n",
      "tensor(4.8017, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
      "        1, 0, 0, 0, 0, 1])\n",
      "tensor(10.0275, grad_fn=<AddBackward0>) loss\n",
      "0.7689419795221843\n",
      "iteration no 293\n",
      "tensor(4.8017, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9736, grad_fn=<AddBackward0>) loss\n",
      "0.7691609977324263\n",
      "iteration no 294\n",
      "tensor(4.8017, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 1, 0, 0, 0, 1])\n",
      "tensor(9.9345, grad_fn=<AddBackward0>) loss\n",
      "0.7690395480225989\n",
      "iteration no 295\n",
      "tensor(4.8017, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0])\n",
      "tensor(9.9084, grad_fn=<AddBackward0>) loss\n",
      "0.7692567567567568\n",
      "iteration no 296\n",
      "tensor(4.8017, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8760, grad_fn=<AddBackward0>) loss\n",
      "0.7693602693602694\n",
      "iteration no 297\n",
      "tensor(4.8017, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.8502, grad_fn=<AddBackward0>) loss\n",
      "0.7695749440715883\n",
      "iteration no 298\n",
      "tensor(4.8017, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n",
      "        0, 0, 0, 1, 1, 0])\n",
      "tensor(9.9042, grad_fn=<AddBackward0>) loss\n",
      "0.769453734671126\n",
      "iteration no 299\n",
      "tensor(4.8017, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 1])\n",
      "tensor(9.9591, grad_fn=<AddBackward0>) loss\n",
      "0.7696666666666667\n",
      "iteration no 300\n",
      "tensor(4.8017, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n",
      "        1, 0, 0, 0, 0, 1])\n",
      "tensor(10.0558, grad_fn=<AddBackward0>) loss\n",
      "0.7692137320044297\n",
      "iteration no 301\n",
      "tensor(4.8017, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
      "        0, 1, 1, 0, 0, 0])\n",
      "tensor(10.0885, grad_fn=<AddBackward0>) loss\n",
      "0.7687637969094923\n",
      "iteration no 302\n",
      "tensor(4.8017, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.9295, grad_fn=<AddBackward0>) loss\n",
      "0.768976897689769\n",
      "iteration no 303\n",
      "tensor(4.8017, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8937, grad_fn=<AddBackward0>) loss\n",
      "0.7690789473684211\n",
      "iteration no 304\n",
      "tensor(4.8017, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 1])\n",
      "tensor(10.0319, grad_fn=<AddBackward0>) loss\n",
      "0.7689617486338798\n",
      "iteration no 305\n",
      "tensor(4.8017, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9267, grad_fn=<AddBackward0>) loss\n",
      "0.7690631808278867\n",
      "iteration no 306\n",
      "tensor(4.8017, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.8998, grad_fn=<AddBackward0>) loss\n",
      "0.7692725298588491\n",
      "iteration no 307\n",
      "tensor(4.8017, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1])\n",
      "tensor(9.8268, grad_fn=<AddBackward0>) loss\n",
      "0.7695887445887446\n",
      "iteration no 308\n",
      "tensor(4.8017, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 1, 0])\n",
      "tensor(9.8198, grad_fn=<AddBackward0>) loss\n",
      "0.7699029126213592\n",
      "iteration no 309\n",
      "tensor(4.8016, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1, 1, 0, 0, 0])\n",
      "tensor(10.0737, grad_fn=<AddBackward0>) loss\n",
      "0.7694623655913978\n",
      "iteration no 310\n",
      "tensor(4.8016, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9320, grad_fn=<AddBackward0>) loss\n",
      "0.7695605573419079\n",
      "iteration no 311\n",
      "tensor(4.8016, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 1, 1, 1, 0, 0])\n",
      "tensor(10.0860, grad_fn=<AddBackward0>) loss\n",
      "0.7688034188034188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 312\n",
      "tensor(4.8016, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9289, grad_fn=<AddBackward0>) loss\n",
      "0.7689030883919062\n",
      "iteration no 313\n",
      "tensor(4.8016, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9217, grad_fn=<AddBackward0>) loss\n",
      "0.7691082802547771\n",
      "iteration no 314\n",
      "tensor(4.8016, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8653, grad_fn=<AddBackward0>) loss\n",
      "0.7694179894179894\n",
      "iteration no 315\n",
      "tensor(4.8016, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 1])\n",
      "tensor(9.9267, grad_fn=<AddBackward0>) loss\n",
      "0.7691983122362869\n",
      "iteration no 316\n",
      "tensor(4.8016, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9036, grad_fn=<AddBackward0>) loss\n",
      "0.7692954784437435\n",
      "iteration no 317\n",
      "tensor(4.8016, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9880, grad_fn=<AddBackward0>) loss\n",
      "0.769182389937107\n",
      "iteration no 318\n",
      "tensor(4.8016, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.8759, grad_fn=<AddBackward0>) loss\n",
      "0.7693834900731452\n",
      "iteration no 319\n",
      "tensor(4.8016, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9468, grad_fn=<AddBackward0>) loss\n",
      "0.769375\n",
      "iteration no 320\n",
      "tensor(4.8016, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.9221, grad_fn=<AddBackward0>) loss\n",
      "0.7693665628245068\n",
      "iteration no 321\n",
      "tensor(4.8016, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1, 1, 0])\n",
      "tensor(9.9342, grad_fn=<AddBackward0>) loss\n",
      "0.7693581780538302\n",
      "iteration no 322\n",
      "tensor(4.8016, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(10.0390, grad_fn=<AddBackward0>) loss\n",
      "0.7692466460268318\n",
      "iteration no 323\n",
      "tensor(4.8016, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(10.0031, grad_fn=<AddBackward0>) loss\n",
      "0.769238683127572\n",
      "iteration no 324\n",
      "tensor(4.8016, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0, 0, 1])\n",
      "tensor(10.0503, grad_fn=<AddBackward0>) loss\n",
      "0.7691282051282051\n",
      "iteration no 325\n",
      "tensor(4.8016, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(10.0097, grad_fn=<AddBackward0>) loss\n",
      "0.7689161554192229\n",
      "iteration no 326\n",
      "tensor(4.8016, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 1, 0, 0])\n",
      "tensor(9.9606, grad_fn=<AddBackward0>) loss\n",
      "0.7692150866462794\n",
      "iteration no 327\n",
      "tensor(4.8016, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,\n",
      "        1, 0, 1, 0, 1, 0])\n",
      "tensor(10.1137, grad_fn=<AddBackward0>) loss\n",
      "0.7688008130081301\n",
      "iteration no 328\n",
      "tensor(4.8016, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.8451, grad_fn=<AddBackward0>) loss\n",
      "0.7689969604863222\n",
      "iteration no 329\n",
      "tensor(4.8016, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0])\n",
      "tensor(10.0002, grad_fn=<AddBackward0>) loss\n",
      "0.7687878787878788\n",
      "iteration no 330\n",
      "tensor(4.8016, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8151, grad_fn=<AddBackward0>) loss\n",
      "0.7691842900302115\n",
      "iteration no 331\n",
      "tensor(4.8016, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.8663, grad_fn=<AddBackward0>) loss\n",
      "0.7695783132530121\n",
      "iteration no 332\n",
      "tensor(4.8016, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.8898, grad_fn=<AddBackward0>) loss\n",
      "0.7700700700700701\n",
      "iteration no 333\n",
      "tensor(4.8016, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9594, grad_fn=<AddBackward0>) loss\n",
      "0.770059880239521\n",
      "iteration no 334\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 1, 0, 0, 1, 0])\n",
      "tensor(9.9378, grad_fn=<AddBackward0>) loss\n",
      "0.7699502487562189\n",
      "iteration no 335\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1])\n",
      "tensor(9.9919, grad_fn=<AddBackward0>) loss\n",
      "0.7698412698412699\n",
      "iteration no 336\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0])\n",
      "tensor(9.8763, grad_fn=<AddBackward0>) loss\n",
      "0.7700296735905044\n",
      "iteration no 337\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.9726, grad_fn=<AddBackward0>) loss\n",
      "0.7701183431952663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 338\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(10.0227, grad_fn=<AddBackward0>) loss\n",
      "0.7703048180924287\n",
      "iteration no 339\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9755, grad_fn=<AddBackward0>) loss\n",
      "0.7701960784313725\n",
      "iteration no 340\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.7447, grad_fn=<AddBackward0>) loss\n",
      "0.7705767350928642\n",
      "iteration no 341\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(10.0367, grad_fn=<AddBackward0>) loss\n",
      "0.7707602339181286\n",
      "iteration no 342\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 1, 0, 1, 1])\n",
      "tensor(9.9785, grad_fn=<AddBackward0>) loss\n",
      "0.7707482993197279\n",
      "iteration no 343\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
      "        0, 1, 0, 1, 1, 0])\n",
      "tensor(9.9570, grad_fn=<AddBackward0>) loss\n",
      "0.7705426356589147\n",
      "iteration no 344\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9775, grad_fn=<AddBackward0>) loss\n",
      "0.7704347826086957\n",
      "iteration no 345\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9630, grad_fn=<AddBackward0>) loss\n",
      "0.7705202312138728\n",
      "iteration no 346\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.8917, grad_fn=<AddBackward0>) loss\n",
      "0.7707012487992315\n",
      "iteration no 347\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,\n",
      "        0, 0, 1, 1, 0, 0])\n",
      "tensor(10.0206, grad_fn=<AddBackward0>) loss\n",
      "0.7704022988505748\n",
      "iteration no 348\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n",
      "        1, 0, 1, 0, 1, 0])\n",
      "tensor(10.3315, grad_fn=<AddBackward0>) loss\n",
      "0.76981852913085\n",
      "iteration no 349\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8037, grad_fn=<AddBackward0>) loss\n",
      "0.7703809523809524\n",
      "iteration no 350\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8551, grad_fn=<AddBackward0>) loss\n",
      "0.7706552706552706\n",
      "iteration no 351\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
      "        1, 0, 0, 0, 1, 0])\n",
      "tensor(9.9748, grad_fn=<AddBackward0>) loss\n",
      "0.7706439393939394\n",
      "iteration no 352\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9795, grad_fn=<AddBackward0>) loss\n",
      "0.7707271010387158\n",
      "iteration no 353\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "        1, 1, 1, 0, 0, 0])\n",
      "tensor(10.0289, grad_fn=<AddBackward0>) loss\n",
      "0.7708097928436911\n",
      "iteration no 354\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8746, grad_fn=<AddBackward0>) loss\n",
      "0.7708920187793428\n",
      "iteration no 355\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
      "        0, 0, 1, 1, 1, 1])\n",
      "tensor(10.0185, grad_fn=<AddBackward0>) loss\n",
      "0.7707865168539326\n",
      "iteration no 356\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 1, 0, 1, 0, 0])\n",
      "tensor(9.9087, grad_fn=<AddBackward0>) loss\n",
      "0.7709617180205416\n",
      "iteration no 357\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9146, grad_fn=<AddBackward0>) loss\n",
      "0.7710428305400372\n",
      "iteration no 358\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
      "        0, 1, 0, 0, 0, 1])\n",
      "tensor(9.8421, grad_fn=<AddBackward0>) loss\n",
      "0.7711234911792015\n",
      "iteration no 359\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(10.0543, grad_fn=<AddBackward0>) loss\n",
      "0.7709259259259259\n",
      "iteration no 360\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(10.0615, grad_fn=<AddBackward0>) loss\n",
      "0.7708217913204063\n",
      "iteration no 361\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(10.0125, grad_fn=<AddBackward0>) loss\n",
      "0.7705340699815838\n",
      "iteration no 362\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(10.0063, grad_fn=<AddBackward0>) loss\n",
      "0.7702479338842976\n",
      "iteration no 363\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 1, 0])\n",
      "tensor(9.8685, grad_fn=<AddBackward0>) loss\n",
      "0.7702380952380953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 364\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(10.0172, grad_fn=<AddBackward0>) loss\n",
      "0.7700456621004567\n",
      "iteration no 365\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 1, 0, 1])\n",
      "tensor(9.9130, grad_fn=<AddBackward0>) loss\n",
      "0.770127504553734\n",
      "iteration no 366\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 1, 1, 0])\n",
      "tensor(9.9743, grad_fn=<AddBackward0>) loss\n",
      "0.7701180744777475\n",
      "iteration no 367\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 1, 1])\n",
      "tensor(10.0992, grad_fn=<AddBackward0>) loss\n",
      "0.7697463768115942\n",
      "iteration no 368\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1])\n",
      "tensor(10.0222, grad_fn=<AddBackward0>) loss\n",
      "0.7696476964769647\n",
      "iteration no 369\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.9272, grad_fn=<AddBackward0>) loss\n",
      "0.7695495495495496\n",
      "iteration no 370\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(10.0175, grad_fn=<AddBackward0>) loss\n",
      "0.7695417789757413\n",
      "iteration no 371\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 1, 0, 0, 0])\n",
      "tensor(9.9753, grad_fn=<AddBackward0>) loss\n",
      "0.7695340501792115\n",
      "iteration no 372\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9660, grad_fn=<AddBackward0>) loss\n",
      "0.769615728328865\n",
      "iteration no 373\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9728, grad_fn=<AddBackward0>) loss\n",
      "0.7696078431372549\n",
      "iteration no 374\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9902, grad_fn=<AddBackward0>) loss\n",
      "0.7694222222222222\n",
      "iteration no 375\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(10.0068, grad_fn=<AddBackward0>) loss\n",
      "0.7693262411347518\n",
      "iteration no 376\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8623, grad_fn=<AddBackward0>) loss\n",
      "0.7694960212201591\n",
      "iteration no 377\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8657, grad_fn=<AddBackward0>) loss\n",
      "0.769753086419753\n",
      "iteration no 378\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,\n",
      "        0, 1, 0, 0, 1, 0])\n",
      "tensor(10.0923, grad_fn=<AddBackward0>) loss\n",
      "0.7693931398416887\n",
      "iteration no 379\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
      "        1, 1, 0, 0, 0, 0])\n",
      "tensor(10.0142, grad_fn=<AddBackward0>) loss\n",
      "0.7690350877192983\n",
      "iteration no 380\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,\n",
      "        1, 1, 0, 1, 0, 0])\n",
      "tensor(10.0679, grad_fn=<AddBackward0>) loss\n",
      "0.768678915135608\n",
      "iteration no 381\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8768, grad_fn=<AddBackward0>) loss\n",
      "0.768760907504363\n",
      "iteration no 382\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 1, 1, 0])\n",
      "tensor(9.9974, grad_fn=<AddBackward0>) loss\n",
      "0.768668407310705\n",
      "iteration no 383\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9190, grad_fn=<AddBackward0>) loss\n",
      "0.76875\n",
      "iteration no 384\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "        1, 1, 0, 0, 0, 1])\n",
      "tensor(10.0049, grad_fn=<AddBackward0>) loss\n",
      "0.7685714285714286\n",
      "iteration no 385\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9189, grad_fn=<AddBackward0>) loss\n",
      "0.7684801381692573\n",
      "iteration no 386\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.8411, grad_fn=<AddBackward0>) loss\n",
      "0.7685615848406546\n",
      "iteration no 387\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8010, grad_fn=<AddBackward0>) loss\n",
      "0.7689862542955327\n",
      "iteration no 388\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.8657, grad_fn=<AddBackward0>) loss\n",
      "0.7691516709511568\n",
      "iteration no 389\n",
      "tensor(4.8015, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 0, 0])\n",
      "tensor(9.8893, grad_fn=<AddBackward0>) loss\n",
      "0.7692307692307693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 390\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.8579, grad_fn=<AddBackward0>) loss\n",
      "0.769309462915601\n",
      "iteration no 391\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 0, 0])\n",
      "tensor(9.8482, grad_fn=<AddBackward0>) loss\n",
      "0.7694727891156462\n",
      "iteration no 392\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n",
      "        1, 1, 1, 0, 0, 0])\n",
      "tensor(10.1088, grad_fn=<AddBackward0>) loss\n",
      "0.7692111959287532\n",
      "iteration no 393\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 1, 0])\n",
      "tensor(10.0140, grad_fn=<AddBackward0>) loss\n",
      "0.7689509306260576\n",
      "iteration no 394\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 1, 0, 1])\n",
      "tensor(10.0405, grad_fn=<AddBackward0>) loss\n",
      "0.7685232067510549\n",
      "iteration no 395\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 1, 0, 0, 0])\n",
      "tensor(9.9780, grad_fn=<AddBackward0>) loss\n",
      "0.7683501683501683\n",
      "iteration no 396\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 1, 0, 0])\n",
      "tensor(9.9142, grad_fn=<AddBackward0>) loss\n",
      "0.7685138539042821\n",
      "iteration no 397\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9273, grad_fn=<AddBackward0>) loss\n",
      "0.7685092127303182\n",
      "iteration no 398\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 1, 0, 1, 0, 0])\n",
      "tensor(9.9867, grad_fn=<AddBackward0>) loss\n",
      "0.7685045948203842\n",
      "iteration no 399\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 1, 0])\n",
      "tensor(10.0644, grad_fn=<AddBackward0>) loss\n",
      "0.76825\n",
      "iteration no 400\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9362, grad_fn=<AddBackward0>) loss\n",
      "0.7683291770573566\n",
      "iteration no 401\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 1, 0, 0, 1])\n",
      "tensor(9.9858, grad_fn=<AddBackward0>) loss\n",
      "0.7684079601990049\n",
      "iteration no 402\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
      "        1, 0, 1, 0, 0, 0])\n",
      "tensor(10.0639, grad_fn=<AddBackward0>) loss\n",
      "0.7682382133995037\n",
      "iteration no 403\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9067, grad_fn=<AddBackward0>) loss\n",
      "0.7684818481848185\n",
      "iteration no 404\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1])\n",
      "tensor(9.8974, grad_fn=<AddBackward0>) loss\n",
      "0.7684773662551441\n",
      "iteration no 405\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.9018, grad_fn=<AddBackward0>) loss\n",
      "0.7685550082101806\n",
      "iteration no 406\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 1, 0, 1, 0, 0])\n",
      "tensor(9.9333, grad_fn=<AddBackward0>) loss\n",
      "0.7686322686322686\n",
      "iteration no 407\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9340, grad_fn=<AddBackward0>) loss\n",
      "0.7684640522875817\n",
      "iteration no 408\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0])\n",
      "tensor(9.9836, grad_fn=<AddBackward0>) loss\n",
      "0.7685411572942136\n",
      "iteration no 409\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.8933, grad_fn=<AddBackward0>) loss\n",
      "0.7685365853658537\n",
      "iteration no 410\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 1, 1, 0])\n",
      "tensor(10.0080, grad_fn=<AddBackward0>) loss\n",
      "0.7684509326845094\n",
      "iteration no 411\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 0])\n",
      "tensor(10.0896, grad_fn=<AddBackward0>) loss\n",
      "0.7681229773462783\n",
      "iteration no 412\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
      "        1, 0, 1, 1, 0, 0])\n",
      "tensor(10.0999, grad_fn=<AddBackward0>) loss\n",
      "0.7678773204196933\n",
      "iteration no 413\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9685, grad_fn=<AddBackward0>) loss\n",
      "0.7678743961352656\n",
      "iteration no 414\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9105, grad_fn=<AddBackward0>) loss\n",
      "0.7681124497991968\n",
      "iteration no 415\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1])\n",
      "tensor(10.0651, grad_fn=<AddBackward0>) loss\n",
      "0.767948717948718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 416\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9315, grad_fn=<AddBackward0>) loss\n",
      "0.7678657074340528\n",
      "iteration no 417\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
      "        0, 0, 1, 1, 0, 0])\n",
      "tensor(9.9607, grad_fn=<AddBackward0>) loss\n",
      "0.7677033492822967\n",
      "iteration no 418\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8357, grad_fn=<AddBackward0>) loss\n",
      "0.7680190930787589\n",
      "iteration no 419\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8628, grad_fn=<AddBackward0>) loss\n",
      "0.7681746031746032\n",
      "iteration no 420\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 1, 0, 0])\n",
      "tensor(9.8920, grad_fn=<AddBackward0>) loss\n",
      "0.7680918448139351\n",
      "iteration no 421\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9316, grad_fn=<AddBackward0>) loss\n",
      "0.7682464454976303\n",
      "iteration no 422\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.8187, grad_fn=<AddBackward0>) loss\n",
      "0.7684791174152876\n",
      "iteration no 423\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8285, grad_fn=<AddBackward0>) loss\n",
      "0.7688679245283019\n",
      "iteration no 424\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 1, 0, 0, 1])\n",
      "tensor(9.9329, grad_fn=<AddBackward0>) loss\n",
      "0.7687058823529411\n",
      "iteration no 425\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 1])\n",
      "tensor(9.9420, grad_fn=<AddBackward0>) loss\n",
      "0.7687793427230047\n",
      "iteration no 426\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(10.0220, grad_fn=<AddBackward0>) loss\n",
      "0.768696330991413\n",
      "iteration no 427\n",
      "tensor(4.8014, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8294, grad_fn=<AddBackward0>) loss\n",
      "0.7689252336448598\n",
      "iteration no 428\n",
      "tensor(4.8013, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9161, grad_fn=<AddBackward0>) loss\n",
      "0.768997668997669\n",
      "iteration no 429\n",
      "tensor(4.8013, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9423, grad_fn=<AddBackward0>) loss\n",
      "0.7689922480620155\n",
      "iteration no 430\n",
      "tensor(4.8013, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n",
      "        0, 1, 0, 1, 0, 0])\n",
      "tensor(10.0591, grad_fn=<AddBackward0>) loss\n",
      "0.7688321732405259\n",
      "iteration no 431\n",
      "tensor(4.8013, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9529, grad_fn=<AddBackward0>) loss\n",
      "0.7688271604938272\n",
      "iteration no 432\n",
      "tensor(4.8013, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 0])\n",
      "tensor(10.0255, grad_fn=<AddBackward0>) loss\n",
      "0.7686682063125481\n",
      "iteration no 433\n",
      "tensor(4.8013, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8112, grad_fn=<AddBackward0>) loss\n",
      "0.7687403993855607\n",
      "iteration no 434\n",
      "tensor(4.8013, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 1, 0, 0, 1])\n",
      "tensor(10.0373, grad_fn=<AddBackward0>) loss\n",
      "0.7685823754789272\n",
      "iteration no 435\n",
      "tensor(4.8013, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9967, grad_fn=<AddBackward0>) loss\n",
      "0.7684250764525994\n",
      "iteration no 436\n",
      "tensor(4.8013, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9370, grad_fn=<AddBackward0>) loss\n",
      "0.7685736079328757\n",
      "iteration no 437\n",
      "tensor(4.8013, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0])\n",
      "tensor(9.8354, grad_fn=<AddBackward0>) loss\n",
      "0.7687214611872146\n",
      "iteration no 438\n",
      "tensor(4.8013, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 0, 1, 1, 0, 0])\n",
      "tensor(10.0064, grad_fn=<AddBackward0>) loss\n",
      "0.7686408504176158\n",
      "iteration no 439\n",
      "tensor(4.8013, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 1, 0, 1])\n",
      "tensor(9.9524, grad_fn=<AddBackward0>) loss\n",
      "0.768560606060606\n",
      "iteration no 440\n",
      "tensor(4.8013, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9979, grad_fn=<AddBackward0>) loss\n",
      "0.7683295540438397\n",
      "iteration no 441\n",
      "tensor(4.8013, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(10.0074, grad_fn=<AddBackward0>) loss\n",
      "0.7682503770739065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 442\n",
      "tensor(4.8013, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(10.0249, grad_fn=<AddBackward0>) loss\n",
      "0.7680963130173063\n",
      "iteration no 443\n",
      "tensor(4.8013, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8722, grad_fn=<AddBackward0>) loss\n",
      "0.7682432432432432\n",
      "iteration no 444\n",
      "tensor(4.8013, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(10.1056, grad_fn=<AddBackward0>) loss\n",
      "0.7679400749063671\n",
      "iteration no 445\n",
      "tensor(4.8013, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 1, 0, 0])\n",
      "tensor(10.0459, grad_fn=<AddBackward0>) loss\n",
      "0.7678624813153961\n",
      "iteration no 446\n",
      "tensor(4.8013, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
      "        1, 1, 0, 0, 0, 0])\n",
      "tensor(9.9608, grad_fn=<AddBackward0>) loss\n",
      "0.7679343773303505\n",
      "iteration no 447\n",
      "tensor(4.8013, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9262, grad_fn=<AddBackward0>) loss\n",
      "0.7680059523809524\n",
      "iteration no 448\n",
      "tensor(4.8013, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8718, grad_fn=<AddBackward0>) loss\n",
      "0.7681514476614699\n",
      "iteration no 449\n",
      "tensor(4.8013, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 0, 1, 0, 1])\n",
      "tensor(9.9137, grad_fn=<AddBackward0>) loss\n",
      "0.768\n",
      "iteration no 450\n",
      "tensor(4.8013, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9471, grad_fn=<AddBackward0>) loss\n",
      "0.7680709534368071\n",
      "iteration no 451\n",
      "tensor(4.8013, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 1])\n",
      "tensor(9.9744, grad_fn=<AddBackward0>) loss\n",
      "0.7680678466076696\n",
      "iteration no 452\n",
      "tensor(4.8013, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 1, 0, 1, 0, 1])\n",
      "tensor(9.9389, grad_fn=<AddBackward0>) loss\n",
      "0.7679911699779249\n",
      "iteration no 453\n",
      "tensor(4.8013, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 1])\n",
      "tensor(10.0879, grad_fn=<AddBackward0>) loss\n",
      "0.76784140969163\n",
      "iteration no 454\n",
      "tensor(4.8013, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9255, grad_fn=<AddBackward0>) loss\n",
      "0.7678388278388278\n",
      "iteration no 455\n",
      "tensor(4.8013, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(10.0113, grad_fn=<AddBackward0>) loss\n",
      "0.7676900584795322\n",
      "iteration no 456\n",
      "tensor(4.8013, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8211, grad_fn=<AddBackward0>) loss\n",
      "0.7679795769511306\n",
      "iteration no 457\n",
      "tensor(4.8013, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
      "        0, 0, 1, 0, 1, 0])\n",
      "tensor(9.9813, grad_fn=<AddBackward0>) loss\n",
      "0.7678311499272198\n",
      "iteration no 458\n",
      "tensor(4.8013, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.9596, grad_fn=<AddBackward0>) loss\n",
      "0.7679738562091504\n",
      "iteration no 459\n",
      "tensor(4.8013, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.7905, grad_fn=<AddBackward0>) loss\n",
      "0.7682608695652174\n",
      "iteration no 460\n",
      "tensor(4.8012, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9375, grad_fn=<AddBackward0>) loss\n",
      "0.7684020245842371\n",
      "iteration no 461\n",
      "tensor(4.8012, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n",
      "        1, 0, 1, 1, 0, 1])\n",
      "tensor(10.1164, grad_fn=<AddBackward0>) loss\n",
      "0.7680375180375181\n",
      "iteration no 462\n",
      "tensor(4.8012, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8776, grad_fn=<AddBackward0>) loss\n",
      "0.7681785457163427\n",
      "iteration no 463\n",
      "tensor(4.8012, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 0])\n",
      "tensor(9.8077, grad_fn=<AddBackward0>) loss\n",
      "0.7683908045977011\n",
      "iteration no 464\n",
      "tensor(4.8012, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.9636, grad_fn=<AddBackward0>) loss\n",
      "0.7683154121863799\n",
      "iteration no 465\n",
      "tensor(4.8012, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.8792, grad_fn=<AddBackward0>) loss\n",
      "0.7685264663805437\n",
      "iteration no 466\n",
      "tensor(4.8012, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
      "        1, 0, 0, 0, 0, 1])\n",
      "tensor(10.1703, grad_fn=<AddBackward0>) loss\n",
      "0.7684511063526053\n",
      "iteration no 467\n",
      "tensor(4.8012, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
      "        0, 1, 0, 1, 1, 0])\n",
      "tensor(9.9733, grad_fn=<AddBackward0>) loss\n",
      "0.7685185185185185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 468\n",
      "tensor(4.8012, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.8719, grad_fn=<AddBackward0>) loss\n",
      "0.768727789623312\n",
      "iteration no 469\n",
      "tensor(4.8012, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9671, grad_fn=<AddBackward0>) loss\n",
      "0.768581560283688\n",
      "iteration no 470\n",
      "tensor(4.8012, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9458, grad_fn=<AddBackward0>) loss\n",
      "0.7685774946921444\n",
      "iteration no 471\n",
      "tensor(4.8012, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 1, 1])\n",
      "tensor(10.2202, grad_fn=<AddBackward0>) loss\n",
      "0.7682203389830509\n",
      "iteration no 472\n",
      "tensor(4.8012, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 1, 1, 0, 0])\n",
      "tensor(9.9234, grad_fn=<AddBackward0>) loss\n",
      "0.7682875264270613\n",
      "iteration no 473\n",
      "tensor(4.8012, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.9893, grad_fn=<AddBackward0>) loss\n",
      "0.7682137834036569\n",
      "iteration no 474\n",
      "tensor(4.8012, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
      "        1, 1, 0, 0, 0, 0])\n",
      "tensor(9.9055, grad_fn=<AddBackward0>) loss\n",
      "0.768280701754386\n",
      "iteration no 475\n",
      "tensor(4.8012, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 1])\n",
      "tensor(9.9485, grad_fn=<AddBackward0>) loss\n",
      "0.7684173669467788\n",
      "iteration no 476\n",
      "tensor(4.8012, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n",
      "        1, 0, 0, 0, 1, 0])\n",
      "tensor(10.0581, grad_fn=<AddBackward0>) loss\n",
      "0.7680642907058002\n",
      "iteration no 477\n",
      "tensor(4.8012, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9034, grad_fn=<AddBackward0>) loss\n",
      "0.7682008368200837\n",
      "iteration no 478\n",
      "tensor(4.8012, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 1])\n",
      "tensor(9.9389, grad_fn=<AddBackward0>) loss\n",
      "0.7682672233820459\n",
      "iteration no 479\n",
      "tensor(4.8012, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 1])\n",
      "tensor(9.8425, grad_fn=<AddBackward0>) loss\n",
      "0.7684027777777778\n",
      "iteration no 480\n",
      "tensor(4.8012, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9422, grad_fn=<AddBackward0>) loss\n",
      "0.7683991683991684\n",
      "iteration no 481\n",
      "tensor(4.8012, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        1, 1, 1, 0, 0, 0])\n",
      "tensor(9.9822, grad_fn=<AddBackward0>) loss\n",
      "0.7681881051175657\n",
      "iteration no 482\n",
      "tensor(4.8012, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
      "        1, 0, 1, 0, 0, 0])\n",
      "tensor(9.9526, grad_fn=<AddBackward0>) loss\n",
      "0.7681159420289855\n",
      "iteration no 483\n",
      "tensor(4.8012, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
      "        0, 0, 0, 0, 1, 1])\n",
      "tensor(10.1618, grad_fn=<AddBackward0>) loss\n",
      "0.7678374655647383\n",
      "iteration no 484\n",
      "tensor(4.8012, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 0])\n",
      "tensor(10.0922, grad_fn=<AddBackward0>) loss\n",
      "0.7676288659793814\n",
      "iteration no 485\n",
      "tensor(4.8012, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8783, grad_fn=<AddBackward0>) loss\n",
      "0.7678326474622771\n",
      "iteration no 486\n",
      "tensor(4.8012, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 1])\n",
      "tensor(9.8923, grad_fn=<AddBackward0>) loss\n",
      "0.7677618069815195\n",
      "iteration no 487\n",
      "tensor(4.8012, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8890, grad_fn=<AddBackward0>) loss\n",
      "0.767896174863388\n",
      "iteration no 488\n",
      "tensor(4.8012, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9281, grad_fn=<AddBackward0>) loss\n",
      "0.7678936605316974\n",
      "iteration no 489\n",
      "tensor(4.8012, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8629, grad_fn=<AddBackward0>) loss\n",
      "0.7679591836734694\n",
      "iteration no 490\n",
      "tensor(4.8012, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.8944, grad_fn=<AddBackward0>) loss\n",
      "0.768092328581127\n",
      "iteration no 491\n",
      "tensor(4.8012, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0])\n",
      "tensor(9.9201, grad_fn=<AddBackward0>) loss\n",
      "0.7680216802168022\n",
      "iteration no 492\n",
      "tensor(4.8012, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
      "        1, 0, 1, 0, 0, 1])\n",
      "tensor(10.0606, grad_fn=<AddBackward0>) loss\n",
      "0.7677484787018256\n",
      "iteration no 493\n",
      "tensor(4.8011, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 1, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.9641, grad_fn=<AddBackward0>) loss\n",
      "0.7676788124156545\n",
      "iteration no 494\n",
      "tensor(4.8011, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.9108, grad_fn=<AddBackward0>) loss\n",
      "0.7679461279461279\n",
      "iteration no 495\n",
      "tensor(4.8011, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        1, 1, 0, 0, 1, 0])\n",
      "tensor(9.9203, grad_fn=<AddBackward0>) loss\n",
      "0.7679435483870968\n",
      "iteration no 496\n",
      "tensor(4.8011, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 1, 0])\n",
      "tensor(9.9048, grad_fn=<AddBackward0>) loss\n",
      "0.7680751173708921\n",
      "iteration no 497\n",
      "tensor(4.8011, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9188, grad_fn=<AddBackward0>) loss\n",
      "0.7680053547523427\n",
      "iteration no 498\n",
      "tensor(4.8011, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 0, 0])\n",
      "tensor(9.9923, grad_fn=<AddBackward0>) loss\n",
      "0.767935871743487\n",
      "iteration no 499\n",
      "tensor(4.8011, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
      "        1, 0, 1, 1, 1, 0])\n",
      "tensor(9.9849, grad_fn=<AddBackward0>) loss\n",
      "0.7679333333333334\n",
      "iteration no 500\n",
      "tensor(4.8011, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 1, 0])\n",
      "tensor(9.8967, grad_fn=<AddBackward0>) loss\n",
      "0.7679973386560213\n",
      "iteration no 501\n",
      "tensor(4.8011, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.9266, grad_fn=<AddBackward0>) loss\n",
      "0.7679946879150067\n",
      "iteration no 502\n",
      "tensor(4.8011, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.8997, grad_fn=<AddBackward0>) loss\n",
      "0.7679257786613651\n",
      "iteration no 503\n",
      "tensor(4.8011, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 1, 1, 0, 1])\n",
      "tensor(9.9183, grad_fn=<AddBackward0>) loss\n",
      "0.7680555555555556\n",
      "iteration no 504\n",
      "tensor(4.8011, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0])\n",
      "tensor(9.9379, grad_fn=<AddBackward0>) loss\n",
      "0.7680528052805281\n",
      "iteration no 505\n",
      "tensor(4.8011, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.8772, grad_fn=<AddBackward0>) loss\n",
      "0.7681818181818182\n",
      "iteration no 506\n",
      "tensor(4.8011, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
      "        1, 1, 0, 1, 1, 1])\n",
      "tensor(10.0224, grad_fn=<AddBackward0>) loss\n",
      "0.7681130834976989\n",
      "iteration no 507\n",
      "tensor(4.8011, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.8760, grad_fn=<AddBackward0>) loss\n",
      "0.7681102362204725\n",
      "iteration no 508\n",
      "tensor(4.8011, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 0, 1, 0, 1])\n",
      "tensor(9.9510, grad_fn=<AddBackward0>) loss\n",
      "0.7680419122462344\n",
      "iteration no 509\n",
      "tensor(4.8011, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 1])\n",
      "tensor(10.0301, grad_fn=<AddBackward0>) loss\n",
      "0.7679738562091504\n",
      "iteration no 510\n",
      "tensor(4.8011, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
      "        1, 1, 0, 1, 0, 1])\n",
      "tensor(9.9929, grad_fn=<AddBackward0>) loss\n",
      "0.7679060665362035\n",
      "iteration no 511\n",
      "tensor(4.8011, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 1])\n",
      "tensor(9.9942, grad_fn=<AddBackward0>) loss\n",
      "0.7679036458333334\n",
      "iteration no 512\n",
      "tensor(4.8011, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9343, grad_fn=<AddBackward0>) loss\n",
      "0.767966211825861\n",
      "iteration no 513\n",
      "tensor(4.8011, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 1, 0, 0, 1, 0])\n",
      "tensor(10.0258, grad_fn=<AddBackward0>) loss\n",
      "0.767833981841764\n",
      "iteration no 514\n",
      "tensor(4.8011, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1])\n",
      "tensor(9.9897, grad_fn=<AddBackward0>) loss\n",
      "0.7677669902912622\n",
      "iteration no 515\n",
      "tensor(4.8011, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.9072, grad_fn=<AddBackward0>) loss\n",
      "0.7677648578811369\n",
      "iteration no 516\n",
      "tensor(4.8011, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9268, grad_fn=<AddBackward0>) loss\n",
      "0.7677627337201806\n",
      "iteration no 517\n",
      "tensor(4.8011, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8108, grad_fn=<AddBackward0>) loss\n",
      "0.7680180180180181\n",
      "iteration no 518\n",
      "tensor(4.8010, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 1, 0, 1, 1])\n",
      "tensor(9.9890, grad_fn=<AddBackward0>) loss\n",
      "0.7681438664097624\n",
      "iteration no 519\n",
      "tensor(4.8010, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9532, grad_fn=<AddBackward0>) loss\n",
      "0.7682692307692308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 520\n",
      "tensor(4.8010, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 1])\n",
      "tensor(9.9690, grad_fn=<AddBackward0>) loss\n",
      "0.7682021753039028\n",
      "iteration no 521\n",
      "tensor(4.8010, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(10.1395, grad_fn=<AddBackward0>) loss\n",
      "0.767816091954023\n",
      "iteration no 522\n",
      "tensor(4.8010, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.8347, grad_fn=<AddBackward0>) loss\n",
      "0.7680688336520076\n",
      "iteration no 523\n",
      "tensor(4.8010, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9146, grad_fn=<AddBackward0>) loss\n",
      "0.7682569974554707\n",
      "iteration no 524\n",
      "tensor(4.8010, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 1, 0])\n",
      "tensor(9.8661, grad_fn=<AddBackward0>) loss\n",
      "0.7684444444444445\n",
      "iteration no 525\n",
      "tensor(4.8010, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.7935, grad_fn=<AddBackward0>) loss\n",
      "0.7686945500633714\n",
      "iteration no 526\n",
      "tensor(4.8010, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9820, grad_fn=<AddBackward0>) loss\n",
      "0.7686907020872865\n",
      "iteration no 527\n",
      "tensor(4.8010, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
      "        0, 1, 0, 1, 0, 0])\n",
      "tensor(10.0523, grad_fn=<AddBackward0>) loss\n",
      "0.7686237373737373\n",
      "iteration no 528\n",
      "tensor(4.8010, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 1, 0, 0, 0])\n",
      "tensor(10.0459, grad_fn=<AddBackward0>) loss\n",
      "0.7684940138626339\n",
      "iteration no 529\n",
      "tensor(4.8010, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "        0, 1, 0, 1, 1, 1])\n",
      "tensor(9.9082, grad_fn=<AddBackward0>) loss\n",
      "0.7684905660377358\n",
      "iteration no 530\n",
      "tensor(4.8010, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        1, 0, 1, 0, 0, 0])\n",
      "tensor(9.8998, grad_fn=<AddBackward0>) loss\n",
      "0.7685499058380414\n",
      "iteration no 531\n",
      "tensor(4.8010, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
      "        0, 1, 0, 0, 1, 0])\n",
      "tensor(9.9384, grad_fn=<AddBackward0>) loss\n",
      "0.768483709273183\n",
      "iteration no 532\n",
      "tensor(4.8010, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
      "        0, 0, 1, 0, 1, 1])\n",
      "tensor(10.2361, grad_fn=<AddBackward0>) loss\n",
      "0.7679799874921827\n",
      "iteration no 533\n",
      "tensor(4.8010, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        1, 0, 1, 0, 0, 0])\n",
      "tensor(9.9045, grad_fn=<AddBackward0>) loss\n",
      "0.7679775280898876\n",
      "iteration no 534\n",
      "tensor(4.8010, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1])\n",
      "tensor(9.9792, grad_fn=<AddBackward0>) loss\n",
      "0.7678504672897196\n",
      "iteration no 535\n",
      "tensor(4.8010, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9021, grad_fn=<AddBackward0>) loss\n",
      "0.7680348258706468\n",
      "iteration no 536\n",
      "tensor(4.8010, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.9331, grad_fn=<AddBackward0>) loss\n",
      "0.768032278088144\n",
      "iteration no 537\n",
      "tensor(4.8010, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(10.0287, grad_fn=<AddBackward0>) loss\n",
      "0.7678438661710038\n",
      "iteration no 538\n",
      "tensor(4.8010, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9038, grad_fn=<AddBackward0>) loss\n",
      "0.767965367965368\n",
      "iteration no 539\n",
      "tensor(4.8010, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9233, grad_fn=<AddBackward0>) loss\n",
      "0.7680864197530864\n",
      "iteration no 540\n",
      "tensor(4.8009, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 1, 1, 0])\n",
      "tensor(9.9851, grad_fn=<AddBackward0>) loss\n",
      "0.7679605668515096\n",
      "iteration no 541\n",
      "tensor(4.8009, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.8950, grad_fn=<AddBackward0>) loss\n",
      "0.7681426814268143\n",
      "iteration no 542\n",
      "tensor(4.8009, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 0, 1, 0, 0, 1])\n",
      "tensor(10.0542, grad_fn=<AddBackward0>) loss\n",
      "0.7677716390423572\n",
      "iteration no 543\n",
      "tensor(4.8009, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1])\n",
      "tensor(10.0353, grad_fn=<AddBackward0>) loss\n",
      "0.7675857843137255\n",
      "iteration no 544\n",
      "tensor(4.8009, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9893, grad_fn=<AddBackward0>) loss\n",
      "0.7674006116207951\n",
      "iteration no 545\n",
      "tensor(4.8009, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.9132, grad_fn=<AddBackward0>) loss\n",
      "0.7675213675213676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 546\n",
      "tensor(4.8009, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9455, grad_fn=<AddBackward0>) loss\n",
      "0.7675807434491164\n",
      "iteration no 547\n",
      "tensor(4.8009, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 1, 1, 0])\n",
      "tensor(9.9973, grad_fn=<AddBackward0>) loss\n",
      "0.7675182481751824\n",
      "iteration no 548\n",
      "tensor(4.8009, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8990, grad_fn=<AddBackward0>) loss\n",
      "0.7675774134790528\n",
      "iteration no 549\n",
      "tensor(4.8009, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 0, 0, 0, 1, 1])\n",
      "tensor(9.9371, grad_fn=<AddBackward0>) loss\n",
      "0.7676969696969697\n",
      "iteration no 550\n",
      "tensor(4.8009, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1])\n",
      "tensor(10.0659, grad_fn=<AddBackward0>) loss\n",
      "0.767513611615245\n",
      "iteration no 551\n",
      "tensor(4.8009, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 1, 1])\n",
      "tensor(9.9825, grad_fn=<AddBackward0>) loss\n",
      "0.7673913043478261\n",
      "iteration no 552\n",
      "tensor(4.8009, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 1, 0])\n",
      "tensor(9.9271, grad_fn=<AddBackward0>) loss\n",
      "0.7674502712477396\n",
      "iteration no 553\n",
      "tensor(4.8009, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.8153, grad_fn=<AddBackward0>) loss\n",
      "0.7676293622141998\n",
      "iteration no 554\n",
      "tensor(4.8009, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1])\n",
      "tensor(9.9220, grad_fn=<AddBackward0>) loss\n",
      "0.7676876876876877\n",
      "iteration no 555\n",
      "tensor(4.8009, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 1, 0, 0, 0, 1])\n",
      "tensor(9.9498, grad_fn=<AddBackward0>) loss\n",
      "0.7676858513189448\n",
      "iteration no 556\n",
      "tensor(4.8009, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.8086, grad_fn=<AddBackward0>) loss\n",
      "0.7679832435667265\n",
      "iteration no 557\n",
      "tensor(4.8009, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0])\n",
      "tensor(10.0615, grad_fn=<AddBackward0>) loss\n",
      "0.7679808841099164\n",
      "iteration no 558\n",
      "tensor(4.8009, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
      "        1, 0, 1, 0, 0, 0])\n",
      "tensor(9.9550, grad_fn=<AddBackward0>) loss\n",
      "0.7680381633870006\n",
      "iteration no 559\n",
      "tensor(4.8009, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 1, 1, 0, 0, 0])\n",
      "tensor(9.8781, grad_fn=<AddBackward0>) loss\n",
      "0.768154761904762\n",
      "iteration no 560\n",
      "tensor(4.8009, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 0])\n",
      "tensor(9.9188, grad_fn=<AddBackward0>) loss\n",
      "0.7680926916221034\n",
      "iteration no 561\n",
      "tensor(4.8009, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9202, grad_fn=<AddBackward0>) loss\n",
      "0.7680308422301305\n",
      "iteration no 562\n",
      "tensor(4.8009, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.8937, grad_fn=<AddBackward0>) loss\n",
      "0.7680284191829485\n",
      "iteration no 563\n",
      "tensor(4.8009, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9477, grad_fn=<AddBackward0>) loss\n",
      "0.7680851063829788\n",
      "iteration no 564\n",
      "tensor(4.8009, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 1])\n",
      "tensor(9.9369, grad_fn=<AddBackward0>) loss\n",
      "0.7680825958702064\n",
      "iteration no 565\n",
      "tensor(4.8009, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 1, 0, 1])\n",
      "tensor(9.9065, grad_fn=<AddBackward0>) loss\n",
      "0.7681978798586573\n",
      "iteration no 566\n",
      "tensor(4.8008, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(10.0042, grad_fn=<AddBackward0>) loss\n",
      "0.7680188124632569\n",
      "iteration no 567\n",
      "tensor(4.8008, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 0, 1, 0, 1])\n",
      "tensor(9.9550, grad_fn=<AddBackward0>) loss\n",
      "0.7680164319248827\n",
      "iteration no 568\n",
      "tensor(4.8008, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.8738, grad_fn=<AddBackward0>) loss\n",
      "0.7681898066783831\n",
      "iteration no 569\n",
      "tensor(4.8008, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n",
      "        0, 1, 0, 1, 0, 0])\n",
      "tensor(9.9630, grad_fn=<AddBackward0>) loss\n",
      "0.7681286549707602\n",
      "iteration no 570\n",
      "tensor(4.8008, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9517, grad_fn=<AddBackward0>) loss\n",
      "0.7681844716870987\n",
      "iteration no 571\n",
      "tensor(4.8008, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0])\n",
      "tensor(9.9736, grad_fn=<AddBackward0>) loss\n",
      "0.7681235431235431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 572\n",
      "tensor(4.8008, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 1])\n",
      "tensor(10.0185, grad_fn=<AddBackward0>) loss\n",
      "0.7681210005817336\n",
      "iteration no 573\n",
      "tensor(4.8008, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8032, grad_fn=<AddBackward0>) loss\n",
      "0.7683507549361208\n",
      "iteration no 574\n",
      "tensor(4.8008, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
      "        1, 0, 1, 0, 0, 0])\n",
      "tensor(9.9992, grad_fn=<AddBackward0>) loss\n",
      "0.768231884057971\n",
      "iteration no 575\n",
      "tensor(4.8008, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 1, 0])\n",
      "tensor(9.8993, grad_fn=<AddBackward0>) loss\n",
      "0.7682291666666666\n",
      "iteration no 576\n",
      "tensor(4.8008, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(10.0015, grad_fn=<AddBackward0>) loss\n",
      "0.7679953783939919\n",
      "iteration no 577\n",
      "tensor(4.8008, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
      "        1, 1, 0, 0, 0, 0])\n",
      "tensor(9.9458, grad_fn=<AddBackward0>) loss\n",
      "0.7678777393310265\n",
      "iteration no 578\n",
      "tensor(4.8008, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9301, grad_fn=<AddBackward0>) loss\n",
      "0.7679332181922855\n",
      "iteration no 579\n",
      "tensor(4.8008, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 1])\n",
      "tensor(9.8409, grad_fn=<AddBackward0>) loss\n",
      "0.7681034482758621\n",
      "iteration no 580\n",
      "tensor(4.8008, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9911, grad_fn=<AddBackward0>) loss\n",
      "0.76815834767642\n",
      "iteration no 581\n",
      "tensor(4.8008, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1, 0, 1])\n",
      "tensor(9.9453, grad_fn=<AddBackward0>) loss\n",
      "0.7680985108820161\n",
      "iteration no 582\n",
      "tensor(4.8008, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 1, 0, 0, 0, 1])\n",
      "tensor(9.9881, grad_fn=<AddBackward0>) loss\n",
      "0.7678673527730131\n",
      "iteration no 583\n",
      "tensor(4.8008, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9191, grad_fn=<AddBackward0>) loss\n",
      "0.7679794520547946\n",
      "iteration no 584\n",
      "tensor(4.8008, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8424, grad_fn=<AddBackward0>) loss\n",
      "0.7680911680911681\n",
      "iteration no 585\n",
      "tensor(4.8008, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n",
      "        1, 1, 0, 0, 1, 0])\n",
      "tensor(9.9485, grad_fn=<AddBackward0>) loss\n",
      "0.7679749715585893\n",
      "iteration no 586\n",
      "tensor(4.8008, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(10.0078, grad_fn=<AddBackward0>) loss\n",
      "0.767915956842703\n",
      "iteration no 587\n",
      "tensor(4.8008, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.8572, grad_fn=<AddBackward0>) loss\n",
      "0.768140589569161\n",
      "iteration no 588\n",
      "tensor(4.8008, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 1, 1, 1, 0, 0])\n",
      "tensor(9.9869, grad_fn=<AddBackward0>) loss\n",
      "0.768081494057725\n",
      "iteration no 589\n",
      "tensor(4.8008, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 1, 0, 1, 0, 1])\n",
      "tensor(10.0158, grad_fn=<AddBackward0>) loss\n",
      "0.7679661016949153\n",
      "iteration no 590\n",
      "tensor(4.8008, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        0, 1, 1, 1, 0, 0])\n",
      "tensor(9.8968, grad_fn=<AddBackward0>) loss\n",
      "0.7679639029892837\n",
      "iteration no 591\n",
      "tensor(4.8008, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9121, grad_fn=<AddBackward0>) loss\n",
      "0.7681306306306306\n",
      "iteration no 592\n",
      "tensor(4.8008, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8690, grad_fn=<AddBackward0>) loss\n",
      "0.7682967959527824\n",
      "iteration no 593\n",
      "tensor(4.8008, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9113, grad_fn=<AddBackward0>) loss\n",
      "0.7683501683501683\n",
      "iteration no 594\n",
      "tensor(4.8007, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        0, 1, 1, 0, 0, 0])\n",
      "tensor(9.9922, grad_fn=<AddBackward0>) loss\n",
      "0.7683473389355743\n",
      "iteration no 595\n",
      "tensor(4.8007, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
      "        1, 1, 0, 0, 0, 1])\n",
      "tensor(9.9834, grad_fn=<AddBackward0>) loss\n",
      "0.7683445190156599\n",
      "iteration no 596\n",
      "tensor(4.8007, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.9497, grad_fn=<AddBackward0>) loss\n",
      "0.7683417085427136\n",
      "iteration no 597\n",
      "tensor(4.8007, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.8793, grad_fn=<AddBackward0>) loss\n",
      "0.7683946488294314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 598\n",
      "tensor(4.8007, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.8094, grad_fn=<AddBackward0>) loss\n",
      "0.7685587089593767\n",
      "iteration no 599\n",
      "tensor(4.8007, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 0, 0])\n",
      "tensor(9.9324, grad_fn=<AddBackward0>) loss\n",
      "0.7685555555555555\n",
      "iteration no 600\n",
      "tensor(4.8007, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 1])\n",
      "tensor(9.9627, grad_fn=<AddBackward0>) loss\n",
      "0.7685524126455907\n",
      "iteration no 601\n",
      "tensor(4.8007, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,\n",
      "        0, 1, 0, 1, 0, 1])\n",
      "tensor(10.0522, grad_fn=<AddBackward0>) loss\n",
      "0.7683831672203765\n",
      "iteration no 602\n",
      "tensor(4.8007, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 0])\n",
      "tensor(9.8629, grad_fn=<AddBackward0>) loss\n",
      "0.7685461580983969\n",
      "iteration no 603\n",
      "tensor(4.8007, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.8555, grad_fn=<AddBackward0>) loss\n",
      "0.7687086092715232\n",
      "iteration no 604\n",
      "tensor(4.8007, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 1, 0])\n",
      "tensor(9.8941, grad_fn=<AddBackward0>) loss\n",
      "0.7688154269972451\n",
      "iteration no 605\n",
      "tensor(4.8007, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.8667, grad_fn=<AddBackward0>) loss\n",
      "0.769031903190319\n",
      "iteration no 606\n",
      "tensor(4.8007, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.9095, grad_fn=<AddBackward0>) loss\n",
      "0.7691927512355848\n",
      "iteration no 607\n",
      "tensor(4.8007, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 1, 0, 1])\n",
      "tensor(10.0124, grad_fn=<AddBackward0>) loss\n",
      "0.7690789473684211\n",
      "iteration no 608\n",
      "tensor(4.8007, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.8746, grad_fn=<AddBackward0>) loss\n",
      "0.7691844553913519\n",
      "iteration no 609\n",
      "tensor(4.8006, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 1, 0])\n",
      "tensor(10.0533, grad_fn=<AddBackward0>) loss\n",
      "0.7692349726775957\n",
      "iteration no 610\n",
      "tensor(4.8006, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9348, grad_fn=<AddBackward0>) loss\n",
      "0.7693944353518821\n",
      "iteration no 611\n",
      "tensor(4.8006, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 1, 1, 0])\n",
      "tensor(10.1815, grad_fn=<AddBackward0>) loss\n",
      "0.7691176470588236\n",
      "iteration no 612\n",
      "tensor(4.8006, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "        1, 0, 0, 1, 0, 1])\n",
      "tensor(10.0408, grad_fn=<AddBackward0>) loss\n",
      "0.769004893964111\n",
      "iteration no 613\n",
      "tensor(4.8006, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9257, grad_fn=<AddBackward0>) loss\n",
      "0.7690010857763301\n",
      "iteration no 614\n",
      "tensor(4.8006, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9479, grad_fn=<AddBackward0>) loss\n",
      "0.7691056910569106\n",
      "iteration no 615\n",
      "tensor(4.8006, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "        0, 1, 0, 1, 0, 0])\n",
      "tensor(9.9775, grad_fn=<AddBackward0>) loss\n",
      "0.7691017316017316\n",
      "iteration no 616\n",
      "tensor(4.8006, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.8859, grad_fn=<AddBackward0>) loss\n",
      "0.7692598595353862\n",
      "iteration no 617\n",
      "tensor(4.8006, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 0])\n",
      "tensor(9.9982, grad_fn=<AddBackward0>) loss\n",
      "0.7691477885652643\n",
      "iteration no 618\n",
      "tensor(4.8006, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1])\n",
      "tensor(10.0071, grad_fn=<AddBackward0>) loss\n",
      "0.7692514808831449\n",
      "iteration no 619\n",
      "tensor(4.8006, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.7968, grad_fn=<AddBackward0>) loss\n",
      "0.7694623655913978\n",
      "iteration no 620\n",
      "tensor(4.8006, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9079, grad_fn=<AddBackward0>) loss\n",
      "0.7695652173913043\n",
      "iteration no 621\n",
      "tensor(4.8006, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9774, grad_fn=<AddBackward0>) loss\n",
      "0.7695069667738478\n",
      "iteration no 622\n",
      "tensor(4.8006, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 0, 1, 1])\n",
      "tensor(9.9900, grad_fn=<AddBackward0>) loss\n",
      "0.7693953986088817\n",
      "iteration no 623\n",
      "tensor(4.8006, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
      "        0, 0, 0, 1, 0, 1])\n",
      "tensor(10.0949, grad_fn=<AddBackward0>) loss\n",
      "0.7693910256410257\n",
      "iteration no 624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.8006, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 1, 1])\n",
      "tensor(10.1884, grad_fn=<AddBackward0>) loss\n",
      "0.7692266666666666\n",
      "iteration no 625\n",
      "tensor(4.8006, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9477, grad_fn=<AddBackward0>) loss\n",
      "0.7693290734824281\n",
      "iteration no 626\n",
      "tensor(4.8006, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.8555, grad_fn=<AddBackward0>) loss\n",
      "0.7694311536416799\n",
      "iteration no 627\n",
      "tensor(4.8006, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9332, grad_fn=<AddBackward0>) loss\n",
      "0.76947983014862\n",
      "iteration no 628\n",
      "tensor(4.8006, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9349, grad_fn=<AddBackward0>) loss\n",
      "0.7693163751987281\n",
      "iteration no 629\n",
      "tensor(4.8006, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8708, grad_fn=<AddBackward0>) loss\n",
      "0.7694179894179894\n",
      "iteration no 630\n",
      "tensor(4.8006, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8173, grad_fn=<AddBackward0>) loss\n",
      "0.7696777601690439\n",
      "iteration no 631\n",
      "tensor(4.8006, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(10.0925, grad_fn=<AddBackward0>) loss\n",
      "0.769620253164557\n",
      "iteration no 632\n",
      "tensor(4.8006, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9793, grad_fn=<AddBackward0>) loss\n",
      "0.7696682464454976\n",
      "iteration no 633\n",
      "tensor(4.8006, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.8262, grad_fn=<AddBackward0>) loss\n",
      "0.7699263932702418\n",
      "iteration no 634\n",
      "tensor(4.8006, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9159, grad_fn=<AddBackward0>) loss\n",
      "0.7700262467191601\n",
      "iteration no 635\n",
      "tensor(4.8006, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 1])\n",
      "tensor(9.9892, grad_fn=<AddBackward0>) loss\n",
      "0.7699685534591195\n",
      "iteration no 636\n",
      "tensor(4.8005, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9362, grad_fn=<AddBackward0>) loss\n",
      "0.7700680272108843\n",
      "iteration no 637\n",
      "tensor(4.8005, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 1])\n",
      "tensor(9.9958, grad_fn=<AddBackward0>) loss\n",
      "0.7700626959247648\n",
      "iteration no 638\n",
      "tensor(4.8005, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 1, 1])\n",
      "tensor(10.0283, grad_fn=<AddBackward0>) loss\n",
      "0.7700052164840897\n",
      "iteration no 639\n",
      "tensor(4.8005, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.7582, grad_fn=<AddBackward0>) loss\n",
      "0.7703125\n",
      "iteration no 640\n",
      "tensor(4.8005, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 1, 1, 0, 1])\n",
      "tensor(10.0845, grad_fn=<AddBackward0>) loss\n",
      "0.7700988039521581\n",
      "iteration no 641\n",
      "tensor(4.8005, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 1, 1])\n",
      "tensor(9.8978, grad_fn=<AddBackward0>) loss\n",
      "0.7700934579439253\n",
      "iteration no 642\n",
      "tensor(4.8005, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.8598, grad_fn=<AddBackward0>) loss\n",
      "0.770191809227579\n",
      "iteration no 643\n",
      "tensor(4.8005, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.8836, grad_fn=<AddBackward0>) loss\n",
      "0.7702898550724637\n",
      "iteration no 644\n",
      "tensor(4.8005, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9628, grad_fn=<AddBackward0>) loss\n",
      "0.7702325581395348\n",
      "iteration no 645\n",
      "tensor(4.8005, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9048, grad_fn=<AddBackward0>) loss\n",
      "0.7702786377708978\n",
      "iteration no 646\n",
      "tensor(4.8005, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.8541, grad_fn=<AddBackward0>) loss\n",
      "0.7703245749613601\n",
      "iteration no 647\n",
      "tensor(4.8005, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.9634, grad_fn=<AddBackward0>) loss\n",
      "0.7701646090534979\n",
      "iteration no 648\n",
      "tensor(4.8005, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9895, grad_fn=<AddBackward0>) loss\n",
      "0.7699537750385208\n",
      "iteration no 649\n",
      "tensor(4.8005, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9681, grad_fn=<AddBackward0>) loss\n",
      "0.769948717948718\n",
      "iteration no 650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.8005, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0])\n",
      "tensor(9.8183, grad_fn=<AddBackward0>) loss\n",
      "0.7700460829493088\n",
      "iteration no 651\n",
      "tensor(4.8005, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(10.0483, grad_fn=<AddBackward0>) loss\n",
      "0.7699897750511248\n",
      "iteration no 652\n",
      "tensor(4.8005, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8411, grad_fn=<AddBackward0>) loss\n",
      "0.7701378254211332\n",
      "iteration no 653\n",
      "tensor(4.8005, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9383, grad_fn=<AddBackward0>) loss\n",
      "0.7701325178389399\n",
      "iteration no 654\n",
      "tensor(4.8005, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.8487, grad_fn=<AddBackward0>) loss\n",
      "0.7702290076335878\n",
      "iteration no 655\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 1, 0, 1, 1])\n",
      "tensor(10.0928, grad_fn=<AddBackward0>) loss\n",
      "0.7700711382113821\n",
      "iteration no 656\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.9028, grad_fn=<AddBackward0>) loss\n",
      "0.7700152207001522\n",
      "iteration no 657\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0])\n",
      "tensor(9.9520, grad_fn=<AddBackward0>) loss\n",
      "0.7700101317122594\n",
      "iteration no 658\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.8958, grad_fn=<AddBackward0>) loss\n",
      "0.7700556398583712\n",
      "iteration no 659\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.9240, grad_fn=<AddBackward0>) loss\n",
      "0.7700505050505051\n",
      "iteration no 660\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8180, grad_fn=<AddBackward0>) loss\n",
      "0.7702471003530005\n",
      "iteration no 661\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 1, 1, 1, 1])\n",
      "tensor(10.1180, grad_fn=<AddBackward0>) loss\n",
      "0.7699899295065458\n",
      "iteration no 662\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.8463, grad_fn=<AddBackward0>) loss\n",
      "0.7700351935646054\n",
      "iteration no 663\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 0, 1, 0, 1])\n",
      "tensor(10.0242, grad_fn=<AddBackward0>) loss\n",
      "0.769929718875502\n",
      "iteration no 664\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "        1, 0, 0, 1, 0, 1])\n",
      "tensor(9.9929, grad_fn=<AddBackward0>) loss\n",
      "0.7697744360902256\n",
      "iteration no 665\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9651, grad_fn=<AddBackward0>) loss\n",
      "0.7698698698698698\n",
      "iteration no 666\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9665, grad_fn=<AddBackward0>) loss\n",
      "0.7698650674662668\n",
      "iteration no 667\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 1, 1, 0])\n",
      "tensor(9.8598, grad_fn=<AddBackward0>) loss\n",
      "0.7699600798403193\n",
      "iteration no 668\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 1])\n",
      "tensor(10.0211, grad_fn=<AddBackward0>) loss\n",
      "0.7697558545092177\n",
      "iteration no 669\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,\n",
      "        1, 0, 1, 0, 0, 0])\n",
      "tensor(9.9042, grad_fn=<AddBackward0>) loss\n",
      "0.7697014925373135\n",
      "iteration no 670\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,\n",
      "        0, 0, 0, 1, 0, 1])\n",
      "tensor(9.9204, grad_fn=<AddBackward0>) loss\n",
      "0.7696472925981123\n",
      "iteration no 671\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9424, grad_fn=<AddBackward0>) loss\n",
      "0.7696428571428572\n",
      "iteration no 672\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9050, grad_fn=<AddBackward0>) loss\n",
      "0.769787023278851\n",
      "iteration no 673\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8882, grad_fn=<AddBackward0>) loss\n",
      "0.7698813056379822\n",
      "iteration no 674\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.8242, grad_fn=<AddBackward0>) loss\n",
      "0.7701234567901235\n",
      "iteration no 675\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "        0, 0, 1, 0, 0, 1])\n",
      "tensor(10.0965, grad_fn=<AddBackward0>) loss\n",
      "0.7698717948717949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 676\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 1])\n",
      "tensor(10.0848, grad_fn=<AddBackward0>) loss\n",
      "0.7697685869030034\n",
      "iteration no 677\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9930, grad_fn=<AddBackward0>) loss\n",
      "0.7696656833824975\n",
      "iteration no 678\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.7919, grad_fn=<AddBackward0>) loss\n",
      "0.7699067255768287\n",
      "iteration no 679\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0])\n",
      "tensor(9.9738, grad_fn=<AddBackward0>) loss\n",
      "0.7697549019607843\n",
      "iteration no 680\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 1])\n",
      "tensor(10.0247, grad_fn=<AddBackward0>) loss\n",
      "0.7697014194811551\n",
      "iteration no 681\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.8886, grad_fn=<AddBackward0>) loss\n",
      "0.7697458455522972\n",
      "iteration no 682\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9393, grad_fn=<AddBackward0>) loss\n",
      "0.7697901415324548\n",
      "iteration no 683\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0])\n",
      "tensor(10.0278, grad_fn=<AddBackward0>) loss\n",
      "0.7697855750487329\n",
      "iteration no 684\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 1])\n",
      "tensor(10.0271, grad_fn=<AddBackward0>) loss\n",
      "0.7697323600973236\n",
      "iteration no 685\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
      "        0, 0, 0, 1, 1, 0])\n",
      "tensor(10.0882, grad_fn=<AddBackward0>) loss\n",
      "0.7695821185617104\n",
      "iteration no 686\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.8912, grad_fn=<AddBackward0>) loss\n",
      "0.7696749150897623\n",
      "iteration no 687\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(10.0161, grad_fn=<AddBackward0>) loss\n",
      "0.7697189922480621\n",
      "iteration no 688\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 0, 0])\n",
      "tensor(9.9317, grad_fn=<AddBackward0>) loss\n",
      "0.76966618287373\n",
      "iteration no 689\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
      "        1, 1, 0, 1, 0, 1])\n",
      "tensor(10.1573, grad_fn=<AddBackward0>) loss\n",
      "0.7693719806763285\n",
      "iteration no 690\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(10.0250, grad_fn=<AddBackward0>) loss\n",
      "0.769416304872166\n",
      "iteration no 691\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9680, grad_fn=<AddBackward0>) loss\n",
      "0.7694605009633911\n",
      "iteration no 692\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 0, 1])\n",
      "tensor(10.0358, grad_fn=<AddBackward0>) loss\n",
      "0.7693121693121693\n",
      "iteration no 693\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9171, grad_fn=<AddBackward0>) loss\n",
      "0.7693083573487032\n",
      "iteration no 694\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 1, 0, 0, 0, 1])\n",
      "tensor(9.9320, grad_fn=<AddBackward0>) loss\n",
      "0.769304556354916\n",
      "iteration no 695\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.8946, grad_fn=<AddBackward0>) loss\n",
      "0.7693486590038314\n",
      "iteration no 696\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8964, grad_fn=<AddBackward0>) loss\n",
      "0.7694404591104734\n",
      "iteration no 697\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(10.0104, grad_fn=<AddBackward0>) loss\n",
      "0.7693887297039159\n",
      "iteration no 698\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9479, grad_fn=<AddBackward0>) loss\n",
      "0.7694802098235575\n",
      "iteration no 699\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 1])\n",
      "tensor(10.0013, grad_fn=<AddBackward0>) loss\n",
      "0.7694285714285715\n",
      "iteration no 700\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(10.0472, grad_fn=<AddBackward0>) loss\n",
      "0.7692344270090347\n",
      "iteration no 701\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9307, grad_fn=<AddBackward0>) loss\n",
      "0.7691358024691358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 702\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9533, grad_fn=<AddBackward0>) loss\n",
      "0.7691322901849218\n",
      "iteration no 703\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        1, 0, 0, 1, 0, 0])\n",
      "tensor(10.0634, grad_fn=<AddBackward0>) loss\n",
      "0.7690814393939394\n",
      "iteration no 704\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9444, grad_fn=<AddBackward0>) loss\n",
      "0.7690780141843971\n",
      "iteration no 705\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.8123, grad_fn=<AddBackward0>) loss\n",
      "0.7692634560906516\n",
      "iteration no 706\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(10.0076, grad_fn=<AddBackward0>) loss\n",
      "0.7692126355492692\n",
      "iteration no 707\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9848, grad_fn=<AddBackward0>) loss\n",
      "0.7691619585687383\n",
      "iteration no 708\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.8831, grad_fn=<AddBackward0>) loss\n",
      "0.7692054536906441\n",
      "iteration no 709\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9435, grad_fn=<AddBackward0>) loss\n",
      "0.7692488262910798\n",
      "iteration no 710\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(10.0368, grad_fn=<AddBackward0>) loss\n",
      "0.7691514299109236\n",
      "iteration no 711\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 1])\n",
      "tensor(9.9159, grad_fn=<AddBackward0>) loss\n",
      "0.7691479400749064\n",
      "iteration no 712\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 1])\n",
      "tensor(9.9779, grad_fn=<AddBackward0>) loss\n",
      "0.7692847124824684\n",
      "iteration no 713\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0])\n",
      "tensor(9.9175, grad_fn=<AddBackward0>) loss\n",
      "0.769281045751634\n",
      "iteration no 714\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9637, grad_fn=<AddBackward0>) loss\n",
      "0.7693240093240094\n",
      "iteration no 715\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(10.0183, grad_fn=<AddBackward0>) loss\n",
      "0.7692737430167598\n",
      "iteration no 716\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9418, grad_fn=<AddBackward0>) loss\n",
      "0.7692701069270107\n",
      "iteration no 717\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1])\n",
      "tensor(10.0393, grad_fn=<AddBackward0>) loss\n",
      "0.7691736304549674\n",
      "iteration no 718\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9404, grad_fn=<AddBackward0>) loss\n",
      "0.769170143718127\n",
      "iteration no 719\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        1, 0, 0, 0, 1, 0])\n",
      "tensor(9.9932, grad_fn=<AddBackward0>) loss\n",
      "0.7691203703703704\n",
      "iteration no 720\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 1, 1, 0])\n",
      "tensor(10.0173, grad_fn=<AddBackward0>) loss\n",
      "0.7690707350901526\n",
      "iteration no 721\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9249, grad_fn=<AddBackward0>) loss\n",
      "0.769067405355494\n",
      "iteration no 722\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 1, 1, 0])\n",
      "tensor(9.9613, grad_fn=<AddBackward0>) loss\n",
      "0.7690179806362379\n",
      "iteration no 723\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1])\n",
      "tensor(9.9861, grad_fn=<AddBackward0>) loss\n",
      "0.7690607734806629\n",
      "iteration no 724\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8391, grad_fn=<AddBackward0>) loss\n",
      "0.7692873563218391\n",
      "iteration no 725\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9262, grad_fn=<AddBackward0>) loss\n",
      "0.7692837465564738\n",
      "iteration no 726\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        1, 0, 1, 1, 0, 0])\n",
      "tensor(10.0973, grad_fn=<AddBackward0>) loss\n",
      "0.7690508940852819\n",
      "iteration no 727\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 1, 1, 0, 0])\n",
      "tensor(9.9706, grad_fn=<AddBackward0>) loss\n",
      "0.7690018315018315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 728\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 1, 1])\n",
      "tensor(9.9575, grad_fn=<AddBackward0>) loss\n",
      "0.7689529035208048\n",
      "iteration no 729\n",
      "tensor(4.8004, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.9487, grad_fn=<AddBackward0>) loss\n",
      "0.7690410958904109\n",
      "iteration no 730\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9548, grad_fn=<AddBackward0>) loss\n",
      "0.7691290469676243\n",
      "iteration no 731\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9955, grad_fn=<AddBackward0>) loss\n",
      "0.76908014571949\n",
      "iteration no 732\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 1])\n",
      "tensor(9.8463, grad_fn=<AddBackward0>) loss\n",
      "0.7693042291950887\n",
      "iteration no 733\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8356, grad_fn=<AddBackward0>) loss\n",
      "0.7693460490463215\n",
      "iteration no 734\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.8968, grad_fn=<AddBackward0>) loss\n",
      "0.7693877551020408\n",
      "iteration no 735\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1, 1, 0, 1, 0])\n",
      "tensor(9.9933, grad_fn=<AddBackward0>) loss\n",
      "0.769338768115942\n",
      "iteration no 736\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(10.0053, grad_fn=<AddBackward0>) loss\n",
      "0.7692899140660334\n",
      "iteration no 737\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0])\n",
      "tensor(9.9537, grad_fn=<AddBackward0>) loss\n",
      "0.7691960252935862\n",
      "iteration no 738\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.8855, grad_fn=<AddBackward0>) loss\n",
      "0.7693730266125395\n",
      "iteration no 739\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,\n",
      "        0, 0, 1, 0, 1, 1])\n",
      "tensor(10.0391, grad_fn=<AddBackward0>) loss\n",
      "0.7691441441441441\n",
      "iteration no 740\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        1, 1, 0, 0, 0, 0])\n",
      "tensor(9.9939, grad_fn=<AddBackward0>) loss\n",
      "0.7691857849752587\n",
      "iteration no 741\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0])\n",
      "tensor(9.9155, grad_fn=<AddBackward0>) loss\n",
      "0.769182389937107\n",
      "iteration no 742\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.9881, grad_fn=<AddBackward0>) loss\n",
      "0.7690444145356662\n",
      "iteration no 743\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 0, 1, 0, 1, 0])\n",
      "tensor(9.9306, grad_fn=<AddBackward0>) loss\n",
      "0.7691308243727598\n",
      "iteration no 744\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 1, 0])\n",
      "tensor(10.0860, grad_fn=<AddBackward0>) loss\n",
      "0.7688590604026846\n",
      "iteration no 745\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0])\n",
      "tensor(9.9803, grad_fn=<AddBackward0>) loss\n",
      "0.7689008042895442\n",
      "iteration no 746\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 1, 0, 1, 0])\n",
      "tensor(9.9590, grad_fn=<AddBackward0>) loss\n",
      "0.7688531905399375\n",
      "iteration no 747\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,\n",
      "        0, 0, 0, 1, 1, 1])\n",
      "tensor(10.0480, grad_fn=<AddBackward0>) loss\n",
      "0.7686720142602496\n",
      "iteration no 748\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(10.0466, grad_fn=<AddBackward0>) loss\n",
      "0.7685358255451713\n",
      "iteration no 749\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 1, 1, 1])\n",
      "tensor(10.1330, grad_fn=<AddBackward0>) loss\n",
      "0.7683111111111111\n",
      "iteration no 750\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8406, grad_fn=<AddBackward0>) loss\n",
      "0.7683533067021748\n",
      "iteration no 751\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8754, grad_fn=<AddBackward0>) loss\n",
      "0.7684397163120568\n",
      "iteration no 752\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 1, 1, 0, 0])\n",
      "tensor(9.9770, grad_fn=<AddBackward0>) loss\n",
      "0.7683930942895086\n",
      "iteration no 753\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9252, grad_fn=<AddBackward0>) loss\n",
      "0.7683908045977011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 754\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        1, 0, 1, 1, 1, 0])\n",
      "tensor(9.9129, grad_fn=<AddBackward0>) loss\n",
      "0.7683443708609271\n",
      "iteration no 755\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.8542, grad_fn=<AddBackward0>) loss\n",
      "0.7684744268077601\n",
      "iteration no 756\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
      "        0, 0, 1, 0, 1, 1])\n",
      "tensor(9.9579, grad_fn=<AddBackward0>) loss\n",
      "0.7684280052840159\n",
      "iteration no 757\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 1, 0, 0])\n",
      "tensor(9.9659, grad_fn=<AddBackward0>) loss\n",
      "0.7684696569920845\n",
      "iteration no 758\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 1])\n",
      "tensor(9.9749, grad_fn=<AddBackward0>) loss\n",
      "0.7684672815107598\n",
      "iteration no 759\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        1, 1, 1, 0, 0, 0])\n",
      "tensor(10.0831, grad_fn=<AddBackward0>) loss\n",
      "0.7683333333333333\n",
      "iteration no 760\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 1, 1])\n",
      "tensor(9.8203, grad_fn=<AddBackward0>) loss\n",
      "0.768418747262374\n",
      "iteration no 761\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 1])\n",
      "tensor(10.0915, grad_fn=<AddBackward0>) loss\n",
      "0.7681977252843395\n",
      "iteration no 762\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9328, grad_fn=<AddBackward0>) loss\n",
      "0.7681520314547837\n",
      "iteration no 763\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.8788, grad_fn=<AddBackward0>) loss\n",
      "0.7681937172774869\n",
      "iteration no 764\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(10.0617, grad_fn=<AddBackward0>) loss\n",
      "0.7682788671023966\n",
      "iteration no 765\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9039, grad_fn=<AddBackward0>) loss\n",
      "0.7682767624020888\n",
      "iteration no 766\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 1])\n",
      "tensor(10.0327, grad_fn=<AddBackward0>) loss\n",
      "0.7681442850934377\n",
      "iteration no 767\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9931, grad_fn=<AddBackward0>) loss\n",
      "0.7680121527777778\n",
      "iteration no 768\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 1])\n",
      "tensor(9.9593, grad_fn=<AddBackward0>) loss\n",
      "0.7680537494581707\n",
      "iteration no 769\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9747, grad_fn=<AddBackward0>) loss\n",
      "0.768008658008658\n",
      "iteration no 770\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9373, grad_fn=<AddBackward0>) loss\n",
      "0.7679636835278859\n",
      "iteration no 771\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
      "        1, 1, 0, 0, 1, 0])\n",
      "tensor(10.0686, grad_fn=<AddBackward0>) loss\n",
      "0.7677029360967185\n",
      "iteration no 772\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.8760, grad_fn=<AddBackward0>) loss\n",
      "0.7678309616213885\n",
      "iteration no 773\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9411, grad_fn=<AddBackward0>) loss\n",
      "0.767829457364341\n",
      "iteration no 774\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.8938, grad_fn=<AddBackward0>) loss\n",
      "0.7679139784946236\n",
      "iteration no 775\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8229, grad_fn=<AddBackward0>) loss\n",
      "0.7681271477663231\n",
      "iteration no 776\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 1])\n",
      "tensor(9.9974, grad_fn=<AddBackward0>) loss\n",
      "0.7681252681252682\n",
      "iteration no 777\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.9353, grad_fn=<AddBackward0>) loss\n",
      "0.7681233933161954\n",
      "iteration no 778\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.8956, grad_fn=<AddBackward0>) loss\n",
      "0.7682071031236628\n",
      "iteration no 779\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.9937, grad_fn=<AddBackward0>) loss\n",
      "0.7681623931623932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 780\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        1, 0, 1, 0, 0, 0])\n",
      "tensor(9.9621, grad_fn=<AddBackward0>) loss\n",
      "0.7681177976952624\n",
      "iteration no 781\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 1, 1])\n",
      "tensor(9.9946, grad_fn=<AddBackward0>) loss\n",
      "0.768073316283035\n",
      "iteration no 782\n",
      "tensor(4.8003, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.9603, grad_fn=<AddBackward0>) loss\n",
      "0.7680289484887186\n",
      "iteration no 783\n",
      "tensor(4.8002, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.9064, grad_fn=<AddBackward0>) loss\n",
      "0.7680697278911565\n",
      "iteration no 784\n",
      "tensor(4.8002, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0])\n",
      "tensor(10.0056, grad_fn=<AddBackward0>) loss\n",
      "0.7679830148619957\n",
      "iteration no 785\n",
      "tensor(4.8002, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
      "        1, 0, 0, 1, 0, 0])\n",
      "tensor(9.9271, grad_fn=<AddBackward0>) loss\n",
      "0.7680661577608142\n",
      "iteration no 786\n",
      "tensor(4.8002, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9111, grad_fn=<AddBackward0>) loss\n",
      "0.7681914443032614\n",
      "iteration no 787\n",
      "tensor(4.8002, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
      "        1, 1, 1, 1, 0, 0])\n",
      "tensor(10.0464, grad_fn=<AddBackward0>) loss\n",
      "0.7681049069373942\n",
      "iteration no 788\n",
      "tensor(4.8002, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(10.0419, grad_fn=<AddBackward0>) loss\n",
      "0.7680185889311365\n",
      "iteration no 789\n",
      "tensor(4.8002, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9454, grad_fn=<AddBackward0>) loss\n",
      "0.7681012658227848\n",
      "iteration no 790\n",
      "tensor(4.8002, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 1, 1, 0, 0, 0])\n",
      "tensor(9.9924, grad_fn=<AddBackward0>) loss\n",
      "0.7680573114201433\n",
      "iteration no 791\n",
      "tensor(4.8002, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 1, 0, 1, 0, 0])\n",
      "tensor(9.9562, grad_fn=<AddBackward0>) loss\n",
      "0.7680976430976431\n",
      "iteration no 792\n",
      "tensor(4.8002, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9105, grad_fn=<AddBackward0>) loss\n",
      "0.7681378730559059\n",
      "iteration no 793\n",
      "tensor(4.8002, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.9899, grad_fn=<AddBackward0>) loss\n",
      "0.7680940386230058\n",
      "iteration no 794\n",
      "tensor(4.8002, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 1])\n",
      "tensor(9.8842, grad_fn=<AddBackward0>) loss\n",
      "0.7681341719077568\n",
      "iteration no 795\n",
      "tensor(4.8002, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.8699, grad_fn=<AddBackward0>) loss\n",
      "0.7682160804020101\n",
      "iteration no 796\n",
      "tensor(4.8002, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.9009, grad_fn=<AddBackward0>) loss\n",
      "0.7681304893350063\n",
      "iteration no 797\n",
      "tensor(4.8002, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8865, grad_fn=<AddBackward0>) loss\n",
      "0.7682539682539683\n",
      "iteration no 798\n",
      "tensor(4.8002, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
      "        1, 1, 0, 0, 1, 1])\n",
      "tensor(10.0243, grad_fn=<AddBackward0>) loss\n",
      "0.76816854401335\n",
      "iteration no 799\n",
      "tensor(4.8002, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8908, grad_fn=<AddBackward0>) loss\n",
      "0.7682083333333334\n",
      "iteration no 800\n",
      "tensor(4.8002, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.8697, grad_fn=<AddBackward0>) loss\n",
      "0.7683312526009155\n",
      "iteration no 801\n",
      "tensor(4.8002, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9631, grad_fn=<AddBackward0>) loss\n",
      "0.7683291770573566\n",
      "iteration no 802\n",
      "tensor(4.8001, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(10.0331, grad_fn=<AddBackward0>) loss\n",
      "0.7682025736820257\n",
      "iteration no 803\n",
      "tensor(4.8001, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 1])\n",
      "tensor(9.9000, grad_fn=<AddBackward0>) loss\n",
      "0.7682835820895523\n",
      "iteration no 804\n",
      "tensor(4.8001, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1])\n",
      "tensor(9.9286, grad_fn=<AddBackward0>) loss\n",
      "0.7683229813664596\n",
      "iteration no 805\n",
      "tensor(4.8001, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.9545, grad_fn=<AddBackward0>) loss\n",
      "0.7683209263854425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 806\n",
      "tensor(4.8001, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.9546, grad_fn=<AddBackward0>) loss\n",
      "0.7684014869888476\n",
      "iteration no 807\n",
      "tensor(4.8001, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9943, grad_fn=<AddBackward0>) loss\n",
      "0.7683993399339935\n",
      "iteration no 808\n",
      "tensor(4.8001, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0])\n",
      "tensor(9.9574, grad_fn=<AddBackward0>) loss\n",
      "0.7683559950556242\n",
      "iteration no 809\n",
      "tensor(4.8001, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8921, grad_fn=<AddBackward0>) loss\n",
      "0.7683950617283951\n",
      "iteration no 810\n",
      "tensor(4.8001, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8936, grad_fn=<AddBackward0>) loss\n",
      "0.7684751335799425\n",
      "iteration no 811\n",
      "tensor(4.8001, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.8045, grad_fn=<AddBackward0>) loss\n",
      "0.7686371100164203\n",
      "iteration no 812\n",
      "tensor(4.8001, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8661, grad_fn=<AddBackward0>) loss\n",
      "0.7687576875768758\n",
      "iteration no 813\n",
      "tensor(4.8001, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(10.0081, grad_fn=<AddBackward0>) loss\n",
      "0.7686322686322686\n",
      "iteration no 814\n",
      "tensor(4.8001, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,\n",
      "        0, 1, 0, 1, 0, 0])\n",
      "tensor(10.0072, grad_fn=<AddBackward0>) loss\n",
      "0.7685480572597136\n",
      "iteration no 815\n",
      "tensor(4.8001, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 0, 0, 1, 1, 1])\n",
      "tensor(10.1201, grad_fn=<AddBackward0>) loss\n",
      "0.7683823529411765\n",
      "iteration no 816\n",
      "tensor(4.8001, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 1])\n",
      "tensor(9.9386, grad_fn=<AddBackward0>) loss\n",
      "0.7683802529579763\n",
      "iteration no 817\n",
      "tensor(4.8001, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8331, grad_fn=<AddBackward0>) loss\n",
      "0.7685004074979626\n",
      "iteration no 818\n",
      "tensor(4.8001, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 1])\n",
      "tensor(9.9701, grad_fn=<AddBackward0>) loss\n",
      "0.7686609686609687\n",
      "iteration no 819\n",
      "tensor(4.8001, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.8613, grad_fn=<AddBackward0>) loss\n",
      "0.768739837398374\n",
      "iteration no 820\n",
      "tensor(4.8001, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 1, 1, 0])\n",
      "tensor(9.8821, grad_fn=<AddBackward0>) loss\n",
      "0.7688997157937475\n",
      "iteration no 821\n",
      "tensor(4.8001, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8069, grad_fn=<AddBackward0>) loss\n",
      "0.7691403081914031\n",
      "iteration no 822\n",
      "tensor(4.8000, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0])\n",
      "tensor(10.0108, grad_fn=<AddBackward0>) loss\n",
      "0.7690968003240178\n",
      "iteration no 823\n",
      "tensor(4.8000, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1, 0, 1, 1, 0])\n",
      "tensor(9.8579, grad_fn=<AddBackward0>) loss\n",
      "0.7692152103559871\n",
      "iteration no 824\n",
      "tensor(4.8000, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0, 1, 1])\n",
      "tensor(10.0584, grad_fn=<AddBackward0>) loss\n",
      "0.7691313131313131\n",
      "iteration no 825\n",
      "tensor(4.8000, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8478, grad_fn=<AddBackward0>) loss\n",
      "0.7692897497982244\n",
      "iteration no 826\n",
      "tensor(4.8000, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8255, grad_fn=<AddBackward0>) loss\n",
      "0.7694074969770254\n",
      "iteration no 827\n",
      "tensor(4.8000, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9384, grad_fn=<AddBackward0>) loss\n",
      "0.769524959742351\n",
      "iteration no 828\n",
      "tensor(4.8000, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1, 0, 1])\n",
      "tensor(9.9303, grad_fn=<AddBackward0>) loss\n",
      "0.7695215118616807\n",
      "iteration no 829\n",
      "tensor(4.8000, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 1, 0, 0])\n",
      "tensor(9.8774, grad_fn=<AddBackward0>) loss\n",
      "0.7695983935742972\n",
      "iteration no 830\n",
      "tensor(4.8000, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 1])\n",
      "tensor(9.7615, grad_fn=<AddBackward0>) loss\n",
      "0.7697954271961492\n",
      "iteration no 831\n",
      "tensor(4.8000, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9383, grad_fn=<AddBackward0>) loss\n",
      "0.769911858974359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 832\n",
      "tensor(4.8000, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0])\n",
      "tensor(9.9623, grad_fn=<AddBackward0>) loss\n",
      "0.7699079631852741\n",
      "iteration no 833\n",
      "tensor(4.8000, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.9379, grad_fn=<AddBackward0>) loss\n",
      "0.7699440447641887\n",
      "iteration no 834\n",
      "tensor(4.7999, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 1])\n",
      "tensor(9.7789, grad_fn=<AddBackward0>) loss\n",
      "0.770059880239521\n",
      "iteration no 835\n",
      "tensor(4.7999, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(10.0055, grad_fn=<AddBackward0>) loss\n",
      "0.7700159489633174\n",
      "iteration no 836\n",
      "tensor(4.7999, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,\n",
      "        0, 1, 0, 0, 0, 1])\n",
      "tensor(9.8997, grad_fn=<AddBackward0>) loss\n",
      "0.7700119474313023\n",
      "iteration no 837\n",
      "tensor(4.7999, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 1, 1])\n",
      "tensor(9.9448, grad_fn=<AddBackward0>) loss\n",
      "0.7700477326968974\n",
      "iteration no 838\n",
      "tensor(4.7999, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n",
      "        1, 1, 1, 0, 0, 1])\n",
      "tensor(10.2055, grad_fn=<AddBackward0>) loss\n",
      "0.7696861342868494\n",
      "iteration no 839\n",
      "tensor(4.7999, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 1, 0, 0, 0, 1])\n",
      "tensor(9.9666, grad_fn=<AddBackward0>) loss\n",
      "0.7696428571428572\n",
      "iteration no 840\n",
      "tensor(4.7999, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n",
      "        0, 0, 0, 0, 1, 1])\n",
      "tensor(10.0816, grad_fn=<AddBackward0>) loss\n",
      "0.7695204122076893\n",
      "iteration no 841\n",
      "tensor(4.7999, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9848, grad_fn=<AddBackward0>) loss\n",
      "0.7694774346793349\n",
      "iteration no 842\n",
      "tensor(4.7999, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
      "        1, 0, 0, 1, 0, 0])\n",
      "tensor(9.9870, grad_fn=<AddBackward0>) loss\n",
      "0.7694345591142744\n",
      "iteration no 843\n",
      "tensor(4.7999, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 1, 0])\n",
      "tensor(9.9408, grad_fn=<AddBackward0>) loss\n",
      "0.769391785150079\n",
      "iteration no 844\n",
      "tensor(4.7999, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.9394, grad_fn=<AddBackward0>) loss\n",
      "0.7695069033530572\n",
      "iteration no 845\n",
      "tensor(4.7999, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9332, grad_fn=<AddBackward0>) loss\n",
      "0.7695823483057526\n",
      "iteration no 846\n",
      "tensor(4.7999, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9495, grad_fn=<AddBackward0>) loss\n",
      "0.7695395513577332\n",
      "iteration no 847\n",
      "tensor(4.7999, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.8600, grad_fn=<AddBackward0>) loss\n",
      "0.7696540880503144\n",
      "iteration no 848\n",
      "tensor(4.7999, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9636, grad_fn=<AddBackward0>) loss\n",
      "0.7696113074204947\n",
      "iteration no 849\n",
      "tensor(4.7999, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1, 1, 0])\n",
      "tensor(9.9449, grad_fn=<AddBackward0>) loss\n",
      "0.7695686274509804\n",
      "iteration no 850\n",
      "tensor(4.7999, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 1, 0, 0])\n",
      "tensor(9.9605, grad_fn=<AddBackward0>) loss\n",
      "0.7695652173913043\n",
      "iteration no 851\n",
      "tensor(4.7999, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.8409, grad_fn=<AddBackward0>) loss\n",
      "0.7697183098591549\n",
      "iteration no 852\n",
      "tensor(4.7999, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9421, grad_fn=<AddBackward0>) loss\n",
      "0.7697147323173115\n",
      "iteration no 853\n",
      "tensor(4.7999, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 1])\n",
      "tensor(10.0194, grad_fn=<AddBackward0>) loss\n",
      "0.7696330991412959\n",
      "iteration no 854\n",
      "tensor(4.7999, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 1, 1])\n",
      "tensor(9.9315, grad_fn=<AddBackward0>) loss\n",
      "0.7696296296296297\n",
      "iteration no 855\n",
      "tensor(4.7999, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.8865, grad_fn=<AddBackward0>) loss\n",
      "0.7696651090342679\n",
      "iteration no 856\n",
      "tensor(4.7999, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
      "        1, 0, 0, 1, 0, 0])\n",
      "tensor(9.9271, grad_fn=<AddBackward0>) loss\n",
      "0.7695838195254765\n",
      "iteration no 857\n",
      "tensor(4.7999, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 1, 0])\n",
      "tensor(9.9366, grad_fn=<AddBackward0>) loss\n",
      "0.7696581196581197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 858\n",
      "tensor(4.7999, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 1, 1, 1, 1])\n",
      "tensor(9.9374, grad_fn=<AddBackward0>) loss\n",
      "0.7696546371750097\n",
      "iteration no 859\n",
      "tensor(4.7999, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
      "        1, 1, 0, 0, 0, 0])\n",
      "tensor(9.9893, grad_fn=<AddBackward0>) loss\n",
      "0.7695348837209303\n",
      "iteration no 860\n",
      "tensor(4.7999, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 1])\n",
      "tensor(9.8989, grad_fn=<AddBackward0>) loss\n",
      "0.7695702671312428\n",
      "iteration no 861\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(10.0732, grad_fn=<AddBackward0>) loss\n",
      "0.7694122196442382\n",
      "iteration no 862\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 1])\n",
      "tensor(9.9927, grad_fn=<AddBackward0>) loss\n",
      "0.7694090382387022\n",
      "iteration no 863\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0, 1, 0])\n",
      "tensor(9.9336, grad_fn=<AddBackward0>) loss\n",
      "0.7694444444444445\n",
      "iteration no 864\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0])\n",
      "tensor(9.8318, grad_fn=<AddBackward0>) loss\n",
      "0.7695568400770713\n",
      "iteration no 865\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 1])\n",
      "tensor(10.0243, grad_fn=<AddBackward0>) loss\n",
      "0.7694765204003079\n",
      "iteration no 866\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 1, 1])\n",
      "tensor(10.0061, grad_fn=<AddBackward0>) loss\n",
      "0.769434832756632\n",
      "iteration no 867\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
      "        0, 1, 1, 0, 0, 0])\n",
      "tensor(10.1005, grad_fn=<AddBackward0>) loss\n",
      "0.7693548387096775\n",
      "iteration no 868\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9150, grad_fn=<AddBackward0>) loss\n",
      "0.7693901035673187\n",
      "iteration no 869\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(10.0203, grad_fn=<AddBackward0>) loss\n",
      "0.7692337164750958\n",
      "iteration no 870\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
      "        0, 1, 1, 0, 0, 0])\n",
      "tensor(9.9670, grad_fn=<AddBackward0>) loss\n",
      "0.7691924990432453\n",
      "iteration no 871\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 1, 0])\n",
      "tensor(9.9679, grad_fn=<AddBackward0>) loss\n",
      "0.7691896024464832\n",
      "iteration no 872\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.8976, grad_fn=<AddBackward0>) loss\n",
      "0.7693012600229095\n",
      "iteration no 873\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "        1, 1, 1, 0, 0, 0])\n",
      "tensor(10.0140, grad_fn=<AddBackward0>) loss\n",
      "0.7692601067887109\n",
      "iteration no 874\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9881, grad_fn=<AddBackward0>) loss\n",
      "0.7692190476190476\n",
      "iteration no 875\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(10.0088, grad_fn=<AddBackward0>) loss\n",
      "0.7691019786910198\n",
      "iteration no 876\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(10.0861, grad_fn=<AddBackward0>) loss\n",
      "0.7689471683770429\n",
      "iteration no 877\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9225, grad_fn=<AddBackward0>) loss\n",
      "0.7689445709946849\n",
      "iteration no 878\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.8580, grad_fn=<AddBackward0>) loss\n",
      "0.7691315889268108\n",
      "iteration no 879\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8708, grad_fn=<AddBackward0>) loss\n",
      "0.769280303030303\n",
      "iteration no 880\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        1, 0, 1, 0, 0, 1])\n",
      "tensor(10.0060, grad_fn=<AddBackward0>) loss\n",
      "0.7691638289822171\n",
      "iteration no 881\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.9499, grad_fn=<AddBackward0>) loss\n",
      "0.7691987906273621\n",
      "iteration no 882\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9369, grad_fn=<AddBackward0>) loss\n",
      "0.7692336730841827\n",
      "iteration no 883\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
      "        1, 0, 0, 1, 0, 0])\n",
      "tensor(10.0505, grad_fn=<AddBackward0>) loss\n",
      "0.7691176470588236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 884\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9246, grad_fn=<AddBackward0>) loss\n",
      "0.7691902071563088\n",
      "iteration no 885\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(10.0108, grad_fn=<AddBackward0>) loss\n",
      "0.7691497366440933\n",
      "iteration no 886\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.9630, grad_fn=<AddBackward0>) loss\n",
      "0.7691469372416385\n",
      "iteration no 887\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9166, grad_fn=<AddBackward0>) loss\n",
      "0.7691816816816817\n",
      "iteration no 888\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,\n",
      "        1, 0, 0, 0, 1, 0])\n",
      "tensor(10.0900, grad_fn=<AddBackward0>) loss\n",
      "0.769066366704162\n",
      "iteration no 889\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.8663, grad_fn=<AddBackward0>) loss\n",
      "0.7691760299625469\n",
      "iteration no 890\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
      "        1, 0, 1, 0, 0, 0])\n",
      "tensor(9.9527, grad_fn=<AddBackward0>) loss\n",
      "0.7691732136176581\n",
      "iteration no 891\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9894, grad_fn=<AddBackward0>) loss\n",
      "0.7691330343796712\n",
      "iteration no 892\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.7671, grad_fn=<AddBackward0>) loss\n",
      "0.7693169092945129\n",
      "iteration no 893\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 1, 1])\n",
      "tensor(9.9435, grad_fn=<AddBackward0>) loss\n",
      "0.7692393736017897\n",
      "iteration no 894\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(10.0217, grad_fn=<AddBackward0>) loss\n",
      "0.7691620111731844\n",
      "iteration no 895\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
      "        1, 0, 0, 1, 1, 0])\n",
      "tensor(9.8825, grad_fn=<AddBackward0>) loss\n",
      "0.7691592261904762\n",
      "iteration no 896\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 1, 0])\n",
      "tensor(9.8649, grad_fn=<AddBackward0>) loss\n",
      "0.7693050910442215\n",
      "iteration no 897\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
      "        0, 0, 1, 0, 1, 1])\n",
      "tensor(10.0104, grad_fn=<AddBackward0>) loss\n",
      "0.7692279138827023\n",
      "iteration no 898\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n",
      "        0, 0, 0, 1, 1, 0])\n",
      "tensor(10.0805, grad_fn=<AddBackward0>) loss\n",
      "0.7690396737115314\n",
      "iteration no 899\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.7655, grad_fn=<AddBackward0>) loss\n",
      "0.7692222222222223\n",
      "iteration no 900\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.7747, grad_fn=<AddBackward0>) loss\n",
      "0.7693673695893452\n",
      "iteration no 901\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9389, grad_fn=<AddBackward0>) loss\n",
      "0.7694013303769401\n",
      "iteration no 902\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9049, grad_fn=<AddBackward0>) loss\n",
      "0.7694721299372462\n",
      "iteration no 903\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(10.0749, grad_fn=<AddBackward0>) loss\n",
      "0.769358407079646\n",
      "iteration no 904\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.9454, grad_fn=<AddBackward0>) loss\n",
      "0.7693186003683241\n",
      "iteration no 905\n",
      "tensor(4.7998, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
      "        1, 1, 0, 0, 0, 0])\n",
      "tensor(9.9450, grad_fn=<AddBackward0>) loss\n",
      "0.7693156732891833\n",
      "iteration no 906\n",
      "tensor(4.7997, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 1])\n",
      "tensor(9.9468, grad_fn=<AddBackward0>) loss\n",
      "0.769239250275634\n",
      "iteration no 907\n",
      "tensor(4.7997, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8935, grad_fn=<AddBackward0>) loss\n",
      "0.7692731277533039\n",
      "iteration no 908\n",
      "tensor(4.7997, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        1, 0, 0, 1, 0, 0])\n",
      "tensor(9.8773, grad_fn=<AddBackward0>) loss\n",
      "0.7692702603593693\n",
      "iteration no 909\n",
      "tensor(4.7997, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "        0, 1, 1, 0, 0, 0])\n",
      "tensor(9.8681, grad_fn=<AddBackward0>) loss\n",
      "0.7693040293040293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 910\n",
      "tensor(4.7997, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9911, grad_fn=<AddBackward0>) loss\n",
      "0.7692279546286133\n",
      "iteration no 911\n",
      "tensor(4.7997, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.8547, grad_fn=<AddBackward0>) loss\n",
      "0.7693347953216374\n",
      "iteration no 912\n",
      "tensor(4.7997, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9914, grad_fn=<AddBackward0>) loss\n",
      "0.7692953632712669\n",
      "iteration no 913\n",
      "tensor(4.7997, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.8925, grad_fn=<AddBackward0>) loss\n",
      "0.7693654266958424\n",
      "iteration no 914\n",
      "tensor(4.7997, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9152, grad_fn=<AddBackward0>) loss\n",
      "0.7693624772313297\n",
      "iteration no 915\n",
      "tensor(4.7997, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8479, grad_fn=<AddBackward0>) loss\n",
      "0.7694687045123726\n",
      "iteration no 916\n",
      "tensor(4.7997, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 1, 0, 0])\n",
      "tensor(10.0816, grad_fn=<AddBackward0>) loss\n",
      "0.7692475463467829\n",
      "iteration no 917\n",
      "tensor(4.7997, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 1])\n",
      "tensor(9.9006, grad_fn=<AddBackward0>) loss\n",
      "0.769281045751634\n",
      "iteration no 918\n",
      "tensor(4.7997, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 1, 0, 0])\n",
      "tensor(9.9086, grad_fn=<AddBackward0>) loss\n",
      "0.769278200943054\n",
      "iteration no 919\n",
      "tensor(4.7997, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9462, grad_fn=<AddBackward0>) loss\n",
      "0.7693478260869565\n",
      "iteration no 920\n",
      "tensor(4.7997, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.8334, grad_fn=<AddBackward0>) loss\n",
      "0.7694173000361926\n",
      "iteration no 921\n",
      "tensor(4.7997, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.9321, grad_fn=<AddBackward0>) loss\n",
      "0.7694504699927693\n",
      "iteration no 922\n",
      "tensor(4.7997, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9458, grad_fn=<AddBackward0>) loss\n",
      "0.7693391115926327\n",
      "iteration no 923\n",
      "tensor(4.7997, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9292, grad_fn=<AddBackward0>) loss\n",
      "0.7693001443001443\n",
      "iteration no 924\n",
      "tensor(4.7997, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0])\n",
      "tensor(10.0901, grad_fn=<AddBackward0>) loss\n",
      "0.7692252252252252\n",
      "iteration no 925\n",
      "tensor(4.7997, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9025, grad_fn=<AddBackward0>) loss\n",
      "0.7692944564434845\n",
      "iteration no 926\n",
      "tensor(4.7997, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9697, grad_fn=<AddBackward0>) loss\n",
      "0.7692916217188062\n",
      "iteration no 927\n",
      "tensor(4.7997, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 1, 0, 1, 0])\n",
      "tensor(9.8679, grad_fn=<AddBackward0>) loss\n",
      "0.769360632183908\n",
      "iteration no 928\n",
      "tensor(4.7997, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9079, grad_fn=<AddBackward0>) loss\n",
      "0.7694294940796556\n",
      "iteration no 929\n",
      "tensor(4.7997, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 1, 0, 1])\n",
      "tensor(9.9686, grad_fn=<AddBackward0>) loss\n",
      "0.7693906810035842\n",
      "iteration no 930\n",
      "tensor(4.7997, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 1, 0, 0])\n",
      "tensor(10.0205, grad_fn=<AddBackward0>) loss\n",
      "0.7693161475116362\n",
      "iteration no 931\n",
      "tensor(4.7997, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 1, 1, 0, 1, 0])\n",
      "tensor(10.0005, grad_fn=<AddBackward0>) loss\n",
      "0.769277539341917\n",
      "iteration no 932\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9066, grad_fn=<AddBackward0>) loss\n",
      "0.7693104680242944\n",
      "iteration no 933\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9185, grad_fn=<AddBackward0>) loss\n",
      "0.7693433261955746\n",
      "iteration no 934\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 1, 0, 1, 0, 0])\n",
      "tensor(9.9300, grad_fn=<AddBackward0>) loss\n",
      "0.7693761140819965\n",
      "iteration no 935\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 0])\n",
      "tensor(10.1200, grad_fn=<AddBackward0>) loss\n",
      "0.7692663817663817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 936\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 1, 1])\n",
      "tensor(9.9906, grad_fn=<AddBackward0>) loss\n",
      "0.7692280327285663\n",
      "iteration no 937\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0])\n",
      "tensor(10.0039, grad_fn=<AddBackward0>) loss\n",
      "0.7691897654584222\n",
      "iteration no 938\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 1, 1, 0])\n",
      "tensor(10.0859, grad_fn=<AddBackward0>) loss\n",
      "0.7690095846645367\n",
      "iteration no 939\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.9149, grad_fn=<AddBackward0>) loss\n",
      "0.7690070921985815\n",
      "iteration no 940\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.8806, grad_fn=<AddBackward0>) loss\n",
      "0.7690400283386468\n",
      "iteration no 941\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.8847, grad_fn=<AddBackward0>) loss\n",
      "0.7690728945506016\n",
      "iteration no 942\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9172, grad_fn=<AddBackward0>) loss\n",
      "0.7690703428773418\n",
      "iteration no 943\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.8389, grad_fn=<AddBackward0>) loss\n",
      "0.7691737288135593\n",
      "iteration no 944\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1, 1, 0])\n",
      "tensor(10.0109, grad_fn=<AddBackward0>) loss\n",
      "0.7690652557319224\n",
      "iteration no 945\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
      "        0, 1, 1, 0, 0, 0])\n",
      "tensor(10.0651, grad_fn=<AddBackward0>) loss\n",
      "0.7689922480620155\n",
      "iteration no 946\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9405, grad_fn=<AddBackward0>) loss\n",
      "0.7690249912002816\n",
      "iteration no 947\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0])\n",
      "tensor(10.0342, grad_fn=<AddBackward0>) loss\n",
      "0.7690225035161744\n",
      "iteration no 948\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.8696, grad_fn=<AddBackward0>) loss\n",
      "0.7691253951527924\n",
      "iteration no 949\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9100, grad_fn=<AddBackward0>) loss\n",
      "0.7691578947368422\n",
      "iteration no 950\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9114, grad_fn=<AddBackward0>) loss\n",
      "0.769155275148966\n",
      "iteration no 951\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.9851, grad_fn=<AddBackward0>) loss\n",
      "0.7691526610644258\n",
      "iteration no 952\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9633, grad_fn=<AddBackward0>) loss\n",
      "0.7691500524658972\n",
      "iteration no 953\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.8571, grad_fn=<AddBackward0>) loss\n",
      "0.7692173305380853\n",
      "iteration no 954\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9299, grad_fn=<AddBackward0>) loss\n",
      "0.7692844677137871\n",
      "iteration no 955\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.9157, grad_fn=<AddBackward0>) loss\n",
      "0.7693514644351465\n",
      "iteration no 956\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,\n",
      "        1, 0, 1, 0, 1, 0])\n",
      "tensor(10.0888, grad_fn=<AddBackward0>) loss\n",
      "0.7691048415186347\n",
      "iteration no 957\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9375, grad_fn=<AddBackward0>) loss\n",
      "0.7691022964509394\n",
      "iteration no 958\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(10.0699, grad_fn=<AddBackward0>) loss\n",
      "0.7690649982620785\n",
      "iteration no 959\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,\n",
      "        0, 0, 0, 1, 1, 0])\n",
      "tensor(10.0069, grad_fn=<AddBackward0>) loss\n",
      "0.7690625\n",
      "iteration no 960\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9357, grad_fn=<AddBackward0>) loss\n",
      "0.7690600069372182\n",
      "iteration no 961\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9541, grad_fn=<AddBackward0>) loss\n",
      "0.7691268191268191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 962\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(10.0212, grad_fn=<AddBackward0>) loss\n",
      "0.7690896503980617\n",
      "iteration no 963\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 0])\n",
      "tensor(9.9161, grad_fn=<AddBackward0>) loss\n",
      "0.7690525587828493\n",
      "iteration no 964\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 0])\n",
      "tensor(10.0097, grad_fn=<AddBackward0>) loss\n",
      "0.7689464594127806\n",
      "iteration no 965\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0])\n",
      "tensor(9.9901, grad_fn=<AddBackward0>) loss\n",
      "0.7689095928226363\n",
      "iteration no 966\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8699, grad_fn=<AddBackward0>) loss\n",
      "0.768976215098242\n",
      "iteration no 967\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9601, grad_fn=<AddBackward0>) loss\n",
      "0.7689393939393939\n",
      "iteration no 968\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
      "        1, 0, 1, 0, 0, 0])\n",
      "tensor(9.9650, grad_fn=<AddBackward0>) loss\n",
      "0.7689026487788098\n",
      "iteration no 969\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9975, grad_fn=<AddBackward0>) loss\n",
      "0.7689003436426117\n",
      "iteration no 970\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(10.0230, grad_fn=<AddBackward0>) loss\n",
      "0.7687950566426365\n",
      "iteration no 971\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 1, 0])\n",
      "tensor(9.9982, grad_fn=<AddBackward0>) loss\n",
      "0.7687242798353909\n",
      "iteration no 972\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 1, 0])\n",
      "tensor(9.8987, grad_fn=<AddBackward0>) loss\n",
      "0.7687564234326825\n",
      "iteration no 973\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(10.0178, grad_fn=<AddBackward0>) loss\n",
      "0.7688569472963723\n",
      "iteration no 974\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 0, 1, 0, 0])\n",
      "tensor(9.9306, grad_fn=<AddBackward0>) loss\n",
      "0.7688547008547009\n",
      "iteration no 975\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0])\n",
      "tensor(9.8414, grad_fn=<AddBackward0>) loss\n",
      "0.7689890710382513\n",
      "iteration no 976\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9397, grad_fn=<AddBackward0>) loss\n",
      "0.7690549300580006\n",
      "iteration no 977\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9258, grad_fn=<AddBackward0>) loss\n",
      "0.769052488070893\n",
      "iteration no 978\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(10.0962, grad_fn=<AddBackward0>) loss\n",
      "0.7689819543752128\n",
      "iteration no 979\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        1, 0, 0, 1, 0, 0])\n",
      "tensor(9.9833, grad_fn=<AddBackward0>) loss\n",
      "0.7689455782312925\n",
      "iteration no 980\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1, 1, 1, 0, 1])\n",
      "tensor(9.9628, grad_fn=<AddBackward0>) loss\n",
      "0.7689432551817873\n",
      "iteration no 981\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9447, grad_fn=<AddBackward0>) loss\n",
      "0.7689748811948405\n",
      "iteration no 982\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 0, 1, 1, 0])\n",
      "tensor(10.0355, grad_fn=<AddBackward0>) loss\n",
      "0.7689386232621227\n",
      "iteration no 983\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.9994, grad_fn=<AddBackward0>) loss\n",
      "0.7689024390243903\n",
      "iteration no 984\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9322, grad_fn=<AddBackward0>) loss\n",
      "0.7689678510998308\n",
      "iteration no 985\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9727, grad_fn=<AddBackward0>) loss\n",
      "0.7690331304935767\n",
      "iteration no 986\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 1, 0])\n",
      "tensor(9.8585, grad_fn=<AddBackward0>) loss\n",
      "0.7691320499831138\n",
      "iteration no 987\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9837, grad_fn=<AddBackward0>) loss\n",
      "0.7690958164642375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 988\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9245, grad_fn=<AddBackward0>) loss\n",
      "0.7691607684529828\n",
      "iteration no 989\n",
      "tensor(4.7996, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 1, 0, 0, 0, 0])\n",
      "tensor(9.9284, grad_fn=<AddBackward0>) loss\n",
      "0.7691245791245791\n",
      "iteration no 990\n",
      "tensor(4.7995, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9234, grad_fn=<AddBackward0>) loss\n",
      "0.76912209889001\n",
      "iteration no 991\n",
      "tensor(4.7995, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1, 0, 1])\n",
      "tensor(10.0135, grad_fn=<AddBackward0>) loss\n",
      "0.769119623655914\n",
      "iteration no 992\n",
      "tensor(4.7995, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8618, grad_fn=<AddBackward0>) loss\n",
      "0.7692178583417254\n",
      "iteration no 993\n",
      "tensor(4.7995, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9545, grad_fn=<AddBackward0>) loss\n",
      "0.7693158953722334\n",
      "iteration no 994\n",
      "tensor(4.7995, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.9590, grad_fn=<AddBackward0>) loss\n",
      "0.7693132328308208\n",
      "iteration no 995\n",
      "tensor(4.7995, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8745, grad_fn=<AddBackward0>) loss\n",
      "0.7694109772423026\n",
      "iteration no 996\n",
      "tensor(4.7995, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n",
      "        0, 0, 0, 0, 1, 1])\n",
      "tensor(9.9776, grad_fn=<AddBackward0>) loss\n",
      "0.7692744901370779\n",
      "iteration no 997\n",
      "tensor(4.7995, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 0, 1, 0, 0])\n",
      "tensor(10.0320, grad_fn=<AddBackward0>) loss\n",
      "0.7692050768203073\n",
      "iteration no 998\n",
      "tensor(4.7995, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8337, grad_fn=<AddBackward0>) loss\n",
      "0.769336002669336\n",
      "iteration no 999\n",
      "tensor(4.7995, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 1, 0, 1, 0])\n",
      "tensor(9.9535, grad_fn=<AddBackward0>) loss\n",
      "0.7694\n",
      "iteration no 1000\n",
      "tensor(4.7995, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9656, grad_fn=<AddBackward0>) loss\n",
      "0.7693972693972694\n",
      "iteration no 1001\n",
      "tensor(4.7995, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9670, grad_fn=<AddBackward0>) loss\n",
      "0.7693280106453759\n",
      "iteration no 1002\n",
      "tensor(4.7995, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 1])\n",
      "tensor(10.1882, grad_fn=<AddBackward0>) loss\n",
      "0.7692588899966767\n",
      "iteration no 1003\n",
      "tensor(4.7995, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.8808, grad_fn=<AddBackward0>) loss\n",
      "0.7692895086321381\n",
      "iteration no 1004\n",
      "tensor(4.7995, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0])\n",
      "tensor(9.9634, grad_fn=<AddBackward0>) loss\n",
      "0.7692205638474295\n",
      "iteration no 1005\n",
      "tensor(4.7995, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(10.1315, grad_fn=<AddBackward0>) loss\n",
      "0.769118621603711\n",
      "iteration no 1006\n",
      "tensor(4.7995, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
      "        1, 0, 0, 0, 0, 1])\n",
      "tensor(9.8867, grad_fn=<AddBackward0>) loss\n",
      "0.7690830850711685\n",
      "iteration no 1007\n",
      "tensor(4.7995, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
      "        0, 1, 0, 1, 0, 0])\n",
      "tensor(9.9400, grad_fn=<AddBackward0>) loss\n",
      "0.7691468253968254\n",
      "iteration no 1008\n",
      "tensor(4.7995, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
      "        0, 1, 1, 0, 0, 1])\n",
      "tensor(9.9513, grad_fn=<AddBackward0>) loss\n",
      "0.7690782953419227\n",
      "iteration no 1009\n",
      "tensor(4.7995, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(10.0500, grad_fn=<AddBackward0>) loss\n",
      "0.7690429042904291\n",
      "iteration no 1010\n",
      "tensor(4.7995, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.8698, grad_fn=<AddBackward0>) loss\n",
      "0.7690735245631388\n",
      "iteration no 1011\n",
      "tensor(4.7995, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.9149, grad_fn=<AddBackward0>) loss\n",
      "0.769137022397892\n",
      "iteration no 1012\n",
      "tensor(4.7995, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.8918, grad_fn=<AddBackward0>) loss\n",
      "0.7692003948667325\n",
      "iteration no 1013\n",
      "tensor(4.7995, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.8187, grad_fn=<AddBackward0>) loss\n",
      "0.7692965154503616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 1014\n",
      "tensor(4.7995, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.8816, grad_fn=<AddBackward0>) loss\n",
      "0.76935960591133\n",
      "iteration no 1015\n",
      "tensor(4.7995, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.9244, grad_fn=<AddBackward0>) loss\n",
      "0.7694225721784776\n",
      "iteration no 1016\n",
      "tensor(4.7995, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 1, 0])\n",
      "tensor(9.8395, grad_fn=<AddBackward0>) loss\n",
      "0.7694526384791871\n",
      "iteration no 1017\n",
      "tensor(4.7995, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.9062, grad_fn=<AddBackward0>) loss\n",
      "0.7695808775376556\n",
      "iteration no 1018\n",
      "tensor(4.7994, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0])\n",
      "tensor(9.9166, grad_fn=<AddBackward0>) loss\n",
      "0.7696107294733399\n",
      "iteration no 1019\n",
      "tensor(4.7994, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9502, grad_fn=<AddBackward0>) loss\n",
      "0.7696078431372549\n",
      "iteration no 1020\n",
      "tensor(4.7994, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        1, 0, 0, 1, 0, 0])\n",
      "tensor(9.9795, grad_fn=<AddBackward0>) loss\n",
      "0.769539666993144\n",
      "iteration no 1021\n",
      "tensor(4.7994, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1])\n",
      "tensor(9.9416, grad_fn=<AddBackward0>) loss\n",
      "0.7695368558382257\n",
      "iteration no 1022\n",
      "tensor(4.7994, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1])\n",
      "tensor(9.9326, grad_fn=<AddBackward0>) loss\n",
      "0.7695992179863148\n",
      "iteration no 1023\n",
      "tensor(4.7994, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.9757, grad_fn=<AddBackward0>) loss\n",
      "0.7695963541666667\n",
      "iteration no 1024\n",
      "tensor(4.7994, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.8828, grad_fn=<AddBackward0>) loss\n",
      "0.7696910569105692\n",
      "iteration no 1025\n",
      "tensor(4.7994, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(10.0389, grad_fn=<AddBackward0>) loss\n",
      "0.7696231319038337\n",
      "iteration no 1026\n",
      "tensor(4.7994, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
      "        1, 0, 0, 1, 1, 0])\n",
      "tensor(9.9641, grad_fn=<AddBackward0>) loss\n",
      "0.76952288218111\n",
      "iteration no 1027\n",
      "tensor(4.7994, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 1, 1, 1])\n",
      "tensor(9.9904, grad_fn=<AddBackward0>) loss\n",
      "0.7695849546044099\n",
      "iteration no 1028\n",
      "tensor(4.7994, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 1, 0, 1])\n",
      "tensor(9.8990, grad_fn=<AddBackward0>) loss\n",
      "0.7696469063816003\n",
      "iteration no 1029\n",
      "tensor(4.7994, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.9851, grad_fn=<AddBackward0>) loss\n",
      "0.7696116504854369\n",
      "iteration no 1030\n",
      "tensor(4.7994, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 1])\n",
      "tensor(9.9575, grad_fn=<AddBackward0>) loss\n",
      "0.7695764629809246\n",
      "iteration no 1031\n",
      "tensor(4.7994, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9045, grad_fn=<AddBackward0>) loss\n",
      "0.7696059431524548\n",
      "iteration no 1032\n",
      "tensor(4.7994, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9984, grad_fn=<AddBackward0>) loss\n",
      "0.7696353662471765\n",
      "iteration no 1033\n",
      "tensor(4.7994, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 1, 0, 0])\n",
      "tensor(9.9545, grad_fn=<AddBackward0>) loss\n",
      "0.76963249516441\n",
      "iteration no 1034\n",
      "tensor(4.7993, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 1, 0, 0, 1, 0])\n",
      "tensor(9.8922, grad_fn=<AddBackward0>) loss\n",
      "0.7696618357487923\n",
      "iteration no 1035\n",
      "tensor(4.7993, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 1, 1, 1])\n",
      "tensor(10.0872, grad_fn=<AddBackward0>) loss\n",
      "0.7695302445302445\n",
      "iteration no 1036\n",
      "tensor(4.7993, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 1, 1, 1, 1])\n",
      "tensor(10.0468, grad_fn=<AddBackward0>) loss\n",
      "0.7694631951141112\n",
      "iteration no 1037\n",
      "tensor(4.7993, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9213, grad_fn=<AddBackward0>) loss\n",
      "0.7694926140012845\n",
      "iteration no 1038\n",
      "tensor(4.7993, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.8972, grad_fn=<AddBackward0>) loss\n",
      "0.7695540583894771\n",
      "iteration no 1039\n",
      "tensor(4.7993, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.9191, grad_fn=<AddBackward0>) loss\n",
      "0.7695512820512821\n",
      "iteration no 1040\n",
      "tensor(4.7993, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 1, 1, 0, 0, 0])\n",
      "tensor(9.9465, grad_fn=<AddBackward0>) loss\n",
      "0.7695485110470701\n",
      "iteration no 1041\n",
      "tensor(4.7993, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        1, 0, 0, 0, 1, 0])\n",
      "tensor(10.0151, grad_fn=<AddBackward0>) loss\n",
      "0.7695457453614843\n",
      "iteration no 1042\n",
      "tensor(4.7993, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,\n",
      "        0, 1, 0, 0, 1, 1])\n",
      "tensor(10.0685, grad_fn=<AddBackward0>) loss\n",
      "0.7694790667945031\n",
      "iteration no 1043\n",
      "tensor(4.7993, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 0])\n",
      "tensor(9.9374, grad_fn=<AddBackward0>) loss\n",
      "0.7695083014048532\n",
      "iteration no 1044\n",
      "tensor(4.7993, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 0])\n",
      "tensor(10.0374, grad_fn=<AddBackward0>) loss\n",
      "0.7694417862838916\n",
      "iteration no 1045\n",
      "tensor(4.7993, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(10.0596, grad_fn=<AddBackward0>) loss\n",
      "0.7694072657743786\n",
      "iteration no 1046\n",
      "tensor(4.7993, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8999, grad_fn=<AddBackward0>) loss\n",
      "0.7695001591849729\n",
      "iteration no 1047\n",
      "tensor(4.7993, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9968, grad_fn=<AddBackward0>) loss\n",
      "0.769529262086514\n",
      "iteration no 1048\n",
      "tensor(4.7993, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,\n",
      "        0, 1, 1, 0, 0, 1])\n",
      "tensor(10.0635, grad_fn=<AddBackward0>) loss\n",
      "0.7694312043215761\n",
      "iteration no 1049\n",
      "tensor(4.7993, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.8581, grad_fn=<AddBackward0>) loss\n",
      "0.7695238095238095\n",
      "iteration no 1050\n",
      "tensor(4.7993, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8352, grad_fn=<AddBackward0>) loss\n",
      "0.7695528068506184\n",
      "iteration no 1051\n",
      "tensor(4.7993, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9667, grad_fn=<AddBackward0>) loss\n",
      "0.7695500633713561\n",
      "iteration no 1052\n",
      "tensor(4.7993, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 1, 0])\n",
      "tensor(9.9232, grad_fn=<AddBackward0>) loss\n",
      "0.769610636277303\n",
      "iteration no 1053\n",
      "tensor(4.7993, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.8902, grad_fn=<AddBackward0>) loss\n",
      "0.7695762175838077\n",
      "iteration no 1054\n",
      "tensor(4.7993, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 1])\n",
      "tensor(9.9270, grad_fn=<AddBackward0>) loss\n",
      "0.7696050552922591\n",
      "iteration no 1055\n",
      "tensor(4.7993, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 0])\n",
      "tensor(10.0071, grad_fn=<AddBackward0>) loss\n",
      "0.7695391414141414\n",
      "iteration no 1056\n",
      "tensor(4.7993, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0])\n",
      "tensor(9.9605, grad_fn=<AddBackward0>) loss\n",
      "0.7695048880479344\n",
      "iteration no 1057\n",
      "tensor(4.7993, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.8802, grad_fn=<AddBackward0>) loss\n",
      "0.7695652173913043\n",
      "iteration no 1058\n",
      "tensor(4.7993, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9266, grad_fn=<AddBackward0>) loss\n",
      "0.7696254327982374\n",
      "iteration no 1059\n",
      "tensor(4.7993, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 1, 0])\n",
      "tensor(9.9720, grad_fn=<AddBackward0>) loss\n",
      "0.7695911949685534\n",
      "iteration no 1060\n",
      "tensor(4.7993, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 0])\n",
      "tensor(9.9713, grad_fn=<AddBackward0>) loss\n",
      "0.7696198554822494\n",
      "iteration no 1061\n",
      "tensor(4.7993, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8921, grad_fn=<AddBackward0>) loss\n",
      "0.7696170747018205\n",
      "iteration no 1062\n",
      "tensor(4.7993, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9804, grad_fn=<AddBackward0>) loss\n",
      "0.7696142991533396\n",
      "iteration no 1063\n",
      "tensor(4.7993, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 1, 1, 0])\n",
      "tensor(9.8622, grad_fn=<AddBackward0>) loss\n",
      "0.7697055137844612\n",
      "iteration no 1064\n",
      "tensor(4.7993, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0])\n",
      "tensor(9.9169, grad_fn=<AddBackward0>) loss\n",
      "0.7697965571205008\n",
      "iteration no 1065\n",
      "tensor(4.7993, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1])\n",
      "tensor(9.8778, grad_fn=<AddBackward0>) loss\n",
      "0.7698561601000625\n",
      "iteration no 1066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.7993, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(10.0219, grad_fn=<AddBackward0>) loss\n",
      "0.7697906904092471\n",
      "iteration no 1067\n",
      "tensor(4.7992, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.8488, grad_fn=<AddBackward0>) loss\n",
      "0.769912609238452\n",
      "iteration no 1068\n",
      "tensor(4.7992, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8769, grad_fn=<AddBackward0>) loss\n",
      "0.770065481758653\n",
      "iteration no 1069\n",
      "tensor(4.7992, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9296, grad_fn=<AddBackward0>) loss\n",
      "0.7700934579439253\n",
      "iteration no 1070\n",
      "tensor(4.7992, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(10.0250, grad_fn=<AddBackward0>) loss\n",
      "0.7699968876439465\n",
      "iteration no 1071\n",
      "tensor(4.7992, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.7936, grad_fn=<AddBackward0>) loss\n",
      "0.7701181592039801\n",
      "iteration no 1072\n",
      "tensor(4.7992, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(10.0654, grad_fn=<AddBackward0>) loss\n",
      "0.7701460080770426\n",
      "iteration no 1073\n",
      "tensor(4.7992, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 0, 0])\n",
      "tensor(9.9166, grad_fn=<AddBackward0>) loss\n",
      "0.7702048417132216\n",
      "iteration no 1074\n",
      "tensor(4.7992, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.9290, grad_fn=<AddBackward0>) loss\n",
      "0.7702015503875969\n",
      "iteration no 1075\n",
      "tensor(4.7992, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        1, 1, 1, 1, 0, 0])\n",
      "tensor(9.9910, grad_fn=<AddBackward0>) loss\n",
      "0.7701363073110284\n",
      "iteration no 1076\n",
      "tensor(4.7992, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
      "        1, 1, 0, 1, 1, 1])\n",
      "tensor(10.0597, grad_fn=<AddBackward0>) loss\n",
      "0.7700402352212937\n",
      "iteration no 1077\n",
      "tensor(4.7992, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9585, grad_fn=<AddBackward0>) loss\n",
      "0.7700371057513915\n",
      "iteration no 1078\n",
      "tensor(4.7992, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 1, 0])\n",
      "tensor(9.9475, grad_fn=<AddBackward0>) loss\n",
      "0.7700957676861291\n",
      "iteration no 1079\n",
      "tensor(4.7992, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0])\n",
      "tensor(10.0067, grad_fn=<AddBackward0>) loss\n",
      "0.7700617283950617\n",
      "iteration no 1080\n",
      "tensor(4.7992, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(10.0602, grad_fn=<AddBackward0>) loss\n",
      "0.7700585877274129\n",
      "iteration no 1081\n",
      "tensor(4.7992, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0])\n",
      "tensor(9.9379, grad_fn=<AddBackward0>) loss\n",
      "0.7700246457178065\n",
      "iteration no 1082\n",
      "tensor(4.7992, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,\n",
      "        0, 0, 1, 1, 0, 1])\n",
      "tensor(9.9964, grad_fn=<AddBackward0>) loss\n",
      "0.7699599876885196\n",
      "iteration no 1083\n",
      "tensor(4.7992, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.9770, grad_fn=<AddBackward0>) loss\n",
      "0.7699261992619926\n",
      "iteration no 1084\n",
      "tensor(4.7992, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,\n",
      "        0, 0, 0, 1, 1, 0])\n",
      "tensor(9.9919, grad_fn=<AddBackward0>) loss\n",
      "0.7698617511520738\n",
      "iteration no 1085\n",
      "tensor(4.7992, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 0, 0, 0, 1, 1])\n",
      "tensor(10.0233, grad_fn=<AddBackward0>) loss\n",
      "0.7698588090853284\n",
      "iteration no 1086\n",
      "tensor(4.7992, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.8765, grad_fn=<AddBackward0>) loss\n",
      "0.7699172033118675\n",
      "iteration no 1087\n",
      "tensor(4.7992, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0])\n",
      "tensor(9.7821, grad_fn=<AddBackward0>) loss\n",
      "0.7700061274509804\n",
      "iteration no 1088\n",
      "tensor(4.7992, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.8425, grad_fn=<AddBackward0>) loss\n",
      "0.7700948882767065\n",
      "iteration no 1089\n",
      "tensor(4.7992, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 1, 1, 0, 0, 1])\n",
      "tensor(10.0052, grad_fn=<AddBackward0>) loss\n",
      "0.7700305810397553\n",
      "iteration no 1090\n",
      "tensor(4.7992, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8836, grad_fn=<AddBackward0>) loss\n",
      "0.7701191567369385\n",
      "iteration no 1091\n",
      "tensor(4.7992, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.9127, grad_fn=<AddBackward0>) loss\n",
      "0.7701770451770452\n",
      "iteration no 1092\n",
      "tensor(4.7992, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 1, 0])\n",
      "tensor(9.9045, grad_fn=<AddBackward0>) loss\n",
      "0.770204330588594\n",
      "iteration no 1093\n",
      "tensor(4.7992, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(10.0218, grad_fn=<AddBackward0>) loss\n",
      "0.7701401584399756\n",
      "iteration no 1094\n",
      "tensor(4.7992, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 0])\n",
      "tensor(9.8712, grad_fn=<AddBackward0>) loss\n",
      "0.7701674277016742\n",
      "iteration no 1095\n",
      "tensor(4.7992, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 0])\n",
      "tensor(9.8719, grad_fn=<AddBackward0>) loss\n",
      "0.7702250608272506\n",
      "iteration no 1096\n",
      "tensor(4.7991, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(10.0182, grad_fn=<AddBackward0>) loss\n",
      "0.7701914311759344\n",
      "iteration no 1097\n",
      "tensor(4.7991, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0, 0, 1])\n",
      "tensor(9.9228, grad_fn=<AddBackward0>) loss\n",
      "0.770127504553734\n",
      "iteration no 1098\n",
      "tensor(4.7991, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0, 1, 0])\n",
      "tensor(9.9072, grad_fn=<AddBackward0>) loss\n",
      "0.770124355474674\n",
      "iteration no 1099\n",
      "tensor(4.7991, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8518, grad_fn=<AddBackward0>) loss\n",
      "0.7701818181818182\n",
      "iteration no 1100\n",
      "tensor(4.7991, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9431, grad_fn=<AddBackward0>) loss\n",
      "0.7702391765062064\n",
      "iteration no 1101\n",
      "tensor(4.7991, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.8197, grad_fn=<AddBackward0>) loss\n",
      "0.7703871748336358\n",
      "iteration no 1102\n",
      "tensor(4.7991, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.9754, grad_fn=<AddBackward0>) loss\n",
      "0.7703535811423391\n",
      "iteration no 1103\n",
      "tensor(4.7991, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9220, grad_fn=<AddBackward0>) loss\n",
      "0.7703804347826086\n",
      "iteration no 1104\n",
      "tensor(4.7991, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.9288, grad_fn=<AddBackward0>) loss\n",
      "0.7704374057315234\n",
      "iteration no 1105\n",
      "tensor(4.7991, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 1, 1])\n",
      "tensor(9.9843, grad_fn=<AddBackward0>) loss\n",
      "0.7703737191078963\n",
      "iteration no 1106\n",
      "tensor(4.7991, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9448, grad_fn=<AddBackward0>) loss\n",
      "0.7703703703703704\n",
      "iteration no 1107\n",
      "tensor(4.7991, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0])\n",
      "tensor(9.9819, grad_fn=<AddBackward0>) loss\n",
      "0.7703369434416366\n",
      "iteration no 1108\n",
      "tensor(4.7991, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
      "        1, 0, 0, 1, 0, 0])\n",
      "tensor(9.8765, grad_fn=<AddBackward0>) loss\n",
      "0.7703636910129246\n",
      "iteration no 1109\n",
      "tensor(4.7991, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9151, grad_fn=<AddBackward0>) loss\n",
      "0.7704204204204205\n",
      "iteration no 1110\n",
      "tensor(4.7991, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1])\n",
      "tensor(9.8785, grad_fn=<AddBackward0>) loss\n",
      "0.7704170417041705\n",
      "iteration no 1111\n",
      "tensor(4.7991, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0])\n",
      "tensor(9.9843, grad_fn=<AddBackward0>) loss\n",
      "0.7703237410071943\n",
      "iteration no 1112\n",
      "tensor(4.7991, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 1, 0, 0])\n",
      "tensor(9.9494, grad_fn=<AddBackward0>) loss\n",
      "0.7702905061395627\n",
      "iteration no 1113\n",
      "tensor(4.7991, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.8842, grad_fn=<AddBackward0>) loss\n",
      "0.7703171753441054\n",
      "iteration no 1114\n",
      "tensor(4.7991, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.8997, grad_fn=<AddBackward0>) loss\n",
      "0.7704035874439462\n",
      "iteration no 1115\n",
      "tensor(4.7991, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 1, 0, 0, 0, 1])\n",
      "tensor(9.7876, grad_fn=<AddBackward0>) loss\n",
      "0.7704898446833931\n",
      "iteration no 1116\n",
      "tensor(4.7991, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.8863, grad_fn=<AddBackward0>) loss\n",
      "0.7705162638018502\n",
      "iteration no 1117\n",
      "tensor(4.7990, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.8030, grad_fn=<AddBackward0>) loss\n",
      "0.7706618962432916\n",
      "iteration no 1118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.7990, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9118, grad_fn=<AddBackward0>) loss\n",
      "0.7707179028894846\n",
      "iteration no 1119\n",
      "tensor(4.7990, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(10.0120, grad_fn=<AddBackward0>) loss\n",
      "0.7707142857142857\n",
      "iteration no 1120\n",
      "tensor(4.7990, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9311, grad_fn=<AddBackward0>) loss\n",
      "0.7707701457032412\n",
      "iteration no 1121\n",
      "tensor(4.7990, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9595, grad_fn=<AddBackward0>) loss\n",
      "0.7707961972667855\n",
      "iteration no 1122\n",
      "tensor(4.7990, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 1, 0, 0, 1, 1])\n",
      "tensor(9.9845, grad_fn=<AddBackward0>) loss\n",
      "0.770762837637281\n",
      "iteration no 1123\n",
      "tensor(4.7990, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1, 0, 1])\n",
      "tensor(9.9734, grad_fn=<AddBackward0>) loss\n",
      "0.7707888493475682\n",
      "iteration no 1124\n",
      "tensor(4.7990, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        1, 0, 0, 0, 1, 1])\n",
      "tensor(9.9687, grad_fn=<AddBackward0>) loss\n",
      "0.7708148148148148\n",
      "iteration no 1125\n",
      "tensor(4.7990, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
      "        1, 0, 0, 1, 1, 0])\n",
      "tensor(9.9739, grad_fn=<AddBackward0>) loss\n",
      "0.7707223208999407\n",
      "iteration no 1126\n",
      "tensor(4.7990, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 1, 0, 1, 1, 0])\n",
      "tensor(9.9521, grad_fn=<AddBackward0>) loss\n",
      "0.7707187222715173\n",
      "iteration no 1127\n",
      "tensor(4.7990, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9084, grad_fn=<AddBackward0>) loss\n",
      "0.7707446808510638\n",
      "iteration no 1128\n",
      "tensor(4.7990, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.8907, grad_fn=<AddBackward0>) loss\n",
      "0.7708001180986124\n",
      "iteration no 1129\n",
      "tensor(4.7990, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.8980, grad_fn=<AddBackward0>) loss\n",
      "0.7708259587020649\n",
      "iteration no 1130\n",
      "tensor(4.7990, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 1, 1, 0, 0])\n",
      "tensor(9.8486, grad_fn=<AddBackward0>) loss\n",
      "0.7709106984969054\n",
      "iteration no 1131\n",
      "tensor(4.7990, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 1])\n",
      "tensor(10.0647, grad_fn=<AddBackward0>) loss\n",
      "0.7708480565371024\n",
      "iteration no 1132\n",
      "tensor(4.7990, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.8981, grad_fn=<AddBackward0>) loss\n",
      "0.7709326272433068\n",
      "iteration no 1133\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 1, 0, 0])\n",
      "tensor(9.8695, grad_fn=<AddBackward0>) loss\n",
      "0.7709582598471487\n",
      "iteration no 1134\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 1, 0, 1])\n",
      "tensor(9.8537, grad_fn=<AddBackward0>) loss\n",
      "0.7709544787077827\n",
      "iteration no 1135\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9233, grad_fn=<AddBackward0>) loss\n",
      "0.7709800469483568\n",
      "iteration no 1136\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.9524, grad_fn=<AddBackward0>) loss\n",
      "0.7709469363822926\n",
      "iteration no 1137\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9567, grad_fn=<AddBackward0>) loss\n",
      "0.7709724663151728\n",
      "iteration no 1138\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.9633, grad_fn=<AddBackward0>) loss\n",
      "0.771027216856892\n",
      "iteration no 1139\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 0])\n",
      "tensor(9.9534, grad_fn=<AddBackward0>) loss\n",
      "0.7709941520467837\n",
      "iteration no 1140\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8595, grad_fn=<AddBackward0>) loss\n",
      "0.7710780017528484\n",
      "iteration no 1141\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9941, grad_fn=<AddBackward0>) loss\n",
      "0.771015761821366\n",
      "iteration no 1142\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 0])\n",
      "tensor(10.0616, grad_fn=<AddBackward0>) loss\n",
      "0.7709827938174395\n",
      "iteration no 1143\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.9620, grad_fn=<AddBackward0>) loss\n",
      "0.7709498834498835\n",
      "iteration no 1144\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,\n",
      "        0, 0, 0, 1, 0, 1])\n",
      "tensor(9.9832, grad_fn=<AddBackward0>) loss\n",
      "0.7708879184861718\n",
      "iteration no 1145\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        1, 0, 1, 0, 1, 1])\n",
      "tensor(10.0308, grad_fn=<AddBackward0>) loss\n",
      "0.770826061663758\n",
      "iteration no 1146\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.9303, grad_fn=<AddBackward0>) loss\n",
      "0.7708224353385643\n",
      "iteration no 1147\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
      "        0, 0, 0, 0, 1, 1])\n",
      "tensor(9.8456, grad_fn=<AddBackward0>) loss\n",
      "0.7709059233449478\n",
      "iteration no 1148\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9228, grad_fn=<AddBackward0>) loss\n",
      "0.7709892660284305\n",
      "iteration no 1149\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9265, grad_fn=<AddBackward0>) loss\n",
      "0.7710144927536232\n",
      "iteration no 1150\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9245, grad_fn=<AddBackward0>) loss\n",
      "0.7709817549956559\n",
      "iteration no 1151\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9628, grad_fn=<AddBackward0>) loss\n",
      "0.7709490740740741\n",
      "iteration no 1152\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "        1, 0, 1, 1, 0, 0])\n",
      "tensor(10.0667, grad_fn=<AddBackward0>) loss\n",
      "0.7708297195721306\n",
      "iteration no 1153\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9770, grad_fn=<AddBackward0>) loss\n",
      "0.7708261120739457\n",
      "iteration no 1154\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.8268, grad_fn=<AddBackward0>) loss\n",
      "0.7708802308802308\n",
      "iteration no 1155\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(10.0048, grad_fn=<AddBackward0>) loss\n",
      "0.7707612456747405\n",
      "iteration no 1156\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9413, grad_fn=<AddBackward0>) loss\n",
      "0.7707577067127629\n",
      "iteration no 1157\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,\n",
      "        0, 0, 1, 1, 0, 1])\n",
      "tensor(10.1950, grad_fn=<AddBackward0>) loss\n",
      "0.7705238917674151\n",
      "iteration no 1158\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.8904, grad_fn=<AddBackward0>) loss\n",
      "0.7705493241299971\n",
      "iteration no 1159\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 1])\n",
      "tensor(9.8557, grad_fn=<AddBackward0>) loss\n",
      "0.770632183908046\n",
      "iteration no 1160\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.8895, grad_fn=<AddBackward0>) loss\n",
      "0.7706574791846109\n",
      "iteration no 1161\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(10.0967, grad_fn=<AddBackward0>) loss\n",
      "0.7705106138841079\n",
      "iteration no 1162\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9729, grad_fn=<AddBackward0>) loss\n",
      "0.7704786471768414\n",
      "iteration no 1163\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.9819, grad_fn=<AddBackward0>) loss\n",
      "0.770446735395189\n",
      "iteration no 1164\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.8482, grad_fn=<AddBackward0>) loss\n",
      "0.7705007153075822\n",
      "iteration no 1165\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.9303, grad_fn=<AddBackward0>) loss\n",
      "0.7705546026300744\n",
      "iteration no 1166\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        1, 1, 0, 0, 1, 0])\n",
      "tensor(9.9935, grad_fn=<AddBackward0>) loss\n",
      "0.7704655812624964\n",
      "iteration no 1167\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
      "        0, 1, 0, 1, 0, 1])\n",
      "tensor(10.0115, grad_fn=<AddBackward0>) loss\n",
      "0.7703196347031963\n",
      "iteration no 1168\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9779, grad_fn=<AddBackward0>) loss\n",
      "0.7703450242372398\n",
      "iteration no 1169\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9701, grad_fn=<AddBackward0>) loss\n",
      "0.7703703703703704\n",
      "iteration no 1170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
      "        0, 1, 0, 1, 1, 0])\n",
      "tensor(9.9771, grad_fn=<AddBackward0>) loss\n",
      "0.7703387418161116\n",
      "iteration no 1171\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
      "        0, 1, 0, 0, 0, 1])\n",
      "tensor(9.9658, grad_fn=<AddBackward0>) loss\n",
      "0.770278725824801\n",
      "iteration no 1172\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
      "        1, 0, 1, 0, 1, 0])\n",
      "tensor(9.9772, grad_fn=<AddBackward0>) loss\n",
      "0.7702472293265132\n",
      "iteration no 1173\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8347, grad_fn=<AddBackward0>) loss\n",
      "0.7703009653605906\n",
      "iteration no 1174\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9439, grad_fn=<AddBackward0>) loss\n",
      "0.770354609929078\n",
      "iteration no 1175\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9132, grad_fn=<AddBackward0>) loss\n",
      "0.7704081632653061\n",
      "iteration no 1176\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.9692, grad_fn=<AddBackward0>) loss\n",
      "0.7703200226564713\n",
      "iteration no 1177\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9371, grad_fn=<AddBackward0>) loss\n",
      "0.7703452178834183\n",
      "iteration no 1178\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(10.0907, grad_fn=<AddBackward0>) loss\n",
      "0.7702572801809443\n",
      "iteration no 1179\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0])\n",
      "tensor(10.0209, grad_fn=<AddBackward0>) loss\n",
      "0.7701977401129944\n",
      "iteration no 1180\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 1, 0, 1, 0, 0])\n",
      "tensor(9.7711, grad_fn=<AddBackward0>) loss\n",
      "0.7703358735534858\n",
      "iteration no 1181\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0])\n",
      "tensor(9.8651, grad_fn=<AddBackward0>) loss\n",
      "0.770360970107163\n",
      "iteration no 1182\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.7800, grad_fn=<AddBackward0>) loss\n",
      "0.7704987320371935\n",
      "iteration no 1183\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
      "        0, 0, 1, 0, 1, 0])\n",
      "tensor(10.0874, grad_fn=<AddBackward0>) loss\n",
      "0.770411036036036\n",
      "iteration no 1184\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9228, grad_fn=<AddBackward0>) loss\n",
      "0.7704078762306611\n",
      "iteration no 1185\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.8847, grad_fn=<AddBackward0>) loss\n",
      "0.7705171444631815\n",
      "iteration no 1186\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.8764, grad_fn=<AddBackward0>) loss\n",
      "0.770598146588037\n",
      "iteration no 1187\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9255, grad_fn=<AddBackward0>) loss\n",
      "0.7706228956228957\n",
      "iteration no 1188\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(10.0226, grad_fn=<AddBackward0>) loss\n",
      "0.7706195682646482\n",
      "iteration no 1189\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,\n",
      "        1, 0, 1, 0, 0, 0])\n",
      "tensor(9.9701, grad_fn=<AddBackward0>) loss\n",
      "0.7705602240896359\n",
      "iteration no 1190\n",
      "tensor(4.7989, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.8509, grad_fn=<AddBackward0>) loss\n",
      "0.7706409179960817\n",
      "iteration no 1191\n",
      "tensor(4.7988, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.9219, grad_fn=<AddBackward0>) loss\n",
      "0.770665548098434\n",
      "iteration no 1192\n",
      "tensor(4.7988, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0])\n",
      "tensor(9.9962, grad_fn=<AddBackward0>) loss\n",
      "0.7706342553785974\n",
      "iteration no 1193\n",
      "tensor(4.7988, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n",
      "tensor(9.9133, grad_fn=<AddBackward0>) loss\n",
      "0.7706867671691793\n",
      "iteration no 1194\n",
      "tensor(4.7988, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 1, 1])\n",
      "tensor(9.9565, grad_fn=<AddBackward0>) loss\n",
      "0.7706834030683403\n",
      "iteration no 1195\n",
      "tensor(4.7988, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.7918, grad_fn=<AddBackward0>) loss\n",
      "0.770819397993311\n",
      "iteration no 1196\n",
      "tensor(4.7988, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(10.1279, grad_fn=<AddBackward0>) loss\n",
      "0.7706766917293233\n",
      "iteration no 1197\n",
      "tensor(4.7988, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 1, 0, 1, 0, 0])\n",
      "tensor(9.9655, grad_fn=<AddBackward0>) loss\n",
      "0.7706455203116305\n",
      "iteration no 1198\n",
      "tensor(4.7988, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8380, grad_fn=<AddBackward0>) loss\n",
      "0.7707812065610231\n",
      "iteration no 1199\n",
      "tensor(4.7988, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.9649, grad_fn=<AddBackward0>) loss\n",
      "0.77075\n",
      "iteration no 1200\n",
      "tensor(4.7988, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(10.0135, grad_fn=<AddBackward0>) loss\n",
      "0.770774354704413\n",
      "iteration no 1201\n",
      "tensor(4.7988, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8752, grad_fn=<AddBackward0>) loss\n",
      "0.7707986688851913\n",
      "iteration no 1202\n",
      "tensor(4.7988, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8834, grad_fn=<AddBackward0>) loss\n",
      "0.7708783596564145\n",
      "iteration no 1203\n",
      "tensor(4.7988, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.8545, grad_fn=<AddBackward0>) loss\n",
      "0.7709579180509413\n",
      "iteration no 1204\n",
      "tensor(4.7988, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,\n",
      "        1, 1, 0, 0, 1, 0])\n",
      "tensor(10.0281, grad_fn=<AddBackward0>) loss\n",
      "0.7708160442600277\n",
      "iteration no 1205\n",
      "tensor(4.7988, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8257, grad_fn=<AddBackward0>) loss\n",
      "0.7709231619679381\n",
      "iteration no 1206\n",
      "tensor(4.7988, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.8970, grad_fn=<AddBackward0>) loss\n",
      "0.7709748688207677\n",
      "iteration no 1207\n",
      "tensor(4.7988, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 1, 0])\n",
      "tensor(9.8661, grad_fn=<AddBackward0>) loss\n",
      "0.7710264900662251\n",
      "iteration no 1208\n",
      "tensor(4.7988, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8457, grad_fn=<AddBackward0>) loss\n",
      "0.7710780259167356\n",
      "iteration no 1209\n",
      "tensor(4.7987, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8529, grad_fn=<AddBackward0>) loss\n",
      "0.7711019283746556\n",
      "iteration no 1210\n",
      "tensor(4.7987, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.8681, grad_fn=<AddBackward0>) loss\n",
      "0.7712083677401597\n",
      "iteration no 1211\n",
      "tensor(4.7987, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
      "        1, 0, 0, 1, 0, 0])\n",
      "tensor(9.8905, grad_fn=<AddBackward0>) loss\n",
      "0.7712321232123213\n",
      "iteration no 1212\n",
      "tensor(4.7987, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 1])\n",
      "tensor(9.8346, grad_fn=<AddBackward0>) loss\n",
      "0.7713107996702391\n",
      "iteration no 1213\n",
      "tensor(4.7987, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
      "        0, 0, 1, 1, 0, 0])\n",
      "tensor(10.0503, grad_fn=<AddBackward0>) loss\n",
      "0.771224601867106\n",
      "iteration no 1214\n",
      "tensor(4.7987, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(10.0393, grad_fn=<AddBackward0>) loss\n",
      "0.77119341563786\n",
      "iteration no 1215\n",
      "tensor(4.7987, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(10.0224, grad_fn=<AddBackward0>) loss\n",
      "0.7711622807017544\n",
      "iteration no 1216\n",
      "tensor(4.7987, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8445, grad_fn=<AddBackward0>) loss\n",
      "0.7712133662010409\n",
      "iteration no 1217\n",
      "tensor(4.7987, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9794, grad_fn=<AddBackward0>) loss\n",
      "0.7711548987411057\n",
      "iteration no 1218\n",
      "tensor(4.7987, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
      "        0, 1, 0, 0, 0, 1])\n",
      "tensor(10.0130, grad_fn=<AddBackward0>) loss\n",
      "0.7710691823899372\n",
      "iteration no 1219\n",
      "tensor(4.7987, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.8581, grad_fn=<AddBackward0>) loss\n",
      "0.771120218579235\n",
      "iteration no 1220\n",
      "tensor(4.7987, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.9388, grad_fn=<AddBackward0>) loss\n",
      "0.7710892710892711\n",
      "iteration no 1221\n",
      "tensor(4.7987, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1])\n",
      "tensor(9.9872, grad_fn=<AddBackward0>) loss\n",
      "0.7710856519367157\n",
      "iteration no 1222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.7986, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "        0, 1, 1, 0, 1, 0])\n",
      "tensor(9.9064, grad_fn=<AddBackward0>) loss\n",
      "0.7710820387026438\n",
      "iteration no 1223\n",
      "tensor(4.7986, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(10.0231, grad_fn=<AddBackward0>) loss\n",
      "0.7710511982570806\n",
      "iteration no 1224\n",
      "tensor(4.7986, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.9334, grad_fn=<AddBackward0>) loss\n",
      "0.7711020408163265\n",
      "iteration no 1225\n",
      "tensor(4.7986, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.8823, grad_fn=<AddBackward0>) loss\n",
      "0.7711256117455139\n",
      "iteration no 1226\n",
      "tensor(4.7986, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.8690, grad_fn=<AddBackward0>) loss\n",
      "0.7711763107851127\n",
      "iteration no 1227\n",
      "tensor(4.7986, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 0])\n",
      "tensor(10.0064, grad_fn=<AddBackward0>) loss\n",
      "0.7711454940282302\n",
      "iteration no 1228\n",
      "tensor(4.7986, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(10.0093, grad_fn=<AddBackward0>) loss\n",
      "0.7710604827773258\n",
      "iteration no 1229\n",
      "tensor(4.7986, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9198, grad_fn=<AddBackward0>) loss\n",
      "0.7710840108401084\n",
      "iteration no 1230\n",
      "tensor(4.7986, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0])\n",
      "tensor(9.8948, grad_fn=<AddBackward0>) loss\n",
      "0.771161657189277\n",
      "iteration no 1231\n",
      "tensor(4.7986, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9268, grad_fn=<AddBackward0>) loss\n",
      "0.7711580086580087\n",
      "iteration no 1232\n",
      "tensor(4.7986, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.9757, grad_fn=<AddBackward0>) loss\n",
      "0.7711273317112733\n",
      "iteration no 1233\n",
      "tensor(4.7986, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.8014, grad_fn=<AddBackward0>) loss\n",
      "0.771204754186926\n",
      "iteration no 1234\n",
      "tensor(4.7986, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1])\n",
      "tensor(9.8715, grad_fn=<AddBackward0>) loss\n",
      "0.7712280701754386\n",
      "iteration no 1235\n",
      "tensor(4.7986, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 1, 0, 0])\n",
      "tensor(10.0191, grad_fn=<AddBackward0>) loss\n",
      "0.7711704422869471\n",
      "iteration no 1236\n",
      "tensor(4.7986, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 0, 1, 1, 0, 0])\n",
      "tensor(9.9581, grad_fn=<AddBackward0>) loss\n",
      "0.7710859606575047\n",
      "iteration no 1237\n",
      "tensor(4.7986, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
      "        1, 1, 0, 1, 0, 0])\n",
      "tensor(9.8997, grad_fn=<AddBackward0>) loss\n",
      "0.7710554658050619\n",
      "iteration no 1238\n",
      "tensor(4.7986, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "tensor(9.9412, grad_fn=<AddBackward0>) loss\n",
      "0.7710519235942965\n",
      "iteration no 1239\n",
      "tensor(4.7986, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n",
      "tensor(9.9165, grad_fn=<AddBackward0>) loss\n",
      "0.7710483870967741\n",
      "iteration no 1240\n",
      "tensor(4.7986, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 1, 0, 0, 0, 0])\n",
      "tensor(9.8711, grad_fn=<AddBackward0>) loss\n",
      "0.7710717163577759\n",
      "iteration no 1241\n",
      "tensor(4.7986, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 0])\n",
      "tensor(9.8609, grad_fn=<AddBackward0>) loss\n",
      "0.7711486849168009\n",
      "iteration no 1242\n",
      "tensor(4.7986, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0])\n",
      "tensor(9.8753, grad_fn=<AddBackward0>) loss\n",
      "0.7711987127916331\n",
      "iteration no 1243\n",
      "tensor(4.7986, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0003, grad_fn=<MinBackward1>)\n",
      "torch.Size([30])\n",
      "tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 0])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-3143a8adf211>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_syn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mcor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_syn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmult_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-e1caf6eaf949>\u001b[0m in \u001b[0;36mtrain_mask\u001b[0;34m(model, criterion, optimizer, mask, increase, coef)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;31m#preds = model(im)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-environments/py36/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;31m# characters to replace unicode characters with.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-environments/py36/lib/python3.6/site-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_str\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0mtensor_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m                 \u001b[0mtensor_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayout\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrided\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-environments/py36/lib/python3.6/site-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0mformatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_summarized_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msummarize\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_tensor_str_with_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-environments/py36/lib/python3.6/site-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mnonzero_finite_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_view\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_view\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mtensor_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnonzero_finite_vals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr =  0.001  #\n",
    "\n",
    "for epoch in range (1): # real\n",
    "   \n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model_syn.parameters()), lr=lr, weight_decay=0)\n",
    "    cor, size = train_mask(model_syn, criterion, optimizer,mult_mask, True, 0.1)\n",
    "    print (cor,size)\n",
    " \n",
    "    validate_single_label(model_syn)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.875\n",
      "0.7777777777777778\n",
      "0.7\n",
      "0.7272727272727273\n",
      "0.75\n",
      "0.7692307692307693\n",
      "0.7857142857142857\n",
      "0.8\n",
      "0.8125\n",
      "0.8235294117647058\n",
      "0.7777777777777778\n",
      "0.7894736842105263\n",
      "0.8\n",
      "0.8095238095238095\n",
      "0.8181818181818182\n",
      "0.8260869565217391\n",
      "0.8333333333333334\n",
      "0.84\n",
      "0.8461538461538461\n",
      "0.8518518518518519\n",
      "0.8571428571428571\n",
      "0.8620689655172413\n",
      "0.8666666666666667\n",
      "0.8709677419354839\n",
      "0.875\n",
      "0.8787878787878788\n",
      "0.8823529411764706\n",
      "0.8857142857142857\n",
      "0.8888888888888888\n",
      "0.8918918918918919\n",
      "0.8947368421052632\n",
      "0.8974358974358975\n",
      "0.9\n",
      "0.9024390243902439\n",
      "0.9047619047619048\n",
      "0.8837209302325582\n",
      "0.8863636363636364\n",
      "0.8888888888888888\n",
      "0.8913043478260869\n",
      "0.8936170212765957\n",
      "0.8958333333333334\n",
      "0.8979591836734694\n",
      "0.88\n",
      "0.8823529411764706\n",
      "0.8846153846153846\n",
      "0.8867924528301887\n",
      "0.8888888888888888\n",
      "0.8727272727272727\n",
      "0.8571428571428571\n",
      "0.8596491228070176\n",
      "0.8620689655172413\n",
      "0.847457627118644\n",
      "0.85\n",
      "0.8524590163934426\n",
      "0.8548387096774194\n",
      "0.8571428571428571\n",
      "0.859375\n",
      "0.8615384615384616\n",
      "0.8636363636363636\n",
      "0.8656716417910447\n",
      "0.8676470588235294\n",
      "0.8695652173913043\n",
      "0.8571428571428571\n",
      "0.8450704225352113\n",
      "0.8333333333333334\n",
      "0.8356164383561644\n",
      "0.8243243243243243\n",
      "0.8266666666666667\n",
      "0.8289473684210527\n",
      "0.8181818181818182\n",
      "0.8205128205128205\n",
      "0.8227848101265823\n",
      "0.825\n",
      "0.8271604938271605\n",
      "0.8292682926829268\n",
      "0.8192771084337349\n",
      "0.8214285714285714\n",
      "0.8235294117647058\n",
      "0.813953488372093\n",
      "0.8160919540229885\n",
      "0.8181818181818182\n",
      "0.8202247191011236\n",
      "0.8111111111111111\n",
      "0.8131868131868132\n",
      "0.8152173913043478\n",
      "0.8064516129032258\n",
      "0.8085106382978723\n",
      "0.8105263157894737\n",
      "0.8020833333333334\n",
      "0.8041237113402062\n",
      "0.8061224489795918\n",
      "0.8080808080808081\n",
      "0.81\n",
      "0.8118811881188119\n",
      "0.803921568627451\n",
      "0.8058252427184466\n",
      "0.7980769230769231\n",
      "0.8\n",
      "0.8018867924528302\n",
      "0.794392523364486\n",
      "0.7962962962962963\n",
      "0.7981651376146789\n",
      "0.8\n",
      "0.8018018018018018\n",
      "0.8035714285714286\n",
      "0.8053097345132744\n",
      "0.7982456140350878\n",
      "0.8\n",
      "0.8017241379310345\n",
      "0.8034188034188035\n",
      "0.8050847457627118\n",
      "0.7983193277310925\n",
      "0.7916666666666666\n",
      "0.7933884297520661\n",
      "0.7950819672131147\n",
      "0.7967479674796748\n",
      "0.7983870967741935\n",
      "0.792\n",
      "0.7936507936507936\n",
      "0.7952755905511811\n",
      "0.796875\n",
      "0.7906976744186046\n",
      "0.7846153846153846\n",
      "0.7862595419847328\n",
      "0.7878787878787878\n",
      "0.7894736842105263\n",
      "0.7835820895522388\n",
      "0.7777777777777778\n",
      "0.7794117647058824\n",
      "0.781021897810219\n",
      "0.7753623188405797\n",
      "0.7769784172661871\n",
      "0.7714285714285715\n",
      "0.7730496453900709\n",
      "0.7746478873239436\n",
      "0.7692307692307693\n",
      "0.7708333333333334\n",
      "0.7724137931034483\n",
      "0.7671232876712328\n",
      "0.7687074829931972\n",
      "0.7702702702702703\n",
      "0.7651006711409396\n",
      "0.7666666666666667\n",
      "0.7682119205298014\n",
      "0.7697368421052632\n",
      "0.7712418300653595\n",
      "0.7662337662337663\n",
      "0.7677419354838709\n",
      "0.7692307692307693\n",
      "0.7707006369426752\n",
      "0.7721518987341772\n",
      "0.7735849056603774\n",
      "0.76875\n",
      "0.7701863354037267\n",
      "0.7716049382716049\n",
      "0.7668711656441718\n",
      "0.7682926829268293\n",
      "0.7696969696969697\n",
      "0.7710843373493976\n",
      "0.7664670658682635\n",
      "0.7678571428571429\n",
      "0.7692307692307693\n",
      "0.7705882352941177\n",
      "0.7719298245614035\n",
      "0.7674418604651163\n",
      "0.7630057803468208\n",
      "0.764367816091954\n",
      "0.76\n",
      "0.7613636363636364\n",
      "0.7570621468926554\n",
      "0.7584269662921348\n",
      "0.7541899441340782\n",
      "0.7555555555555555\n",
      "0.7569060773480663\n",
      "0.7527472527472527\n",
      "0.7540983606557377\n",
      "0.7554347826086957\n",
      "0.7567567567567568\n",
      "0.7580645161290323\n",
      "0.7593582887700535\n",
      "0.7606382978723404\n",
      "0.7619047619047619\n",
      "0.7631578947368421\n",
      "0.7643979057591623\n",
      "0.765625\n",
      "0.7668393782383419\n",
      "0.7680412371134021\n",
      "0.7692307692307693\n",
      "0.7704081632653061\n",
      "0.766497461928934\n",
      "0.7676767676767676\n",
      "0.7688442211055276\n",
      "0.77\n",
      "0.7711442786069652\n",
      "0.7722772277227723\n",
      "0.7684729064039408\n",
      "0.7647058823529411\n",
      "0.7658536585365854\n",
      "0.7669902912621359\n",
      "0.7681159420289855\n",
      "0.7644230769230769\n",
      "0.7655502392344498\n",
      "0.7666666666666667\n",
      "0.7677725118483413\n",
      "0.7688679245283019\n",
      "0.7699530516431925\n",
      "0.7710280373831776\n",
      "0.772093023255814\n",
      "0.7731481481481481\n",
      "0.7741935483870968\n",
      "0.7752293577981652\n",
      "0.776255707762557\n",
      "0.7772727272727272\n",
      "0.7782805429864253\n",
      "0.7747747747747747\n",
      "0.7757847533632287\n",
      "0.7767857142857143\n",
      "0.7733333333333333\n",
      "0.7743362831858407\n",
      "0.7709251101321586\n",
      "0.7719298245614035\n",
      "0.7729257641921398\n",
      "0.7739130434782608\n",
      "0.7705627705627706\n",
      "0.771551724137931\n",
      "0.7682403433476395\n",
      "0.7692307692307693\n",
      "0.7702127659574468\n",
      "0.7711864406779662\n",
      "0.7721518987341772\n",
      "0.773109243697479\n",
      "0.7698744769874477\n",
      "0.7666666666666667\n",
      "0.7676348547717843\n",
      "0.7644628099173554\n",
      "0.7613168724279835\n",
      "0.7622950819672131\n",
      "0.763265306122449\n",
      "0.7642276422764228\n",
      "0.7651821862348178\n",
      "0.7661290322580645\n",
      "0.7670682730923695\n",
      "0.764\n",
      "0.7649402390438247\n",
      "0.7619047619047619\n",
      "0.7628458498023716\n",
      "0.7598425196850394\n",
      "0.7607843137254902\n",
      "0.76171875\n",
      "0.7626459143968871\n",
      "0.7596899224806202\n",
      "0.7606177606177607\n",
      "0.7576923076923077\n",
      "0.7586206896551724\n",
      "0.7595419847328244\n",
      "0.7604562737642585\n",
      "0.7575757575757576\n",
      "0.7547169811320755\n",
      "0.7518796992481203\n",
      "0.7528089887640449\n",
      "0.75\n",
      "0.7472118959107806\n",
      "0.7444444444444445\n",
      "0.7416974169741697\n",
      "0.7389705882352942\n",
      "0.73992673992674\n",
      "0.7408759124087592\n",
      "0.7418181818181818\n",
      "0.7391304347826086\n",
      "0.740072202166065\n",
      "0.7410071942446043\n",
      "0.7419354838709677\n",
      "0.7428571428571429\n",
      "0.7437722419928826\n",
      "0.7446808510638298\n",
      "0.7420494699646644\n",
      "0.7429577464788732\n",
      "0.7403508771929824\n",
      "0.7377622377622378\n",
      "0.735191637630662\n",
      "0.7326388888888888\n",
      "0.7335640138408305\n",
      "0.7310344827586207\n",
      "0.7319587628865979\n",
      "0.7328767123287672\n",
      "0.7337883959044369\n",
      "0.7312925170068028\n",
      "0.7288135593220338\n",
      "0.7297297297297297\n",
      "0.7306397306397306\n",
      "0.7315436241610739\n",
      "0.7324414715719063\n",
      "0.7333333333333333\n",
      "0.7342192691029901\n",
      "0.7350993377483444\n",
      "0.735973597359736\n",
      "0.7368421052631579\n",
      "0.7377049180327869\n",
      "0.738562091503268\n",
      "0.739413680781759\n",
      "0.7402597402597403\n",
      "0.7411003236245954\n",
      "0.7419354838709677\n",
      "0.7427652733118971\n",
      "0.7435897435897436\n",
      "0.744408945686901\n",
      "0.7452229299363057\n",
      "0.746031746031746\n",
      "0.7468354430379747\n",
      "0.7444794952681388\n",
      "0.7452830188679245\n",
      "0.7429467084639498\n",
      "0.740625\n",
      "0.7414330218068536\n",
      "0.7422360248447205\n",
      "0.739938080495356\n",
      "0.7407407407407407\n",
      "0.7415384615384616\n",
      "0.7392638036809815\n",
      "0.7400611620795107\n",
      "0.7378048780487805\n",
      "0.7386018237082067\n",
      "0.7363636363636363\n",
      "0.7371601208459214\n",
      "0.7379518072289156\n",
      "0.7387387387387387\n",
      "0.7395209580838323\n",
      "0.7402985074626866\n",
      "0.7410714285714286\n",
      "0.7418397626112759\n",
      "0.742603550295858\n",
      "0.7433628318584071\n",
      "0.7441176470588236\n",
      "0.7448680351906158\n",
      "0.7426900584795322\n",
      "0.7405247813411079\n",
      "0.7383720930232558\n",
      "0.7391304347826086\n",
      "0.7398843930635838\n",
      "0.7406340057636888\n",
      "0.7413793103448276\n",
      "0.7421203438395415\n",
      "0.7428571428571429\n",
      "0.7435897435897436\n",
      "0.7443181818181818\n",
      "0.7450424929178471\n",
      "0.7457627118644068\n",
      "0.7464788732394366\n",
      "0.7471910112359551\n",
      "0.7478991596638656\n",
      "0.7486033519553073\n",
      "0.7493036211699164\n",
      "0.7472222222222222\n",
      "0.7479224376731302\n",
      "0.7458563535911602\n",
      "0.7465564738292011\n",
      "0.7472527472527473\n",
      "0.7479452054794521\n",
      "0.7486338797814208\n",
      "0.7465940054495913\n",
      "0.7445652173913043\n",
      "0.7452574525745257\n",
      "0.745945945945946\n",
      "0.7466307277628033\n",
      "0.7473118279569892\n",
      "0.7479892761394102\n",
      "0.7486631016042781\n",
      "0.7493333333333333\n",
      "0.7473404255319149\n",
      "0.7480106100795756\n",
      "0.746031746031746\n",
      "0.7467018469656992\n",
      "0.7473684210526316\n",
      "0.7454068241469817\n",
      "0.7460732984293194\n",
      "0.7467362924281984\n",
      "0.7447916666666666\n",
      "0.7454545454545455\n",
      "0.7461139896373057\n",
      "0.7467700258397932\n",
      "0.7448453608247423\n",
      "0.7455012853470437\n",
      "0.7461538461538462\n",
      "0.7468030690537084\n",
      "0.7474489795918368\n",
      "0.7455470737913485\n",
      "0.7436548223350253\n",
      "0.7443037974683544\n",
      "0.7449494949494949\n",
      "0.7455919395465995\n",
      "0.7462311557788944\n",
      "0.7468671679197995\n",
      "0.7475\n",
      "0.7456359102244389\n",
      "0.746268656716418\n",
      "0.7468982630272953\n",
      "0.7475247524752475\n",
      "0.745679012345679\n",
      "0.7463054187192119\n",
      "0.7469287469287469\n",
      "0.7450980392156863\n",
      "0.7432762836185819\n",
      "0.7439024390243902\n",
      "0.7445255474452555\n",
      "0.7451456310679612\n",
      "0.7433414043583535\n",
      "0.7415458937198067\n",
      "0.7421686746987952\n",
      "0.7427884615384616\n",
      "0.7434052757793765\n",
      "0.7440191387559809\n",
      "0.7446300715990454\n",
      "0.7452380952380953\n",
      "0.7434679334916865\n",
      "0.7417061611374408\n",
      "0.7399527186761229\n",
      "0.7405660377358491\n",
      "0.7411764705882353\n",
      "0.7417840375586855\n",
      "0.7423887587822015\n",
      "0.7429906542056075\n",
      "0.7435897435897436\n",
      "0.7441860465116279\n",
      "0.7447795823665894\n",
      "0.7453703703703703\n",
      "0.745958429561201\n",
      "0.7465437788018433\n",
      "0.7471264367816092\n",
      "0.7477064220183486\n",
      "0.7459954233409611\n",
      "0.7442922374429224\n",
      "0.7425968109339408\n",
      "0.740909090909091\n",
      "0.7414965986394558\n",
      "0.7398190045248869\n",
      "0.7404063205417607\n",
      "0.740990990990991\n",
      "0.7415730337078652\n",
      "0.7399103139013453\n",
      "0.7404921700223713\n",
      "0.7410714285714286\n",
      "0.7416481069042317\n",
      "0.7422222222222222\n",
      "0.7427937915742794\n",
      "0.7433628318584071\n",
      "0.7439293598233996\n",
      "0.7444933920704846\n",
      "0.7450549450549451\n",
      "0.7456140350877193\n",
      "0.7461706783369803\n",
      "0.7467248908296943\n",
      "0.7472766884531591\n",
      "0.7456521739130435\n",
      "0.7462039045553145\n",
      "0.7467532467532467\n",
      "0.7451403887688985\n",
      "0.7435344827586207\n",
      "0.7440860215053764\n",
      "0.7446351931330472\n",
      "0.7451820128479657\n",
      "0.7457264957264957\n",
      "0.746268656716418\n",
      "0.7468085106382979\n",
      "0.7473460721868365\n",
      "0.7457627118644068\n",
      "0.7463002114164905\n",
      "0.7468354430379747\n",
      "0.7473684210526316\n",
      "0.7478991596638656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7463312368972747\n",
      "0.7468619246861925\n",
      "0.7453027139874739\n",
      "0.7458333333333333\n",
      "0.7463617463617463\n",
      "0.7448132780082988\n",
      "0.7453416149068323\n",
      "0.7458677685950413\n",
      "0.7463917525773196\n",
      "0.7469135802469136\n",
      "0.7453798767967146\n",
      "0.7459016393442623\n",
      "0.7464212678936605\n",
      "0.746938775510204\n",
      "0.7474541751527495\n",
      "0.7479674796747967\n",
      "0.7484787018255578\n",
      "0.7489878542510121\n",
      "0.7474747474747475\n",
      "0.7479838709677419\n",
      "0.7484909456740443\n",
      "0.748995983935743\n",
      "0.7474949899799599\n",
      "0.746\n",
      "0.7465069860279441\n",
      "0.7470119521912351\n",
      "0.7475149105367793\n",
      "0.748015873015873\n",
      "0.7465346534653465\n",
      "0.7450592885375494\n",
      "0.7455621301775148\n",
      "0.7460629921259843\n",
      "0.7465618860510805\n",
      "0.7470588235294118\n",
      "0.7475538160469667\n",
      "0.748046875\n",
      "0.7485380116959064\n",
      "0.7490272373540856\n",
      "0.7495145631067961\n",
      "0.75\n",
      "0.7504835589941973\n",
      "0.750965250965251\n",
      "0.7514450867052023\n",
      "0.7519230769230769\n",
      "0.7504798464491362\n",
      "0.7490421455938697\n",
      "0.7476099426386233\n",
      "0.7480916030534351\n",
      "0.7485714285714286\n",
      "0.747148288973384\n",
      "0.7476280834914611\n",
      "0.7462121212121212\n",
      "0.7466918714555766\n",
      "0.7471698113207547\n",
      "0.7476459510357816\n",
      "0.7481203007518797\n",
      "0.7467166979362101\n",
      "0.7471910112359551\n",
      "0.7476635514018691\n",
      "0.746268656716418\n",
      "0.7467411545623837\n",
      "0.7472118959107806\n",
      "0.7476808905380334\n",
      "0.7481481481481481\n",
      "0.7486136783733827\n",
      "0.7490774907749077\n",
      "0.7495395948434622\n",
      "0.75\n",
      "0.7504587155963303\n",
      "0.7509157509157509\n",
      "0.7513711151736746\n",
      "0.75\n",
      "0.7504553734061931\n",
      "0.7490909090909091\n",
      "0.7495462794918331\n",
      "0.7481884057971014\n",
      "0.7468354430379747\n",
      "0.7472924187725631\n",
      "0.745945945945946\n",
      "0.7446043165467626\n",
      "0.7450628366247756\n",
      "0.7455197132616488\n",
      "0.7459749552772809\n",
      "0.7464285714285714\n",
      "0.7450980392156863\n",
      "0.7455516014234875\n",
      "0.7460035523978685\n",
      "0.7464539007092199\n",
      "0.7469026548672566\n",
      "0.7473498233215548\n",
      "0.746031746031746\n",
      "0.7464788732394366\n",
      "0.7469244288224957\n",
      "0.7473684210526316\n",
      "0.7478108581436077\n",
      "0.7482517482517482\n",
      "0.7469458987783595\n",
      "0.7456445993031359\n",
      "0.7460869565217392\n",
      "0.7465277777777778\n",
      "0.7469670710571924\n",
      "0.7474048442906575\n",
      "0.7461139896373057\n",
      "0.746551724137931\n",
      "0.7469879518072289\n",
      "0.7474226804123711\n",
      "0.7478559176672385\n",
      "0.7482876712328768\n",
      "0.7487179487179487\n",
      "0.7491467576791809\n",
      "0.747870528109029\n",
      "0.7482993197278912\n",
      "0.7487266553480475\n",
      "0.7491525423728813\n",
      "0.7495769881556683\n",
      "0.75\n",
      "0.7504215851602024\n",
      "0.7508417508417509\n",
      "0.7495798319327731\n",
      "0.7483221476510067\n",
      "0.7487437185929648\n",
      "0.7491638795986622\n",
      "0.7495826377295493\n",
      "0.75\n",
      "0.7504159733777038\n",
      "0.7508305647840532\n",
      "0.7512437810945274\n",
      "0.7516556291390728\n",
      "0.7520661157024794\n",
      "0.7524752475247525\n",
      "0.7528830313014827\n",
      "0.7532894736842105\n",
      "0.7536945812807881\n",
      "0.7540983606557377\n",
      "0.7545008183306056\n",
      "0.7549019607843137\n",
      "0.7553017944535073\n",
      "0.755700325732899\n",
      "0.7560975609756098\n",
      "0.7564935064935064\n",
      "0.7552674230145867\n",
      "0.7540453074433657\n",
      "0.7544426494345718\n",
      "0.7548387096774194\n",
      "0.7536231884057971\n",
      "0.7540192926045016\n",
      "0.7528089887640449\n",
      "0.7516025641025641\n",
      "0.752\n",
      "0.7523961661341853\n",
      "0.7527910685805422\n",
      "0.7531847133757962\n",
      "0.753577106518283\n",
      "0.753968253968254\n",
      "0.7527733755942948\n",
      "0.7531645569620253\n",
      "0.7519747235387045\n",
      "0.7523659305993691\n",
      "0.752755905511811\n",
      "0.7531446540880503\n",
      "0.7535321821036107\n",
      "0.7539184952978056\n",
      "0.7527386541471048\n",
      "0.7515625\n",
      "0.7519500780031201\n",
      "0.7523364485981309\n",
      "0.7527216174183515\n",
      "0.7531055900621118\n",
      "0.7534883720930232\n",
      "0.7538699690402477\n",
      "0.7542503863987635\n",
      "0.7546296296296297\n",
      "0.7550077041602465\n",
      "0.7538461538461538\n",
      "0.7526881720430108\n",
      "0.7530674846625767\n",
      "0.7534456355283308\n",
      "0.7522935779816514\n",
      "0.751145038167939\n",
      "0.7515243902439024\n",
      "0.7503805175038052\n",
      "0.7507598784194529\n",
      "0.7511380880121397\n",
      "0.7515151515151515\n",
      "0.7503782148260212\n",
      "0.7507552870090635\n",
      "0.751131221719457\n",
      "0.7515060240963856\n",
      "0.750375939849624\n",
      "0.7507507507507507\n",
      "0.7511244377811095\n",
      "0.7514970059880239\n",
      "0.7518684603886397\n",
      "0.7507462686567165\n",
      "0.7511177347242921\n",
      "0.7514880952380952\n",
      "0.7518573551263001\n",
      "0.7507418397626113\n",
      "0.7496296296296296\n",
      "0.75\n",
      "0.7488921713441654\n",
      "0.7492625368731564\n",
      "0.7496318114874816\n",
      "0.75\n",
      "0.750367107195301\n",
      "0.750733137829912\n",
      "0.7510980966325037\n",
      "0.7514619883040936\n",
      "0.7518248175182481\n",
      "0.7521865889212828\n",
      "0.75254730713246\n",
      "0.752906976744186\n",
      "0.7532656023222061\n",
      "0.7536231884057971\n",
      "0.7539797395079595\n",
      "0.7543352601156069\n",
      "0.7546897546897547\n",
      "0.7550432276657061\n",
      "0.753956834532374\n",
      "0.7543103448275862\n",
      "0.7546628407460545\n",
      "0.7550143266475645\n",
      "0.7553648068669528\n",
      "0.7557142857142857\n",
      "0.7560627674750356\n",
      "0.7564102564102564\n",
      "0.7567567567567568\n",
      "0.7571022727272727\n",
      "0.7560283687943262\n",
      "0.7563739376770539\n",
      "0.7567185289957568\n",
      "0.7556497175141242\n",
      "0.7559943582510579\n",
      "0.7563380281690141\n",
      "0.7566807313642757\n",
      "0.7570224719101124\n",
      "0.7573632538569425\n",
      "0.757703081232493\n",
      "0.7580419580419581\n",
      "0.7583798882681564\n",
      "0.7573221757322176\n",
      "0.7576601671309192\n",
      "0.7579972183588317\n",
      "0.7583333333333333\n",
      "0.7586685159500693\n",
      "0.7590027700831025\n",
      "0.7593360995850622\n",
      "0.7596685082872928\n",
      "0.76\n",
      "0.7603305785123967\n",
      "0.7606602475928473\n",
      "0.7596153846153846\n",
      "0.7599451303155007\n",
      "0.7602739726027398\n",
      "0.7606019151846786\n",
      "0.7609289617486339\n",
      "0.7612551159618008\n",
      "0.7615803814713896\n",
      "0.7619047619047619\n",
      "0.7622282608695652\n",
      "0.762550881953867\n",
      "0.7628726287262872\n",
      "0.7631935047361299\n",
      "0.7621621621621621\n",
      "0.7624831309041835\n",
      "0.7628032345013477\n",
      "0.7617765814266487\n",
      "0.7620967741935484\n",
      "0.7624161073825504\n",
      "0.7627345844504021\n",
      "0.7630522088353414\n",
      "0.7633689839572193\n",
      "0.7636849132176236\n",
      "0.764\n",
      "0.7643142476697736\n",
      "0.7646276595744681\n",
      "0.7649402390438247\n",
      "0.7652519893899205\n",
      "0.7655629139072848\n",
      "0.7645502645502645\n",
      "0.7648612945838837\n",
      "0.7651715039577837\n",
      "0.7654808959156785\n",
      "0.7657894736842106\n",
      "0.7660972404730617\n",
      "0.7664041994750657\n",
      "0.7667103538663171\n",
      "0.7670157068062827\n",
      "0.7673202614379085\n",
      "0.7676240208877284\n",
      "0.7679269882659713\n",
      "0.7682291666666666\n",
      "0.7685305591677504\n",
      "0.7688311688311689\n",
      "0.7691309987029832\n",
      "0.7694300518134715\n",
      "0.7684346701164295\n",
      "0.7674418604651163\n",
      "0.7664516129032258\n",
      "0.7667525773195877\n",
      "0.7657657657657657\n",
      "0.7660668380462725\n",
      "0.766367137355584\n",
      "0.7666666666666667\n",
      "0.7669654289372599\n",
      "0.7672634271099744\n",
      "0.7662835249042146\n",
      "0.7665816326530612\n",
      "0.7656050955414013\n",
      "0.7659033078880407\n",
      "0.7662007623888183\n",
      "0.766497461928934\n",
      "0.7667934093789607\n",
      "0.7670886075949367\n",
      "0.7673830594184576\n",
      "0.7664141414141414\n",
      "0.7667087011349306\n",
      "0.7670025188916877\n",
      "0.7672955974842768\n",
      "0.7675879396984925\n",
      "0.766624843161857\n",
      "0.7669172932330827\n",
      "0.7672090112640801\n",
      "0.7675\n",
      "0.766541822721598\n",
      "0.7655860349127181\n",
      "0.7658779576587795\n",
      "0.7661691542288557\n",
      "0.7664596273291926\n",
      "0.7667493796526055\n",
      "0.7670384138785625\n",
      "0.7660891089108911\n",
      "0.7663782447466008\n",
      "0.7666666666666667\n",
      "0.7669543773119606\n",
      "0.7672413793103449\n",
      "0.7675276752767528\n",
      "0.7678132678132679\n",
      "0.7680981595092025\n",
      "0.7683823529411765\n",
      "0.7674418604651163\n",
      "0.7677261613691931\n",
      "0.7680097680097681\n",
      "0.7682926829268293\n",
      "0.7685749086479903\n",
      "0.7688564476885644\n",
      "0.7691373025516404\n",
      "0.7682038834951457\n",
      "0.7672727272727272\n",
      "0.7675544794188862\n",
      "0.7678355501813785\n",
      "0.7669082125603864\n",
      "0.7659831121833535\n",
      "0.7650602409638554\n",
      "0.7653429602888087\n",
      "0.765625\n",
      "0.765906362545018\n",
      "0.7661870503597122\n",
      "0.7652694610778443\n",
      "0.7655502392344498\n",
      "0.7658303464755077\n",
      "0.7649164677804295\n",
      "0.7651966626936829\n",
      "0.7654761904761904\n",
      "0.7645659928656362\n",
      "0.7636579572446556\n",
      "0.763938315539739\n",
      "0.764218009478673\n",
      "0.7633136094674556\n",
      "0.7635933806146572\n",
      "0.7638724911452184\n",
      "0.7641509433962265\n",
      "0.7644287396937574\n",
      "0.7647058823529411\n",
      "0.7649823736780259\n",
      "0.7652582159624414\n",
      "0.7655334114888629\n",
      "0.7646370023419203\n",
      "0.7649122807017544\n",
      "0.7651869158878505\n",
      "0.7654609101516919\n",
      "0.7657342657342657\n",
      "0.7648428405122235\n",
      "0.763953488372093\n",
      "0.7642276422764228\n",
      "0.7645011600928074\n",
      "0.764774044032445\n",
      "0.7638888888888888\n",
      "0.7641618497109827\n",
      "0.7632794457274826\n",
      "0.7635524798154556\n",
      "0.7626728110599078\n",
      "0.762945914844649\n",
      "0.7620689655172413\n",
      "0.7623421354764638\n",
      "0.7626146788990825\n",
      "0.7617411225658648\n",
      "0.7620137299771167\n",
      "0.7622857142857142\n",
      "0.7625570776255708\n",
      "0.7628278221208666\n",
      "0.7619589977220956\n",
      "0.7622298065984073\n",
      "0.7625\n",
      "0.7627695800227015\n",
      "0.7630385487528345\n",
      "0.7621744054360136\n",
      "0.7613122171945701\n",
      "0.7615819209039548\n",
      "0.7618510158013544\n",
      "0.762119503945885\n",
      "0.7623873873873874\n",
      "0.7626546681664792\n",
      "0.7629213483146068\n",
      "0.7620650953984287\n",
      "0.7623318385650224\n",
      "0.7625979843225084\n",
      "0.7628635346756152\n",
      "0.7631284916201118\n",
      "0.7633928571428571\n",
      "0.7625418060200669\n",
      "0.7628062360801782\n",
      "0.7630700778642937\n",
      "0.7622222222222222\n",
      "0.7624861265260822\n",
      "0.7627494456762749\n",
      "0.7630121816168328\n",
      "0.7632743362831859\n",
      "0.7635359116022099\n",
      "0.7626931567328918\n",
      "0.7618522601984564\n",
      "0.762114537444934\n",
      "0.7623762376237624\n",
      "0.7626373626373626\n",
      "0.7628979143798024\n",
      "0.7620614035087719\n",
      "0.7623220153340635\n",
      "0.7614879649890591\n",
      "0.7617486338797814\n",
      "0.7609170305676856\n",
      "0.7611777535441657\n",
      "0.7603485838779956\n",
      "0.7595212187159956\n",
      "0.758695652173913\n",
      "0.758957654723127\n",
      "0.7592190889370932\n",
      "0.7583965330444203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7586580086580087\n",
      "0.7589189189189189\n",
      "0.7591792656587473\n",
      "0.7583603020496225\n",
      "0.7586206896551724\n",
      "0.7578040904198062\n",
      "0.7580645161290323\n",
      "0.7583243823845328\n",
      "0.7585836909871244\n",
      "0.7588424437299035\n",
      "0.7580299785867237\n",
      "0.7572192513368984\n",
      "0.7564102564102564\n",
      "0.7566702241195304\n",
      "0.7569296375266524\n",
      "0.7561235356762513\n",
      "0.7553191489361702\n",
      "0.7555791710945803\n",
      "0.7558386411889597\n",
      "0.7560975609756098\n",
      "0.7563559322033898\n",
      "0.7555555555555555\n",
      "0.7547568710359408\n",
      "0.7550158394931362\n",
      "0.7542194092827004\n",
      "0.7534246575342466\n",
      "0.7536842105263157\n",
      "0.7539432176656151\n",
      "0.7542016806722689\n",
      "0.7534102833158447\n",
      "0.7536687631027253\n",
      "0.7539267015706806\n",
      "0.75418410041841\n",
      "0.754440961337513\n",
      "0.7546972860125261\n",
      "0.7549530761209593\n",
      "0.7552083333333334\n",
      "0.7554630593132154\n",
      "0.7546777546777547\n",
      "0.754932502596054\n",
      "0.7551867219917012\n",
      "0.755440414507772\n",
      "0.7556935817805382\n",
      "0.7559462254395036\n",
      "0.756198347107438\n",
      "0.7564499484004128\n",
      "0.756701030927835\n",
      "0.756951596292482\n",
      "0.757201646090535\n",
      "0.7564234326824255\n",
      "0.75564681724846\n",
      "0.7558974358974359\n",
      "0.7561475409836066\n",
      "0.7563971340839304\n",
      "0.7556237218813906\n",
      "0.7558733401430031\n",
      "0.7551020408163265\n",
      "0.7553516819571865\n",
      "0.7556008146639511\n",
      "0.7558494404883012\n",
      "0.7560975609756098\n",
      "0.7553299492385787\n",
      "0.755578093306288\n",
      "0.7548125633232016\n",
      "0.7540485829959515\n",
      "0.7542972699696663\n",
      "0.7545454545454545\n",
      "0.7547931382441978\n",
      "0.7550403225806451\n",
      "0.7552870090634441\n",
      "0.755533199195171\n",
      "0.7547738693467336\n",
      "0.7550200803212851\n",
      "0.7552657973921765\n",
      "0.7555110220440882\n",
      "0.7557557557557557\n",
      "0.755\n",
      "0.7552447552447552\n",
      "0.7554890219560878\n",
      "0.7557328015952144\n",
      "0.7559760956175299\n",
      "0.7562189054726368\n",
      "0.7564612326043738\n",
      "0.7567030784508441\n",
      "0.7569444444444444\n",
      "0.757185332011893\n",
      "0.7574257425742574\n",
      "0.7576656775469832\n",
      "0.7579051383399209\n",
      "0.7581441263573544\n",
      "0.7583826429980276\n",
      "0.7586206896551724\n",
      "0.7588582677165354\n",
      "0.7581120943952803\n",
      "0.7573673870333988\n",
      "0.7576054955839058\n",
      "0.7578431372549019\n",
      "0.7580803134182175\n",
      "0.7583170254403131\n",
      "0.7585532746823069\n",
      "0.7587890625\n",
      "0.7590243902439024\n",
      "0.7592592592592593\n",
      "0.759493670886076\n",
      "0.7597276264591439\n",
      "0.7599611273080661\n",
      "0.7601941747572816\n",
      "0.7604267701260912\n",
      "0.7606589147286822\n",
      "0.7599225556631172\n",
      "0.7591876208897486\n",
      "0.7594202898550725\n",
      "0.7596525096525096\n",
      "0.759884281581485\n",
      "0.7601156069364162\n",
      "0.7603464870067372\n",
      "0.760576923076923\n",
      "0.760806916426513\n",
      "0.7610364683301344\n",
      "0.7612655800575263\n",
      "0.7614942528735632\n",
      "0.7607655502392344\n",
      "0.7609942638623327\n",
      "0.7612225405921681\n",
      "0.7614503816793893\n",
      "0.7616777883698761\n",
      "0.7619047619047619\n",
      "0.7611798287345385\n",
      "0.7614068441064639\n",
      "0.7606837606837606\n",
      "0.7609108159392789\n",
      "0.7601895734597156\n",
      "0.7604166666666666\n",
      "0.759697256385998\n",
      "0.7599243856332704\n",
      "0.7601510859301227\n",
      "0.7603773584905661\n",
      "0.760603204524034\n",
      "0.7608286252354048\n",
      "0.7601128880526811\n",
      "0.7603383458646616\n",
      "0.7605633802816901\n",
      "0.7607879924953096\n",
      "0.761012183692596\n",
      "0.7612359550561798\n",
      "0.7614593077642656\n",
      "0.7616822429906542\n",
      "0.7619047619047619\n",
      "0.7611940298507462\n",
      "0.7614165890027959\n",
      "0.7616387337057728\n",
      "0.761860465116279\n",
      "0.7611524163568774\n",
      "0.7613741875580315\n",
      "0.761595547309833\n",
      "0.7618164967562558\n",
      "0.7620370370370371\n",
      "0.7622571692876966\n",
      "0.7624768946395564\n",
      "0.76269621421976\n",
      "0.7629151291512916\n",
      "0.7622119815668202\n",
      "0.7624309392265194\n",
      "0.7626494940202392\n",
      "0.7628676470588235\n",
      "0.7621671258034894\n",
      "0.7623853211009174\n",
      "0.7626031164069661\n",
      "0.7628205128205128\n",
      "0.7630375114364135\n",
      "0.7632541133455211\n",
      "0.7625570776255708\n",
      "0.7618613138686131\n",
      "0.7620783956244302\n",
      "0.7622950819672131\n",
      "0.7625113739763422\n",
      "0.7627272727272727\n",
      "0.7629427792915532\n",
      "0.7631578947368421\n",
      "0.7633726201269265\n",
      "0.7635869565217391\n",
      "0.7638009049773755\n",
      "0.7640144665461122\n",
      "0.7642276422764228\n",
      "0.7635379061371841\n",
      "0.763751127141569\n",
      "0.763063063063063\n",
      "0.7632763276327633\n",
      "0.7634892086330936\n",
      "0.7637017070979335\n",
      "0.7639138240574507\n",
      "0.7641255605381166\n",
      "0.7634408602150538\n",
      "0.7636526410026858\n",
      "0.7638640429338104\n",
      "0.7631814119749777\n",
      "0.7633928571428571\n",
      "0.7636039250669046\n",
      "0.7638146167557932\n",
      "0.7640249332146037\n",
      "0.7642348754448398\n",
      "0.7644444444444445\n",
      "0.7646536412078153\n",
      "0.7648624667258208\n",
      "0.7650709219858156\n",
      "0.7652790079716564\n",
      "0.7654867256637168\n",
      "0.7656940760389036\n",
      "0.7659010600706714\n",
      "0.766107678729038\n",
      "0.7663139329805997\n",
      "0.7665198237885462\n",
      "0.766725352112676\n",
      "0.7669305189094108\n",
      "0.7671353251318102\n",
      "0.7673397717295873\n",
      "0.7666666666666667\n",
      "0.7668711656441718\n",
      "0.7661996497373029\n",
      "0.7664041994750657\n",
      "0.7666083916083916\n",
      "0.7668122270742358\n",
      "0.7661431064572426\n",
      "0.7663469921534438\n",
      "0.7656794425087108\n",
      "0.7658833768494343\n",
      "0.7652173913043478\n",
      "0.7654213727193745\n",
      "0.765625\n",
      "0.7658282740676496\n",
      "0.7660311958405546\n",
      "0.7662337662337663\n",
      "0.7664359861591695\n",
      "0.766637856525497\n",
      "0.7668393782383419\n",
      "0.7670405522001725\n",
      "0.7672413793103449\n",
      "0.7674418604651163\n",
      "0.7676419965576592\n",
      "0.7678417884780739\n",
      "0.7680412371134021\n",
      "0.7682403433476395\n",
      "0.7684391080617495\n",
      "0.7686375321336761\n",
      "0.7688356164383562\n",
      "0.7681779298545766\n",
      "0.7683760683760684\n",
      "0.7685738684884714\n",
      "0.7679180887372014\n",
      "0.7681159420289855\n",
      "0.768313458262351\n",
      "0.7685106382978724\n",
      "0.7687074829931972\n",
      "0.7689039932030586\n",
      "0.769100169779287\n",
      "0.7684478371501272\n",
      "0.7686440677966102\n",
      "0.768839966130398\n",
      "0.7690355329949239\n",
      "0.7692307692307693\n",
      "0.7694256756756757\n",
      "0.7687763713080169\n",
      "0.7689713322091062\n",
      "0.7683235046335299\n",
      "0.7676767676767676\n",
      "0.767031118587048\n",
      "0.7663865546218488\n",
      "0.7665827036104114\n",
      "0.7659395973154363\n",
      "0.7661357921207042\n",
      "0.7663316582914573\n",
      "0.7665271966527196\n",
      "0.7658862876254181\n",
      "0.7660818713450293\n",
      "0.7662771285475793\n",
      "0.7664720600500416\n",
      "0.7666666666666667\n",
      "0.7668609492089925\n",
      "0.7662229617304492\n",
      "0.7664172901080631\n",
      "0.7666112956810631\n",
      "0.7668049792531121\n",
      "0.7669983416252073\n",
      "0.7671913835956918\n",
      "0.7673841059602649\n",
      "0.7675765095119934\n",
      "0.7677685950413223\n",
      "0.7679603633360859\n",
      "0.7673267326732673\n",
      "0.7666941467436109\n",
      "0.7668863261943987\n",
      "0.7670781893004115\n",
      "0.7672697368421053\n",
      "0.7674609695973705\n",
      "0.7676518883415435\n",
      "0.7670221493027072\n",
      "0.7672131147540984\n",
      "0.7674037674037674\n",
      "0.7675941080196399\n",
      "0.7669664758789861\n",
      "0.7671568627450981\n",
      "0.7673469387755102\n",
      "0.767536704730832\n",
      "0.7677261613691931\n",
      "0.7679153094462541\n",
      "0.7672904800650936\n",
      "0.767479674796748\n",
      "0.7676685621445979\n",
      "0.7678571428571429\n",
      "0.7680454176804542\n",
      "0.7682333873581848\n",
      "0.7676113360323886\n",
      "0.767799352750809\n",
      "0.7679870654810024\n",
      "0.7681744749596123\n",
      "0.7675544794188862\n",
      "0.7669354838709678\n",
      "0.7671232876712328\n",
      "0.7673107890499195\n",
      "0.7674979887369268\n",
      "0.7676848874598071\n",
      "0.7670682730923695\n",
      "0.7664526484751204\n",
      "0.7666399358460305\n",
      "0.7668269230769231\n",
      "0.7662129703763011\n",
      "0.7656\n",
      "0.7657873701039168\n",
      "0.7659744408945687\n",
      "0.7661612130885874\n",
      "0.766347687400319\n",
      "0.7665338645418327\n",
      "0.76671974522293\n",
      "0.7669053301511536\n",
      "0.7670906200317965\n",
      "0.767275615567911\n",
      "0.7666666666666667\n",
      "0.7668517049960349\n",
      "0.7670364500792393\n",
      "0.7672209026128266\n",
      "0.7674050632911392\n",
      "0.7675889328063241\n",
      "0.7677725118483413\n",
      "0.7679558011049724\n",
      "0.7681388012618297\n",
      "0.7675334909377463\n",
      "0.7677165354330708\n",
      "0.7678992918961448\n",
      "0.7672955974842768\n",
      "0.7666928515318147\n",
      "0.7668759811616954\n",
      "0.7670588235294118\n",
      "0.7672413793103449\n",
      "0.7674236491777604\n",
      "0.7676056338028169\n",
      "0.7677873338545739\n",
      "0.7671875\n",
      "0.7673692427790788\n",
      "0.7675507020280812\n",
      "0.7677318784099766\n",
      "0.7671339563862928\n",
      "0.7673151750972763\n",
      "0.7674961119751167\n",
      "0.7668997668997669\n",
      "0.7670807453416149\n",
      "0.7672614429790535\n",
      "0.7674418604651163\n",
      "0.7676219984508134\n",
      "0.7678018575851393\n",
      "0.7679814385150812\n",
      "0.768160741885626\n",
      "0.7683397683397684\n",
      "0.7685185185185185\n",
      "0.7686969930609098\n",
      "0.7688751926040062\n",
      "0.7682832948421863\n",
      "0.7676923076923077\n",
      "0.7678708685626441\n",
      "0.7680491551459293\n",
      "0.7682271680736761\n",
      "0.7676380368098159\n",
      "0.7670498084291187\n",
      "0.7664624808575804\n",
      "0.7666411629686305\n",
      "0.7660550458715596\n",
      "0.7662337662337663\n",
      "0.766412213740458\n",
      "0.7658276125095347\n",
      "0.7660060975609756\n",
      "0.7661843107387661\n",
      "0.7663622526636226\n",
      "0.7665399239543726\n",
      "0.7667173252279635\n",
      "0.7668944570994685\n",
      "0.7670713201820941\n",
      "0.7664897649734648\n",
      "0.7666666666666667\n",
      "0.7668433005299016\n",
      "0.7670196671709532\n",
      "0.7671957671957672\n",
      "0.7666163141993958\n",
      "0.7660377358490567\n",
      "0.7662141779788839\n",
      "0.7663903541823662\n",
      "0.7658132530120482\n",
      "0.7652370203160271\n",
      "0.7646616541353384\n",
      "0.7640871525169046\n",
      "0.7642642642642643\n",
      "0.7644411102775694\n",
      "0.7646176911544228\n",
      "0.7647940074906368\n",
      "0.7649700598802395\n",
      "0.7643979057591623\n",
      "0.7645739910313901\n",
      "0.7647498132935027\n",
      "0.7649253731343284\n",
      "0.7643549589858315\n",
      "0.7645305514157973\n",
      "0.7647058823529411\n",
      "0.7648809523809523\n",
      "0.7643122676579925\n",
      "0.7644873699851411\n",
      "0.7646622123236823\n",
      "0.7648367952522255\n",
      "0.765011119347665\n",
      "0.7651851851851852\n",
      "0.764618800888231\n",
      "0.7640532544378699\n",
      "0.7634885439763488\n",
      "0.7629246676514032\n",
      "0.76309963099631\n",
      "0.7632743362831859\n",
      "0.763448784082535\n",
      "0.7636229749631811\n",
      "0.7630610743193524\n",
      "0.763235294117647\n",
      "0.7626745040411462\n",
      "0.762848751835536\n",
      "0.7622890682318415\n",
      "0.7624633431085044\n",
      "0.7619047619047619\n",
      "0.7620790629575402\n",
      "0.7622531089978054\n",
      "0.7624269005847953\n",
      "0.7626004382761139\n",
      "0.7627737226277372\n",
      "0.7629467541940189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7631195335276968\n",
      "0.763292061179898\n",
      "0.7634643377001455\n",
      "0.7629090909090909\n",
      "0.7630813953488372\n",
      "0.7632534495279594\n",
      "0.7634252539912917\n",
      "0.7635968092820885\n",
      "0.763768115942029\n",
      "0.7639391745112237\n",
      "0.76410998552822\n",
      "0.7642805495300072\n",
      "0.7644508670520231\n",
      "0.7646209386281588\n",
      "0.7647907647907648\n",
      "0.7642393655371305\n",
      "0.7644092219020173\n",
      "0.7638588912886969\n",
      "0.7633093525179856\n",
      "0.7634795111430626\n",
      "0.7636494252873564\n",
      "0.7638190954773869\n",
      "0.7639885222381636\n",
      "0.7634408602150538\n",
      "0.7628939828080229\n",
      "0.7630637079455977\n",
      "0.7632331902718169\n",
      "0.7626876340243031\n",
      "0.7628571428571429\n",
      "0.7630264097073519\n",
      "0.7624821683309557\n",
      "0.7626514611546685\n",
      "0.7621082621082621\n",
      "0.7615658362989324\n",
      "0.7617354196301565\n",
      "0.7619047619047619\n",
      "0.7620738636363636\n",
      "0.7622427253371186\n",
      "0.7624113475177305\n",
      "0.7625797306874557\n",
      "0.7627478753541076\n",
      "0.7629157820240623\n",
      "0.7630834512022631\n",
      "0.7632508833922261\n",
      "0.7634180790960452\n",
      "0.7635850388143967\n",
      "0.7637517630465445\n",
      "0.7639182522903453\n",
      "0.7640845070422535\n",
      "0.7642505277973258\n",
      "0.7644163150492265\n",
      "0.7645818692902319\n",
      "0.764747191011236\n",
      "0.7649122807017544\n",
      "0.7650771388499299\n",
      "0.764540995094604\n",
      "0.7640056022408963\n",
      "0.7634709587123862\n",
      "0.7636363636363637\n",
      "0.7638015373864431\n",
      "0.7639664804469274\n",
      "0.7634333565945569\n",
      "0.7635983263598326\n",
      "0.7637630662020906\n",
      "0.7639275766016713\n",
      "0.7640918580375783\n",
      "0.7635605006954103\n",
      "0.763724808895066\n",
      "0.7638888888888888\n",
      "0.7640527411519777\n",
      "0.7642163661581137\n",
      "0.7643797643797644\n",
      "0.7638504155124654\n",
      "0.7633217993079585\n",
      "0.7634854771784232\n",
      "0.7629578438147893\n",
      "0.763121546961326\n",
      "0.7632850241545893\n",
      "0.763448275862069\n",
      "0.7636113025499656\n",
      "0.7630853994490359\n",
      "0.7632484514796972\n",
      "0.7634112792297112\n",
      "0.763573883161512\n",
      "0.7637362637362637\n",
      "0.7638984214138641\n",
      "0.7640603566529492\n",
      "0.7635366689513365\n",
      "0.7636986301369864\n",
      "0.7638603696098563\n",
      "0.7633378932968536\n",
      "0.7634996582365003\n",
      "0.7629781420765027\n",
      "0.7631399317406143\n",
      "0.7633015006821282\n",
      "0.7627811860940695\n",
      "0.7629427792915532\n",
      "0.7631041524846834\n",
      "0.7625850340136054\n",
      "0.7627464309993202\n",
      "0.7622282608695652\n",
      "0.7617107942973523\n",
      "0.7618724559023067\n",
      "0.7620338983050847\n",
      "0.7621951219512195\n",
      "0.7623561272850372\n",
      "0.7618403247631935\n",
      "0.762001352265044\n",
      "0.7621621621621621\n",
      "0.762322754895341\n",
      "0.7624831309041835\n",
      "0.7619689817936615\n",
      "0.7621293800539084\n",
      "0.7622895622895622\n",
      "0.7624495289367429\n",
      "0.7626092804303968\n",
      "0.7620967741935484\n",
      "0.7615849563465413\n",
      "0.7610738255033557\n",
      "0.7605633802816901\n",
      "0.7607238605898123\n",
      "0.7602143335565975\n",
      "0.7603748326639893\n",
      "0.7605351170568562\n",
      "0.7606951871657754\n",
      "0.7608550434201736\n",
      "0.760347129506008\n",
      "0.7605070046697798\n",
      "0.76\n",
      "0.760159893404397\n",
      "0.7603195739014648\n",
      "0.7604790419161677\n",
      "0.7606382978723404\n",
      "0.760797342192691\n",
      "0.7609561752988048\n",
      "0.761114797611148\n",
      "0.7612732095490716\n",
      "0.7614314115308151\n",
      "0.7609271523178808\n",
      "0.7610853739245532\n",
      "0.7605820105820106\n",
      "0.7607402511566425\n",
      "0.7602377807133421\n",
      "0.7603960396039604\n",
      "0.7605540897097626\n",
      "0.7607119314436388\n",
      "0.7608695652173914\n",
      "0.761026991441738\n",
      "0.7611842105263158\n",
      "0.7613412228796844\n",
      "0.7614980289093298\n",
      "0.7616546290216678\n",
      "0.7618110236220472\n",
      "0.7619672131147541\n",
      "0.7621231979030144\n",
      "0.7616240995415848\n",
      "0.7617801047120419\n",
      "0.761935905820798\n",
      "0.7620915032679738\n",
      "0.7622468974526453\n",
      "0.762402088772846\n",
      "0.7625570776255708\n",
      "0.7627118644067796\n",
      "0.762214983713355\n",
      "0.7623697916666666\n",
      "0.7625243981782693\n",
      "0.7626788036410923\n",
      "0.7628330084470435\n",
      "0.762987012987013\n",
      "0.7624918883841661\n",
      "0.7619974059662775\n",
      "0.76150356448477\n",
      "0.7616580310880829\n",
      "0.7611650485436893\n",
      "0.7613195342820182\n",
      "0.7614738202973497\n",
      "0.7616279069767442\n",
      "0.7617817947062621\n",
      "0.7619354838709678\n",
      "0.7620889748549323\n",
      "0.7615979381443299\n",
      "0.7617514488087572\n",
      "0.7619047619047619\n",
      "0.7614147909967846\n",
      "0.7615681233933161\n",
      "0.7617212588310854\n",
      "0.7618741976893453\n",
      "0.7620269403463759\n",
      "0.7615384615384615\n",
      "0.7616912235746316\n",
      "0.7612035851472471\n",
      "0.7613563659628919\n",
      "0.7608695652173914\n",
      "0.7603833865814696\n",
      "0.7605363984674329\n",
      "0.7606892150606254\n",
      "0.7608418367346939\n",
      "0.7603569152326323\n",
      "0.7605095541401274\n",
      "0.7606619987269255\n",
      "0.7608142493638677\n",
      "0.7609663064208518\n",
      "0.761118170266836\n",
      "0.7606349206349207\n",
      "0.7607868020304569\n",
      "0.7609384908053266\n",
      "0.7610899873257287\n",
      "0.7612412919569348\n",
      "0.760759493670886\n",
      "0.7609108159392789\n",
      "0.7610619469026548\n",
      "0.7612128869235628\n",
      "0.7607323232323232\n",
      "0.7608832807570978\n",
      "0.7604035308953342\n",
      "0.7599243856332704\n",
      "0.7600755667506297\n",
      "0.7602265575833858\n",
      "0.7603773584905661\n",
      "0.759899434318039\n",
      "0.7600502512562815\n",
      "0.7602008788449467\n",
      "0.7603513174404015\n",
      "0.7598746081504703\n",
      "0.7600250626566416\n",
      "0.7595491546649968\n",
      "0.7590738423028786\n",
      "0.7592245153220762\n",
      "0.75875\n",
      "0.7589006870705809\n",
      "0.7590511860174781\n",
      "0.7592014971927635\n",
      "0.7593516209476309\n",
      "0.7588785046728972\n",
      "0.7590286425902865\n",
      "0.7585563161169881\n",
      "0.7587064676616916\n",
      "0.7588564325668117\n",
      "0.7590062111801242\n",
      "0.7591558038485413\n",
      "0.7593052109181141\n",
      "0.759454432734036\n",
      "0.7596034696406444\n",
      "0.7591331269349845\n",
      "0.7592821782178217\n",
      "0.7594310451453309\n",
      "0.7595797280593325\n",
      "0.7597282273008029\n",
      "0.7598765432098765\n",
      "0.7600246761258482\n",
      "0.7601726263871763\n",
      "0.7597042513863216\n",
      "0.7598522167487685\n",
      "0.76\n",
      "0.7601476014760148\n",
      "0.7602950215119852\n",
      "0.7604422604422605\n",
      "0.7599754450583179\n",
      "0.7601226993865031\n",
      "0.7602697731453096\n",
      "0.7604166666666666\n",
      "0.7605633802816901\n",
      "0.7607099143206855\n",
      "0.7608562691131499\n",
      "0.761002444987775\n",
      "0.7605375687232743\n",
      "0.7606837606837606\n",
      "0.760829774252593\n",
      "0.7603658536585366\n",
      "0.7605118829981719\n",
      "0.7600487210718636\n",
      "0.7601947656725502\n",
      "0.7603406326034063\n",
      "0.7604863221884498\n",
      "0.7600243013365735\n",
      "0.7601700060716454\n",
      "0.7603155339805825\n",
      "0.7604608853850818\n",
      "0.76\n",
      "0.7601453664445791\n",
      "0.7602905569007264\n",
      "0.7604355716878403\n",
      "0.7605804111245466\n",
      "0.760725075528701\n",
      "0.7608695652173914\n",
      "0.7610138805069403\n",
      "0.7611580217129071\n",
      "0.7606992163954189\n",
      "0.7608433734939759\n",
      "0.7609873570138471\n",
      "0.7611311672683514\n",
      "0.7612748045700541\n",
      "0.7614182692307693\n",
      "0.760960960960961\n",
      "0.7611044417767107\n",
      "0.76124775044991\n",
      "0.7613908872901679\n",
      "0.7615338526063511\n",
      "0.7610778443113773\n",
      "0.760622381807301\n",
      "0.7607655502392344\n",
      "0.7609085475194262\n",
      "0.7610513739545998\n",
      "0.7611940298507462\n",
      "0.7613365155131265\n",
      "0.7614788312462731\n",
      "0.7616209773539928\n",
      "0.7611673615247171\n",
      "0.7613095238095238\n",
      "0.7614515169541939\n",
      "0.760998810939358\n",
      "0.7605466428995841\n",
      "0.7606888361045131\n",
      "0.7608308605341246\n",
      "0.7609727164887308\n",
      "0.7605216360403082\n",
      "0.7600710900473934\n",
      "0.7602131438721137\n",
      "0.7597633136094675\n",
      "0.7599053814311059\n",
      "0.7600472813238771\n",
      "0.7601890135853514\n",
      "0.7603305785123967\n",
      "0.7604719764011799\n",
      "0.7600235849056604\n",
      "0.7601649970536241\n",
      "0.7603062426383981\n",
      "0.7598587404355504\n",
      "0.76\n",
      "0.7601410934744268\n",
      "0.7602820211515864\n",
      "0.7604227833235467\n",
      "0.7605633802816901\n",
      "0.7601173020527859\n",
      "0.7602579132473622\n",
      "0.760398359695372\n",
      "0.7605386416861827\n",
      "0.7606787595084845\n",
      "0.7608187134502924\n",
      "0.7603740502630041\n",
      "0.759929906542056\n",
      "0.7600700525394045\n",
      "0.7602100350058343\n",
      "0.7603498542274052\n",
      "0.7604895104895105\n",
      "0.7600465928945835\n",
      "0.759604190919674\n",
      "0.7597440372309482\n",
      "0.7598837209302326\n",
      "0.7600232423009878\n",
      "0.759581881533101\n",
      "0.759141033081834\n",
      "0.7592807424593968\n",
      "0.758840579710145\n",
      "0.7584009269988412\n",
      "0.7579617834394905\n",
      "0.7575231481481481\n",
      "0.757085020242915\n",
      "0.7572254335260116\n",
      "0.756787983824379\n",
      "0.75635103926097\n",
      "0.7559145989613387\n",
      "0.7560553633217993\n",
      "0.7561959654178675\n",
      "0.756336405529954\n",
      "0.7559009786989062\n",
      "0.7554660529344074\n",
      "0.7556066705002875\n",
      "0.7557471264367817\n",
      "0.7558874210224009\n",
      "0.7560275545350172\n",
      "0.7561675272518646\n",
      "0.7563073394495413\n",
      "0.7564469914040115\n",
      "0.7560137457044673\n",
      "0.7561534058385804\n",
      "0.7562929061784897\n",
      "0.7564322469982847\n",
      "0.7565714285714286\n",
      "0.7567104511707595\n",
      "0.7568493150684932\n",
      "0.7569880205362236\n",
      "0.7565564424173318\n",
      "0.7561253561253561\n",
      "0.7556947608200456\n",
      "0.755264655663062\n",
      "0.7554038680318543\n",
      "0.755542922114838\n",
      "0.7556818181818182\n",
      "0.7558205565019875\n",
      "0.7559591373439274\n",
      "0.7555303460011344\n",
      "0.7556689342403629\n",
      "0.7558073654390934\n",
      "0.7559456398640997\n",
      "0.7555178268251274\n",
      "0.755656108597285\n",
      "0.7557942340305257\n",
      "0.7553672316384181\n",
      "0.7555053642010163\n",
      "0.7556433408577878\n",
      "0.7557811618725324\n",
      "0.7559188275084555\n",
      "0.756056338028169\n",
      "0.7561936936936937\n",
      "0.7563308947664603\n",
      "0.7564679415073116\n",
      "0.7566048341765037\n",
      "0.7567415730337078\n",
      "0.7568781583380123\n",
      "0.7570145903479237\n",
      "0.7565900168255749\n",
      "0.7561659192825112\n",
      "0.7563025210084033\n",
      "0.7558790593505039\n",
      "0.7560156687185227\n",
      "0.756152125279642\n",
      "0.7557294577976523\n",
      "0.7558659217877095\n",
      "0.756002233389168\n",
      "0.7561383928571429\n",
      "0.7562744004461796\n",
      "0.7564102564102564\n",
      "0.7559888579387186\n",
      "0.7561247216035635\n",
      "0.7562604340567612\n",
      "0.7558398220244716\n",
      "0.7559755419677598\n",
      "0.7561111111111111\n",
      "0.7556912826207662\n",
      "0.755826859045505\n",
      "0.7554076539101497\n",
      "0.7555432372505543\n",
      "0.7556786703601108\n",
      "0.7558139534883721\n",
      "0.7559490868843387\n",
      "0.7560840707964602\n",
      "0.7562189054726368\n",
      "0.7563535911602209\n",
      "0.7564881281060187\n",
      "0.7566225165562914\n",
      "0.7562051847766134\n",
      "0.7563395810363837\n",
      "0.756473829201102\n",
      "0.7560572687224669\n",
      "0.7561915244909191\n",
      "0.7563256325632564\n",
      "0.7564595931830677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7565934065934066\n",
      "0.756727073036793\n",
      "0.756860592755214\n",
      "0.7569939659901261\n",
      "0.7571271929824561\n",
      "0.7572602739726028\n",
      "0.7568455640744798\n",
      "0.7569786535303776\n",
      "0.7571115973741794\n",
      "0.7572443958447239\n",
      "0.7573770491803279\n",
      "0.7575095576187876\n",
      "0.75764192139738\n",
      "0.7577741407528642\n",
      "0.757360959651036\n",
      "0.7574931880108992\n",
      "0.7576252723311547\n",
      "0.7577572128470332\n",
      "0.7578890097932536\n",
      "0.758020663404024\n",
      "0.7581521739130435\n",
      "0.7582835415535035\n",
      "0.758414766558089\n",
      "0.7585458491589799\n",
      "0.7586767895878525\n",
      "0.7588075880758808\n",
      "0.7589382448537378\n",
      "0.758527341635084\n",
      "0.7581168831168831\n",
      "0.7577068685776095\n",
      "0.7578378378378379\n",
      "0.7579686655861696\n",
      "0.7580993520518359\n",
      "0.7582298974635726\n",
      "0.7583603020496225\n",
      "0.7584905660377359\n",
      "0.7580818965517241\n",
      "0.7582121701669359\n",
      "0.7583423035522067\n",
      "0.7584722969338354\n",
      "0.7586021505376344\n",
      "0.7587318645889307\n",
      "0.7588614393125671\n",
      "0.7584541062801933\n",
      "0.7580472103004292\n",
      "0.7581769436997319\n",
      "0.7583065380493034\n",
      "0.7584359935725763\n",
      "0.7585653104925053\n",
      "0.7581594435527019\n",
      "0.758288770053476\n",
      "0.757883484767504\n",
      "0.7580128205128205\n",
      "0.7581420181526962\n",
      "0.7582710779082177\n",
      "0.7584\n",
      "0.7585287846481876\n",
      "0.7586574320724561\n",
      "0.7587859424920128\n",
      "0.7589143161255987\n",
      "0.7590425531914894\n",
      "0.759170653907496\n",
      "0.759298618490967\n",
      "0.7594264471587892\n",
      "0.7590233545647559\n",
      "0.7591511936339522\n",
      "0.7587486744432662\n",
      "0.7583465818759937\n",
      "0.7584745762711864\n",
      "0.7580730545262043\n",
      "0.7582010582010582\n",
      "0.7583289264939186\n",
      "0.7584566596194503\n",
      "0.7585842577918648\n",
      "0.7587117212249208\n",
      "0.7588390501319261\n",
      "0.7584388185654009\n",
      "0.7585661570901423\n",
      "0.7586933614330874\n",
      "0.7588204318062138\n",
      "0.758421052631579\n",
      "0.7585481325618095\n",
      "0.7586750788643533\n",
      "0.7582764056752496\n",
      "0.7584033613445378\n",
      "0.7580052493438321\n",
      "0.7581322140608604\n",
      "0.7582590456213949\n",
      "0.7583857442348009\n",
      "0.7579884756416972\n",
      "0.7581151832460733\n",
      "0.7582417582417582\n",
      "0.7583682008368201\n",
      "0.7584945112388918\n",
      "0.7586206896551724\n",
      "0.7587467362924282\n",
      "0.7588726513569938\n",
      "0.758998435054773\n",
      "0.7586027111574557\n",
      "0.7582073996873372\n",
      "0.7583333333333333\n",
      "0.7584591358667361\n",
      "0.7585848074921956\n",
      "0.7581903276131046\n",
      "0.7583160083160083\n",
      "0.7579220779220779\n",
      "0.7580477673935618\n",
      "0.7581733264141152\n",
      "0.7577800829875518\n",
      "0.7579056505961638\n",
      "0.7580310880829015\n",
      "0.7581563956499223\n",
      "0.7582815734989649\n",
      "0.7584066218313502\n",
      "0.7585315408479835\n",
      "0.758656330749354\n",
      "0.7582644628099173\n",
      "0.7583892617449665\n",
      "0.7585139318885449\n",
      "0.7586384734399175\n",
      "0.7587628865979381\n",
      "0.758887171561051\n",
      "0.758496395468589\n",
      "0.7581060216160577\n",
      "0.7582304526748971\n",
      "0.757840616966581\n",
      "0.7579650565262076\n",
      "0.7580893682588598\n",
      "0.7582135523613963\n",
      "0.7578245253976398\n",
      "0.757948717948718\n",
      "0.7580727831881087\n",
      "0.7581967213114754\n",
      "0.7583205325140809\n",
      "0.7584442169907881\n",
      "0.7585677749360614\n",
      "0.7586912065439673\n",
      "0.7583035258048033\n",
      "0.7584269662921348\n",
      "0.7585502807554875\n",
      "0.7586734693877552\n",
      "0.7582865884752678\n",
      "0.7584097859327217\n",
      "0.7585328578706062\n",
      "0.7581466395112016\n",
      "0.7577608142493639\n",
      "0.757884028484232\n",
      "0.7580071174377224\n",
      "0.758130081300813\n",
      "0.7582529202640934\n",
      "0.7583756345177665\n",
      "0.7584982242516489\n",
      "0.7586206896551724\n",
      "0.7582361885453623\n",
      "0.7583586626139818\n",
      "0.7584810126582279\n",
      "0.7580971659919028\n",
      "0.7582195245321194\n",
      "0.7583417593528817\n",
      "0.7584638706417383\n",
      "0.7580808080808081\n",
      "0.7582029278142353\n",
      "0.7578203834510595\n",
      "0.7579425113464447\n",
      "0.7580645161290323\n",
      "0.7581863979848866\n",
      "0.7583081570996979\n",
      "0.758429793658782\n",
      "0.7585513078470825\n",
      "0.7586726998491704\n",
      "0.7582914572864322\n",
      "0.7584128578603717\n",
      "0.7585341365461847\n",
      "0.7581535373808329\n",
      "0.7582748244734202\n",
      "0.7578947368421053\n",
      "0.7580160320641283\n",
      "0.758137205808713\n",
      "0.7577577577577578\n",
      "0.7578789394697348\n",
      "0.7575\n",
      "0.7576211894052973\n",
      "0.7577422577422578\n",
      "0.7578632051922117\n",
      "0.7574850299401198\n",
      "0.7576059850374065\n",
      "0.7577268195413759\n",
      "0.757847533632287\n",
      "0.7579681274900398\n",
      "0.7575908412145346\n",
      "0.7572139303482587\n",
      "0.757334659373446\n",
      "0.7569582504970179\n",
      "0.7570789865871833\n",
      "0.7571996027805362\n",
      "0.7573200992555831\n",
      "0.7574404761904762\n",
      "0.7575607337630144\n",
      "0.7576808721506442\n",
      "0.7578008915304606\n",
      "0.7579207920792079\n",
      "0.7580405739732805\n",
      "0.7581602373887241\n",
      "0.7582797825012358\n",
      "0.7579051383399209\n",
      "0.7580246913580246\n",
      "0.7576505429417572\n",
      "0.7577701036013813\n",
      "0.7578895463510849\n",
      "0.7580088713652046\n",
      "0.758128078817734\n",
      "0.758247168882324\n",
      "0.7583661417322834\n",
      "0.7579931136251845\n",
      "0.7581120943952803\n",
      "0.7577395577395577\n",
      "0.7578585461689588\n",
      "0.757486499754541\n",
      "0.7576054955839058\n",
      "0.7572339382050024\n",
      "0.7573529411764706\n",
      "0.7569818716315532\n",
      "0.7566111655239961\n",
      "0.7567302985805189\n",
      "0.7568493150684932\n",
      "0.7569682151589242\n",
      "0.7570869990224829\n",
      "0.7572056668295066\n",
      "0.75732421875\n",
      "0.7574426549536359\n",
      "0.7570731707317073\n",
      "0.7571916138469039\n",
      "0.75682261208577\n",
      "0.7564539698002922\n",
      "0.7565725413826679\n",
      "0.7566909975669099\n",
      "0.7568093385214008\n",
      "0.7569275644141954\n",
      "0.7570456754130224\n",
      "0.7571636716852841\n",
      "0.7572815533980582\n",
      "0.757399320718098\n",
      "0.7575169738118331\n",
      "0.7576345128453709\n",
      "0.7577519379844961\n",
      "0.7578692493946732\n",
      "0.7579864472410455\n",
      "0.7581035316884374\n",
      "0.758220502901354\n",
      "0.7583373610439826\n",
      "0.7584541062801933\n",
      "0.7585707387735393\n",
      "0.7586872586872587\n",
      "0.7583212735166426\n",
      "0.7584378013500482\n",
      "0.7585542168674699\n",
      "0.7586705202312138\n",
      "0.7587867116032739\n",
      "0.7584215591915303\n",
      "0.7585377585377585\n",
      "0.7586538461538461\n",
      "0.758769822200865\n",
      "0.7584053794428435\n",
      "0.7580412866058569\n",
      "0.7581573896353166\n",
      "0.758273381294964\n",
      "0.7583892617449665\n",
      "0.7580258744609487\n",
      "0.7581417624521073\n",
      "0.7582575394925802\n",
      "0.7583732057416268\n",
      "0.7580105212816834\n",
      "0.7581261950286807\n",
      "0.7582417582417582\n",
      "0.7583572110792741\n",
      "0.7579952267303103\n",
      "0.7581106870229007\n",
      "0.7582260371959942\n",
      "0.7583412774070544\n",
      "0.7584564078132444\n",
      "0.7585714285714286\n",
      "0.7586863398381724\n",
      "0.7588011417697431\n",
      "0.7589158345221113\n",
      "0.7590304182509505\n",
      "0.7586698337292161\n",
      "0.758309591642925\n",
      "0.7584242999525391\n",
      "0.7580645161290323\n",
      "0.7581792318634424\n",
      "0.7582938388625592\n",
      "0.7584083372809095\n",
      "0.7585227272727273\n",
      "0.7581637482252721\n",
      "0.7578051087984863\n",
      "0.757919621749409\n",
      "0.7575614366729678\n",
      "0.757203589985829\n",
      "0.7568460812086875\n",
      "0.7569608305804625\n",
      "0.7570754716981132\n",
      "0.7567185289957568\n",
      "0.7568331762488218\n",
      "0.7569477154969383\n",
      "0.7570621468926554\n",
      "0.7571764705882353\n",
      "0.7568203198494826\n",
      "0.7569346497414199\n",
      "0.7570488721804511\n",
      "0.7571629873179897\n",
      "0.7572769953051643\n",
      "0.7569216330361332\n",
      "0.7570356472795498\n",
      "0.757149554617909\n",
      "0.7567947516401125\n",
      "0.7569086651053865\n",
      "0.7565543071161048\n",
      "0.756200280767431\n",
      "0.7563143124415341\n",
      "0.7564282374941561\n",
      "0.7565420560747663\n",
      "0.7566557683325549\n",
      "0.7567693744164332\n",
      "0.756882874475035\n",
      "0.7569962686567164\n",
      "0.7571095571095571\n",
      "0.7572227399813607\n",
      "0.7573358174196554\n",
      "0.7574487895716946\n",
      "0.7570963238715682\n",
      "0.7572093023255814\n",
      "0.7568572756857276\n",
      "0.7569702602230484\n",
      "0.7570831398049234\n",
      "0.7571959145775302\n",
      "0.757308584686775\n",
      "0.7574211502782932\n",
      "0.7570700046360687\n",
      "0.7571825764596849\n",
      "0.7572950440018527\n",
      "0.7574074074074074\n",
      "0.7575196668209162\n",
      "0.757631822386679\n",
      "0.7577438742487286\n",
      "0.7573937153419593\n",
      "0.7575057736720554\n",
      "0.7571560480147738\n",
      "0.7568066451315182\n",
      "0.7569188191881919\n",
      "0.7570308898109728\n",
      "0.7571428571428571\n",
      "0.7572547213265776\n",
      "0.757366482504604\n",
      "0.7570179475379659\n",
      "0.7571297148114076\n",
      "0.7572413793103449\n",
      "0.7568933823529411\n",
      "0.7570050528249885\n",
      "0.7571166207529844\n",
      "0.7572280862781092\n",
      "0.7573394495412844\n",
      "0.7574507106831728\n",
      "0.7575618698441796\n",
      "0.7576729271644526\n",
      "0.7573260073260073\n",
      "0.7574370709382151\n",
      "0.757548032936871\n",
      "0.757201646090535\n",
      "0.7573126142595978\n",
      "0.7574234810415715\n",
      "0.7575342465753425\n",
      "0.7576449109995436\n",
      "0.7577554744525548\n",
      "0.7578659370725034\n",
      "0.7579762989972653\n",
      "0.757630979498861\n",
      "0.7577413479052824\n",
      "0.7578516158397816\n",
      "0.7579617834394905\n",
      "0.7580718508412915\n",
      "0.7581818181818182\n",
      "0.7582916855974557\n",
      "0.7579473206176204\n",
      "0.758057194734453\n",
      "0.7577132486388385\n",
      "0.7573696145124716\n",
      "0.757479601087942\n",
      "0.7575894879927504\n",
      "0.7576992753623188\n",
      "0.7578089633318243\n",
      "0.7579185520361991\n",
      "0.7580280416101312\n",
      "0.7576853526220615\n",
      "0.7577948486217804\n",
      "0.7579042457091237\n",
      "0.7580135440180586\n",
      "0.7581227436823105\n",
      "0.7582318448353631\n",
      "0.7578899909828675\n",
      "0.7575484452456062\n",
      "0.7576576576576577\n",
      "0.7577667717244484\n",
      "0.7578757875787578\n",
      "0.7575348627980207\n",
      "0.7576438848920863\n",
      "0.7577528089887641\n",
      "0.7578616352201258\n",
      "0.7579703637180063\n",
      "0.7576301615798923\n",
      "0.7577388963660835\n",
      "0.757847533632287\n",
      "0.7579560735096369\n",
      "0.7580645161290323\n",
      "0.7581728616211375\n",
      "0.7582811101163832\n",
      "0.7583892617449665\n",
      "0.7584973166368515\n",
      "0.7586052749217702\n",
      "0.7587131367292225\n",
      "0.758820902188477\n",
      "0.7589285714285714\n",
      "0.7590361445783133\n",
      "0.7591436217662801\n",
      "0.7592510031208203\n",
      "0.7593582887700535\n",
      "0.7590200445434299\n",
      "0.7591273374888691\n",
      "0.7592345349354696\n",
      "0.7593416370106761\n",
      "0.7594486438417074\n",
      "0.7595555555555555\n",
      "0.7596623722789871\n",
      "0.7597690941385435\n",
      "0.7598757212605415\n",
      "0.7599822537710736\n",
      "0.7600886917960089\n",
      "0.7601950354609929\n",
      "0.7603012848914489\n",
      "0.7604074402125776\n",
      "0.7600708277999114\n",
      "0.7601769911504425\n",
      "0.7602830605926582\n",
      "0.7603890362511052\n",
      "0.7604949182501105\n",
      "0.7606007067137809\n",
      "0.7602649006622516\n",
      "0.7599293909973521\n",
      "0.7600352889280988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7601410934744268\n",
      "0.7602468047598061\n",
      "0.7599118942731278\n",
      "0.7600176133861735\n",
      "0.7601232394366197\n",
      "0.7602287725472944\n",
      "0.7598944591029023\n",
      "0.76\n",
      "0.7601054481546573\n",
      "0.7602108036890646\n",
      "0.7603160667251976\n",
      "0.7599824484422992\n",
      "0.7600877192982456\n",
      "0.7601928978518194\n",
      "0.7602979842243646\n",
      "0.7604029785370127\n",
      "0.760507880910683\n",
      "0.7606126914660831\n",
      "0.7607174103237095\n",
      "0.7608220376038478\n",
      "0.7609265734265734\n",
      "0.7610310179117519\n",
      "0.7611353711790393\n",
      "0.761239633347883\n",
      "0.7613438045375218\n",
      "0.7610117749672918\n",
      "0.7611159546643418\n",
      "0.7612200435729848\n",
      "0.7608885017421603\n",
      "0.760992599042229\n",
      "0.7606614447345518\n",
      "0.7603305785123967\n",
      "0.7604347826086957\n",
      "0.7605388961321164\n",
      "0.760642919200695\n",
      "0.7607468519322622\n",
      "0.7604166666666666\n",
      "0.7605206073752712\n",
      "0.7606244579358196\n",
      "0.7602947550931947\n",
      "0.7603986135181976\n",
      "0.7605023819835427\n",
      "0.7606060606060606\n",
      "0.76070964950238\n",
      "0.7608131487889274\n",
      "0.7604842196281885\n",
      "0.7601555747623163\n",
      "0.7602591792656588\n",
      "0.7603626943005182\n",
      "0.7604661199827363\n",
      "0.7601380500431406\n",
      "0.7602414833980163\n",
      "0.7603448275862069\n",
      "0.7604480827229643\n",
      "0.7605512489233419\n",
      "0.7606543263021954\n",
      "0.7607573149741824\n",
      "0.7608602150537634\n",
      "0.7609630266552021\n",
      "0.7610657498925655\n",
      "0.7607388316151202\n",
      "0.7608415629025332\n",
      "0.7609442060085837\n",
      "0.7610467610467611\n",
      "0.7611492281303602\n",
      "0.760822974710673\n",
      "0.760497000856898\n",
      "0.7605995717344753\n",
      "0.7607020547945206\n",
      "0.7608044501497646\n",
      "0.760906757912746\n",
      "0.7610089781958101\n",
      "0.7611111111111111\n",
      "0.7612131567706109\n",
      "0.7613151152860803\n",
      "0.7614169867690994\n",
      "0.7610921501706485\n",
      "0.7611940298507462\n",
      "0.7612958226768969\n",
      "0.7613975287601193\n",
      "0.7614991482112436\n",
      "0.7611749680715197\n",
      "0.7612765957446809\n",
      "0.7613781369629945\n",
      "0.7614795918367347\n",
      "0.7615809604759881\n",
      "0.7616822429906542\n",
      "0.7617834394904459\n",
      "0.7614601018675722\n",
      "0.7615613067458634\n",
      "0.7612383375742154\n",
      "0.761339550657058\n",
      "0.7614406779661017\n",
      "0.7615417196103346\n",
      "0.7612193056731583\n",
      "0.7613203554803216\n",
      "0.7614213197969543\n",
      "0.7615221987315011\n",
      "0.7616229923922232\n",
      "0.761723700887199\n",
      "0.7618243243243243\n",
      "0.7619248628113128\n",
      "0.7620253164556962\n",
      "0.762125685364825\n",
      "0.7622259696458684\n",
      "0.7623261694058154\n",
      "0.7624262847514743\n",
      "0.7621052631578947\n",
      "0.7622053872053872\n",
      "0.7623054270088346\n",
      "0.7624053826745164\n",
      "0.762505254308533\n",
      "0.7626050420168067\n",
      "0.7627047459050819\n",
      "0.762384550797649\n",
      "0.7624842635333613\n",
      "0.7625838926174496\n",
      "0.7622641509433963\n",
      "0.7623637887678122\n",
      "0.7620444072056975\n",
      "0.7621440536013401\n",
      "0.7618250313938887\n",
      "0.7615062761506276\n",
      "0.7616060225846926\n",
      "0.7617056856187291\n",
      "0.7618052653572921\n",
      "0.7619047619047619\n",
      "0.7615866388308977\n",
      "0.7612687813021702\n",
      "0.7613683771380892\n",
      "0.7610508757297748\n",
      "0.7611504793664027\n",
      "0.76125\n",
      "0.7613494377342774\n",
      "0.7614487926727727\n",
      "0.7615480649188514\n",
      "0.7612312811980033\n",
      "0.760914760914761\n",
      "0.7610141313383209\n",
      "0.7606979642708767\n",
      "0.760797342192691\n",
      "0.7604815276048152\n",
      "0.7605809128630705\n",
      "0.7602654500207383\n",
      "0.7603648424543947\n",
      "0.7600497306257771\n",
      "0.7597348798674399\n",
      "0.7598343685300207\n",
      "0.7599337748344371\n",
      "0.7600330988829127\n",
      "0.760132340777502\n",
      "0.7602315006200909\n",
      "0.7603305785123967\n",
      "0.7604295745559686\n",
      "0.7605284888521883\n",
      "0.7606273215022699\n",
      "0.7603135313531353\n",
      "0.7604123711340206\n",
      "0.7605111294311624\n",
      "0.7606098063452822\n",
      "0.7602965403624382\n",
      "0.7603952243721697\n",
      "0.7604938271604939\n",
      "0.7605923488276429\n",
      "0.7602796052631579\n",
      "0.7603781339909577\n",
      "0.7604765817584224\n",
      "0.760164271047228\n",
      "0.7602627257799671\n",
      "0.7603610997127616\n",
      "0.760459392945037\n",
      "0.7605576055760558\n",
      "0.760655737704918\n",
      "0.7607537894305613\n",
      "0.7604422604422605\n",
      "0.7605403192795743\n",
      "0.7606382978723404\n",
      "0.7607361963190185\n",
      "0.7608340147179068\n",
      "0.7609317531671435\n",
      "0.7610294117647058\n",
      "0.7611269906084116\n",
      "0.7608163265306123\n",
      "0.7609139126886985\n",
      "0.7610114192495921\n",
      "0.76110884631064\n",
      "0.7612061939690301\n",
      "0.7613034623217922\n",
      "0.761400651465798\n",
      "0.7610907610907611\n",
      "0.7607811228641171\n",
      "0.7604717364782432\n",
      "0.760569105691057\n",
      "0.7606663957740756\n",
      "0.7607636068237206\n",
      "0.7608607389362566\n",
      "0.7609577922077922\n",
      "0.76105476673428\n",
      "0.7607461476074615\n",
      "0.7608431293068504\n",
      "0.7609400324149108\n",
      "0.7610368570271365\n",
      "0.7611336032388664\n",
      "0.7612302711452853\n",
      "0.761326860841424\n",
      "0.7614233724221593\n",
      "0.761519805982215\n",
      "0.7616161616161616\n",
      "0.7617124394184168\n",
      "0.7614049253128785\n",
      "0.761501210653753\n",
      "0.7615974183138362\n",
      "0.7616935483870968\n",
      "0.7617896009673518\n",
      "0.7618855761482676\n",
      "0.7619814740233588\n",
      "0.7620772946859904\n",
      "0.7621730382293762\n",
      "0.7622687047465808\n",
      "0.7623642943305187\n",
      "0.762459807073955\n",
      "0.7625552430695058\n",
      "0.7626506024096386\n",
      "0.762745885186672\n",
      "0.7628410914927769\n",
      "0.762936221419976\n",
      "0.7630312750601443\n",
      "0.76312625250501\n",
      "0.7632211538461539\n",
      "0.76331597917501\n",
      "0.7634107285828663\n",
      "0.7635054021608644\n",
      "0.7632\n",
      "0.7632946821271491\n",
      "0.7633892885691447\n",
      "0.7634838194167\n",
      "0.7635782747603834\n",
      "0.7636726546906187\n",
      "0.7637669592976856\n",
      "0.7638611886717191\n",
      "0.7639553429027113\n",
      "0.7640494220805102\n",
      "0.7641434262948207\n",
      "0.7642373556352051\n",
      "0.7643312101910829\n",
      "0.7640270592916832\n",
      "0.7641209228321401\n",
      "0.7642147117296223\n",
      "0.764308426073132\n",
      "0.7644020659515296\n",
      "0.7640984908657665\n",
      "0.7641921397379913\n",
      "0.7642857142857142\n",
      "0.764379214597382\n",
      "0.7644726407613005\n",
      "0.7645659928656362\n",
      "0.7642630744849446\n",
      "0.7643564356435644\n",
      "0.764449722882027\n",
      "0.7645429362880887\n",
      "0.7646360759493671\n",
      "0.7647291419533413\n",
      "0.7644268774703558\n",
      "0.76451995258791\n",
      "0.764218009478673\n",
      "0.7643110935649428\n",
      "0.7644041041831097\n",
      "0.7644970414201183\n",
      "0.764589905362776\n",
      "0.7646826960977533\n",
      "0.764775413711584\n",
      "0.7648680582906656\n",
      "0.7649606299212598\n",
      "0.7650531286894924\n",
      "0.7647521636506688\n",
      "0.7648446716476602\n",
      "0.764937106918239\n",
      "0.7650294695481336\n",
      "0.7647289866457188\n",
      "0.7648213584609345\n",
      "0.7645211930926217\n",
      "0.7646135739505688\n",
      "0.7647058823529411\n",
      "0.764798118384947\n",
      "0.7644984326018809\n",
      "0.764198981590286\n",
      "0.7638997650743931\n",
      "0.7639921722113503\n",
      "0.7640845070422535\n",
      "0.7641767696519358\n",
      "0.7642689601250977\n",
      "0.7643610785463072\n",
      "0.7640625\n",
      "0.7637641546270988\n",
      "0.7638563622170179\n",
      "0.7639484978540773\n",
      "0.7640405616224649\n",
      "0.7641325536062378\n",
      "0.7642244738893219\n",
      "0.7643163225555123\n",
      "0.7644080996884736\n",
      "0.76449980537174\n",
      "0.7642023346303501\n",
      "0.764294049008168\n",
      "0.7639968895800933\n",
      "0.7640886125145744\n",
      "0.7637917637917638\n",
      "0.7634951456310679\n",
      "0.7635869565217391\n",
      "0.7636786961583236\n",
      "0.7637703646237394\n",
      "0.7638619620007755\n",
      "0.7635658914728682\n",
      "0.7636574970941495\n",
      "0.7633617350890782\n",
      "0.7630662020905923\n",
      "0.7627708978328174\n",
      "0.7628626692456479\n",
      "0.762954369682908\n",
      "0.7626594511016621\n",
      "0.7623647604327666\n",
      "0.7624565469293163\n",
      "0.7621621621621621\n",
      "0.7622539560015438\n",
      "0.7623456790123457\n",
      "0.762051677593521\n",
      "0.7621434078643022\n",
      "0.761849710982659\n",
      "0.7619414483821263\n",
      "0.7616480554485945\n",
      "0.7617397998460355\n",
      "0.761446710273182\n",
      "0.7611538461538462\n",
      "0.7612456747404844\n",
      "0.7613374327440431\n",
      "0.7610449481367653\n",
      "0.761136712749616\n",
      "0.7612284069097889\n",
      "0.7609363008442057\n",
      "0.7610280015343307\n",
      "0.7611196319018405\n",
      "0.7608279034112687\n",
      "0.7609195402298851\n",
      "0.7606281118345462\n",
      "0.7603369065849923\n",
      "0.7604286261002678\n",
      "0.7605202754399388\n",
      "0.7606118546845124\n",
      "0.7607033639143731\n",
      "0.7604126862820023\n",
      "0.7605042016806722\n",
      "0.7605956471935853\n",
      "0.7606870229007634\n",
      "0.7603967951163678\n",
      "0.7604881769641495\n",
      "0.7605794891345787\n",
      "0.760670731707317\n",
      "0.7607619047619048\n",
      "0.7608530083777608\n",
      "0.7605633802816901\n",
      "0.760654490106545\n",
      "0.7607455306200076\n",
      "0.7608365019011407\n",
      "0.7609274040288864\n",
      "0.7610182370820668\n",
      "0.7611090011393847\n",
      "0.761199696279423\n",
      "0.7609108159392789\n",
      "0.7606221547799696\n",
      "0.7607129313613955\n",
      "0.7608036391205458\n",
      "0.7608942781356575\n",
      "0.7609848484848485\n",
      "0.7610753502461189\n",
      "0.7611657834973505\n",
      "0.7612561483163073\n",
      "0.7613464447806354\n",
      "0.7610586011342155\n",
      "0.7611489040060468\n",
      "0.7608613524744995\n",
      "0.7609516616314199\n",
      "0.7610419026047565\n",
      "0.7607547169811321\n",
      "0.7608449641644662\n",
      "0.7609351432880844\n",
      "0.7606483226535997\n",
      "0.7607385079125848\n",
      "0.7608286252354048\n",
      "0.7609186746987951\n",
      "0.7610086563793752\n",
      "0.7607223476297968\n",
      "0.7608123354644604\n",
      "0.7609022556390977\n",
      "0.7606163096580233\n",
      "0.7607062359128475\n",
      "0.7604205782951559\n",
      "0.7605105105105106\n",
      "0.7606003752345216\n",
      "0.7606901725431358\n",
      "0.7604049493813273\n",
      "0.7604947526236882\n",
      "0.760584488572499\n",
      "0.7606741573033707\n",
      "0.7607637588918008\n",
      "0.7608532934131736\n",
      "0.7609427609427609\n",
      "0.7610321615557217\n",
      "0.7607476635514019\n",
      "0.7604633781763827\n",
      "0.7605528576765036\n",
      "0.760268857356236\n",
      "0.7603583426651735\n",
      "0.7604477611940299\n",
      "0.7605371130175308\n",
      "0.7602535421327368\n",
      "0.7603428997390981\n",
      "0.7600596125186289\n",
      "0.7601489757914339\n",
      "0.7602382725241995\n",
      "0.7599553405284704\n",
      "0.7600446428571429\n",
      "0.7601338787653402\n",
      "0.7598513011152417\n",
      "0.7599405425492382\n",
      "0.7600297176820208\n",
      "0.7601188265874489\n",
      "0.7602078693392724\n",
      "0.7602968460111317\n",
      "0.7600148367952523\n",
      "0.7601038190582128\n",
      "0.7601927353595256\n",
      "0.7602815857725084\n",
      "0.7603703703703704\n",
      "0.7604590892262125\n",
      "0.7605477424130274\n",
      "0.7606363300036996\n",
      "0.7607248520710059\n",
      "0.7608133086876155\n",
      "0.7609016999260901\n",
      "0.7609900258588844\n",
      "0.7607090103397341\n",
      "0.760797342192691\n",
      "0.7608856088560886\n",
      "0.7609738104020657\n",
      "0.7610619469026548\n",
      "0.7611500184297826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7612380250552689\n",
      "0.7609576427255985\n",
      "0.7606774668630338\n",
      "0.7607655502392344\n",
      "0.7608535688005886\n",
      "0.7609415226186098\n",
      "0.7610294117647058\n",
      "0.7611172363101801\n",
      "0.7612049963262307\n",
      "0.7612926918839515\n",
      "0.7613803230543319\n",
      "0.7614678899082569\n",
      "0.7611885546588408\n",
      "0.7612761276127613\n",
      "0.7609970674486803\n",
      "0.7610846463906192\n",
      "0.7608058608058608\n",
      "0.7608934456243135\n",
      "0.7609809663250366\n",
      "0.761068422978412\n",
      "0.7611558156547183\n",
      "0.7612431444241317\n",
      "0.7613304093567251\n",
      "0.7614176105224698\n",
      "0.7615047479912345\n",
      "0.7615918218327857\n",
      "0.7616788321167883\n",
      "0.7617657789128055\n",
      "0.761852662290299\n",
      "0.7619394823186293\n",
      "0.7616618075801749\n",
      "0.7617486338797814\n",
      "0.7618353969410051\n",
      "0.7619220968329087\n",
      "0.7620087336244541\n",
      "0.7620953073845035\n",
      "0.7621818181818182\n",
      "0.7622682660850599\n",
      "0.7619912790697675\n",
      "0.7620777333817653\n",
      "0.7621641249092229\n",
      "0.7618874773139745\n",
      "0.761611030478955\n",
      "0.7616974972796517\n",
      "0.76178390137781\n",
      "0.7618702428416093\n",
      "0.7615942028985507\n",
      "0.761680550525172\n",
      "0.7617668356263577\n",
      "0.7614911328266377\n",
      "0.7615774240231549\n",
      "0.7616636528028933\n",
      "0.7617498192335502\n",
      "0.761835923382725\n",
      "0.7619219653179191\n",
      "0.761646803900325\n",
      "0.7617328519855595\n",
      "0.7618188379646337\n",
      "0.7619047619047619\n",
      "0.7619906238730617\n",
      "0.7620764239365537\n",
      "0.7621621621621621\n",
      "0.7618876080691642\n",
      "0.7616132517104789\n",
      "0.761699064074874\n",
      "0.7617848146815401\n",
      "0.7618705035971223\n",
      "0.7619561308881697\n",
      "0.7620416966211359\n",
      "0.7617678763923823\n",
      "0.7618534482758621\n",
      "0.7619389587073608\n",
      "0.7620244077530509\n",
      "0.7621097954790097\n",
      "0.7618364418938307\n",
      "0.761921835783435\n",
      "0.7616487455197133\n",
      "0.7617341454675743\n",
      "0.7618194842406877\n",
      "0.761546723952739\n",
      "0.7612741589119542\n",
      "0.7613595706618962\n",
      "0.7614449213161659\n",
      "0.7615302109402932\n",
      "0.7612580414581844\n",
      "0.7613433369060378\n",
      "0.7610714285714286\n",
      "0.7611567297393788\n",
      "0.7612419700214133\n",
      "0.7613271494826971\n",
      "0.761055634807418\n",
      "0.7611408199643493\n",
      "0.7608695652173914\n",
      "0.7609547559672248\n",
      "0.761039886039886\n",
      "0.7607689569241723\n",
      "0.7608540925266903\n",
      "0.7609391675560299\n",
      "0.7610241820768137\n",
      "0.7611091361535727\n",
      "0.7611940298507462\n",
      "0.761278863232682\n",
      "0.7613636363636364\n",
      "0.7614483493077743\n",
      "0.7611781405251952\n",
      "0.7612628591699184\n",
      "0.7609929078014185\n",
      "0.7610776320453739\n",
      "0.7611622962437987\n",
      "0.7608926673751328\n",
      "0.7609773371104815\n",
      "0.7610619469026548\n",
      "0.7611464968152867\n",
      "0.7612309869119208\n",
      "0.7613154172560113\n",
      "0.761046306115235\n",
      "0.76113074204947\n",
      "0.7612151183327446\n",
      "0.7612994350282486\n",
      "0.7613836921990822\n",
      "0.7614678899082569\n",
      "0.7615520282186948\n",
      "0.7616361071932299\n",
      "0.761720126894607\n",
      "0.7618040873854828\n",
      "0.7618879887284254\n",
      "0.7619718309859155\n",
      "0.7617036254839845\n",
      "0.7617874736101337\n",
      "0.7618712627506156\n",
      "0.7619549929676512\n",
      "0.7620386643233743\n",
      "0.7621222768798314\n",
      "0.7622058306989814\n",
      "0.7622893258426966\n",
      "0.762021762021762\n",
      "0.7621052631578947\n",
      "0.7621887057172921\n",
      "0.761921458625526\n",
      "0.7620049071153172\n",
      "0.7620882971268396\n",
      "0.7621716287215412\n",
      "0.7622549019607843\n",
      "0.7623381169058453\n",
      "0.7624212736179147\n",
      "0.7625043721580972\n",
      "0.7622377622377622\n",
      "0.7619713386927648\n",
      "0.7620545073375262\n",
      "0.7621376178833391\n",
      "0.7618715083798883\n",
      "0.7616055846422338\n",
      "0.76168876482903\n",
      "0.7617718869898848\n",
      "0.7618549511854951\n",
      "0.7619379574764726\n",
      "0.7620209059233449\n",
      "0.7621037965865552\n",
      "0.7618384401114207\n",
      "0.76192133658197\n",
      "0.7616562282533055\n",
      "0.7617391304347826\n",
      "0.7618219749652295\n",
      "0.7615571776155717\n",
      "0.7616400277970813\n",
      "0.7617228204237583\n",
      "0.7618055555555555\n",
      "0.7618882332523429\n",
      "0.761970853573907\n",
      "0.7620534165799514\n",
      "0.7621359223300971\n",
      "0.7622183708838821\n",
      "0.7623007623007623\n",
      "0.7623830966401108\n",
      "0.7621191135734072\n",
      "0.7622014537902388\n",
      "0.7619377162629758\n",
      "0.762020062262193\n",
      "0.7621023513139695\n",
      "0.7621845834773592\n",
      "0.7622667588113338\n",
      "0.7623488773747841\n",
      "0.7624309392265194\n",
      "0.762167759751467\n",
      "0.7622498274672188\n",
      "0.7623318385650224\n",
      "0.7624137931034483\n",
      "0.7624956911409859\n",
      "0.7625775327360441\n",
      "0.7626593179469514\n",
      "0.7627410468319559\n",
      "0.7624784853700516\n",
      "0.7622161046111493\n",
      "0.762297901616787\n",
      "0.7623796423658872\n",
      "0.7624613269164662\n",
      "0.7625429553264604\n",
      "0.7622810030917211\n",
      "0.7623626373626373\n",
      "0.7624442155853073\n",
      "0.762525737817433\n",
      "0.7626072041166381\n",
      "0.7626886145404664\n",
      "0.7627699691463833\n",
      "0.7628512679917752\n",
      "0.76293251113395\n",
      "0.763013698630137\n",
      "0.7630948305374872\n",
      "0.7631759069130732\n",
      "0.7632569278138899\n",
      "0.7633378932968536\n",
      "0.7634188034188034\n",
      "0.7634996582365003\n",
      "0.763580457806628\n",
      "0.7636612021857924\n",
      "0.7637418914305224\n",
      "0.7638225255972696\n",
      "0.7639031047424087\n",
      "0.7639836289222374\n",
      "0.7640640981929765\n",
      "0.7638036809815951\n",
      "0.7635434412265758\n",
      "0.7636239782016349\n",
      "0.7637044603336738\n",
      "0.763784887678693\n",
      "0.7635250085062947\n",
      "0.7636054421768708\n",
      "0.763685821149269\n",
      "0.7634262406526172\n",
      "0.763506625891947\n",
      "0.7635869565217391\n",
      "0.763667232597623\n",
      "0.763408010862186\n",
      "0.7634882931795046\n",
      "0.7632293080054274\n",
      "0.7633095964733808\n",
      "0.7633898305084745\n",
      "0.7634700101660454\n",
      "0.7632113821138211\n",
      "0.7632915678970539\n",
      "0.7633716993906567\n",
      "0.7631133671742809\n",
      "0.7631935047361299\n",
      "0.7632735880960433\n",
      "0.7630155510480054\n",
      "0.7630956404190605\n",
      "0.7628378378378379\n",
      "0.7629179331306991\n",
      "0.7629979743416611\n",
      "0.7630779615254809\n",
      "0.7631578947368421\n",
      "0.7629005059021923\n",
      "0.7626432906271072\n",
      "0.7627232895180317\n",
      "0.7628032345013477\n",
      "0.762546311889525\n",
      "0.7626262626262627\n",
      "0.7627061595422416\n",
      "0.7627860026917901\n",
      "0.7625294315506222\n",
      "0.7622730329522529\n",
      "0.7623529411764706\n",
      "0.7624327956989247\n",
      "0.7625125965737319\n",
      "0.7622565480188046\n",
      "0.7623363544813696\n",
      "0.7624161073825504\n",
      "0.7621603488762161\n",
      "0.7622401073105298\n",
      "0.7623198122695273\n",
      "0.7623994638069705\n",
      "0.7621440536013401\n",
      "0.7622237106496986\n",
      "0.7619685302979579\n",
      "0.7617135207496654\n",
      "0.7617932418869187\n",
      "0.7618729096989967\n",
      "0.7616181878970244\n",
      "0.7616978609625669\n",
      "0.7614433678583361\n",
      "0.761189044756179\n",
      "0.7612687813021702\n",
      "0.7613484646194927\n",
      "0.7614280947614281\n",
      "0.7615076717811875\n",
      "0.7615871957319106\n",
      "0.7616666666666667\n",
      "0.7617460846384538\n",
      "0.7618254497001998\n",
      "0.7619047619047619\n",
      "0.7616511318242344\n",
      "0.7617304492512479\n",
      "0.7618097139055223\n",
      "0.7618889258397074\n",
      "0.761968085106383\n",
      "0.7617148554336989\n",
      "0.7617940199335548\n",
      "0.7618731318498838\n",
      "0.7619521912350598\n",
      "0.7616993030202456\n",
      "0.7617783676177837\n",
      "0.7618573797678275\n",
      "0.7619363395225465\n",
      "0.7620152469340404\n",
      "0.7620941020543406\n",
      "0.7618416694269625\n",
      "0.7619205298013245\n",
      "0.7619993379675604\n",
      "0.7620780939774984\n",
      "0.7621567978828978\n",
      "0.7622354497354498\n",
      "0.7619834710743801\n",
      "0.7620621282220753\n",
      "0.7621407333994054\n",
      "0.761889035667107\n",
      "0.7619676460878178\n",
      "0.762046204620462\n",
      "0.7621247113163973\n",
      "0.762203166226913\n",
      "0.7622815694032311\n",
      "0.7623599208965063\n",
      "0.7624382207578254\n",
      "0.7625164690382081\n",
      "0.7625946657886071\n",
      "0.7626728110599078\n",
      "0.7627509049029286\n",
      "0.7625\n",
      "0.7625780993094377\n",
      "0.7626561472715319\n",
      "0.7624055208675649\n",
      "0.7624835742444153\n",
      "0.7625615763546798\n",
      "0.762639527248851\n",
      "0.7627174269773548\n",
      "0.7627952755905512\n",
      "0.762873073138734\n",
      "0.7626229508196721\n",
      "0.7627007538511963\n",
      "0.762778505897772\n",
      "0.7628562070094989\n",
      "0.7629338572364113\n",
      "0.7630114566284779\n",
      "0.7630890052356021\n",
      "0.7631665031076218\n",
      "0.76324395029431\n",
      "0.7633213468453743\n",
      "0.7633986928104575\n",
      "0.7634759882391375\n",
      "0.7635532331809275\n",
      "0.7636304276852759\n",
      "0.7637075718015666\n",
      "0.7637846655791191\n",
      "0.7638617090671885\n",
      "0.7639387023149657\n",
      "0.7636897001303781\n",
      "0.7637666992505702\n",
      "0.7638436482084691\n",
      "0.7635949202214263\n",
      "0.7633463541666666\n",
      "0.7634233647901074\n",
      "0.7631750162654521\n",
      "0.7632520325203253\n",
      "0.7630039011703511\n",
      "0.7630809229769255\n",
      "0.7631578947368421\n",
      "0.7632348164988633\n",
      "0.7633116883116883\n",
      "0.7630639402791302\n",
      "0.763140817650876\n",
      "0.7632176451508271\n",
      "0.7632944228274967\n",
      "0.7630470016207456\n",
      "0.7631237848347375\n",
      "0.7632005183025591\n",
      "0.7632772020725389\n",
      "0.7633538361929427\n",
      "0.7634304207119741\n",
      "0.7635069556777742\n",
      "0.7635834411384217\n",
      "0.7636598771419334\n",
      "0.7634130575307045\n",
      "0.7634894991922455\n",
      "0.7635658914728682\n",
      "0.7633193412980304\n",
      "0.763395739186572\n",
      "0.7634720877702484\n",
      "0.7635483870967742\n",
      "0.763302160593357\n",
      "0.7633784655061251\n",
      "0.7634547212375121\n",
      "0.7635309278350515\n",
      "0.7636070853462158\n",
      "0.763683193818416\n",
      "0.7637592532990023\n",
      "0.7635135135135135\n",
      "0.7635895786426504\n",
      "0.7636655948553055\n",
      "0.76374156219865\n",
      "0.7638174807197944\n",
      "0.7638933504657887\n",
      "0.7639691714836223\n",
      "0.7637239165329053\n",
      "0.7637997432605905\n",
      "0.7638755213346167\n",
      "0.763951250801796\n",
      "0.764026931708881\n",
      "0.7637820512820512\n",
      "0.7635373277795579\n",
      "0.7632927610506086\n",
      "0.7633685558757605\n",
      "0.7634443021766966\n",
      "0.7632\n",
      "0.7629558541266794\n",
      "0.7630316597377679\n",
      "0.7627877237851662\n",
      "0.7628635346756152\n",
      "0.7629392971246006\n",
      "0.7626956244011498\n",
      "0.7627713920817369\n",
      "0.7628471113948292\n",
      "0.7629227823867263\n",
      "0.7629984051036682\n",
      "0.7630739795918368\n",
      "0.7631495058973542\n",
      "0.762906309751434\n",
      "0.7629818413507486\n",
      "0.7627388535031847\n",
      "0.7628143903215536\n",
      "0.7628898790579249\n",
      "0.7629653197581928\n",
      "0.7630407124681934\n",
      "0.762798092209857\n",
      "0.7628734901462174\n",
      "0.7629488401652368\n",
      "0.7630241423125794\n",
      "0.7630993966338521\n",
      "0.7631746031746032\n",
      "0.7632497619803237\n",
      "0.7633248730964467\n",
      "0.7630827783063748\n",
      "0.7631578947368421\n",
      "0.7632329635499208\n",
      "0.7633079847908745\n",
      "0.7630662020905923\n",
      "0.7628245725142495\n",
      "0.7625830959164293\n",
      "0.7626582278481012\n",
      "0.762733312242961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7628083491461101\n",
      "0.7628833386025925\n",
      "0.7626422250316056\n",
      "0.7627172195892575\n",
      "0.7627921667719519\n",
      "0.7628670666245658\n",
      "0.7629419191919192\n",
      "0.7630167245187757\n",
      "0.762776025236593\n",
      "0.7628508356985179\n",
      "0.7629255989911727\n",
      "0.762685156003782\n",
      "0.7627599243856332\n",
      "0.7628346456692914\n",
      "0.7629093198992444\n",
      "0.762669184765502\n",
      "0.7627438640654499\n",
      "0.7628184963825102\n",
      "0.7628930817610063\n",
      "0.7629676202452059\n",
      "0.7627278441231929\n",
      "0.7628023876845743\n",
      "0.7625628140703518\n",
      "0.7623233908948195\n",
      "0.7623979912115505\n",
      "0.7621587700031377\n",
      "0.7619196988707654\n",
      "0.7616807776732518\n",
      "0.7617554858934169\n",
      "0.761830147289251\n",
      "0.7619047619047619\n",
      "0.7619793297839023\n",
      "0.7620538509705698\n",
      "0.7621283255086072\n",
      "0.7618898623279099\n",
      "0.7619643415702221\n",
      "0.7620387742338962\n",
      "0.7621131603626133\n",
      "0.761875\n",
      "0.7619493908153702\n",
      "0.7617114303560275\n",
      "0.7617858257883234\n",
      "0.7615480649188514\n",
      "0.761622464898596\n",
      "0.7616968184653774\n",
      "0.7617711256626131\n",
      "0.7618453865336658\n",
      "0.7619196011218448\n",
      "0.761993769470405\n",
      "0.7620678916225475\n",
      "0.7618306351183064\n",
      "0.7619047619047619\n",
      "0.7619788425637835\n",
      "0.7620528771384136\n",
      "0.7621268656716418\n",
      "0.7622008082064035\n",
      "0.7622747047855811\n",
      "0.7623485554520038\n",
      "0.7624223602484472\n",
      "0.7624961192176343\n",
      "0.7622594661700807\n",
      "0.7620229599751784\n",
      "0.7617866004962779\n",
      "0.7615503875968992\n",
      "0.7616243025418475\n",
      "0.7616981716764797\n",
      "0.7614622057001239\n",
      "0.7612263858779807\n",
      "0.7613003095975233\n",
      "0.7613741875580315\n",
      "0.7614480198019802\n",
      "0.7615218063717909\n",
      "0.761595547309833\n",
      "0.7616692426584235\n",
      "0.761742892459827\n",
      "0.7618164967562558\n",
      "0.7618900555898703\n",
      "0.7619635690027786\n",
      "0.7620370370370371\n",
      "0.7621104597346497\n",
      "0.7621838371375694\n",
      "0.7622571692876966\n",
      "0.7623304562268804\n",
      "0.7624036979969183\n",
      "0.7624768946395564\n",
      "0.762550046196489\n",
      "0.7623152709359606\n",
      "0.7623884272083719\n",
      "0.7624615384615384\n",
      "0.762534604737004\n",
      "0.7626076260762608\n",
      "0.7626806025207501\n",
      "0.7627535341118623\n",
      "0.762826420890937\n",
      "0.7625921375921376\n",
      "0.762665029167946\n",
      "0.7627378759975445\n",
      "0.7625038355323719\n",
      "0.7625766871165645\n",
      "0.7626494940202392\n",
      "0.7627222562844881\n",
      "0.7627949739503525\n",
      "0.7628676470588235\n",
      "0.7629402756508422\n",
      "0.7630128597672995\n",
      "0.7630853994490359\n",
      "0.7628518971848225\n",
      "0.7626185377791374\n",
      "0.7623853211009174\n",
      "0.7624579639254051\n",
      "0.7622249388753056\n",
      "0.7622975863122518\n",
      "0.7623701893708003\n",
      "0.7624427480916031\n",
      "0.7625152625152625\n",
      "0.7625877326823314\n",
      "0.762660158633313\n",
      "0.7627325404086612\n",
      "0.7628048780487805\n",
      "0.7628771715940262\n",
      "0.7629494210847044\n",
      "0.7630216265610722\n",
      "0.7630937880633374\n",
      "0.7631659056316591\n",
      "0.7632379793061473\n",
      "0.7633100091268634\n",
      "0.7630778588807786\n",
      "0.7631498935846762\n",
      "0.7632218844984803\n",
      "0.7632938316621087\n",
      "0.7630619684082625\n",
      "0.7628302459763134\n",
      "0.7629022465088039\n",
      "0.7626707132018209\n",
      "0.7627427184466019\n",
      "0.7625113739763422\n",
      "0.7625833838690115\n",
      "0.7626553501060928\n",
      "0.7627272727272727\n",
      "0.7624962132687064\n",
      "0.7625681405208964\n",
      "0.7626400242204057\n",
      "0.7627118644067796\n",
      "0.7627836611195159\n",
      "0.7628554143980641\n",
      "0.762624735409737\n",
      "0.7626964933494559\n",
      "0.7624660018132366\n",
      "0.7625377643504532\n",
      "0.7626094835397161\n",
      "0.7626811594202898\n",
      "0.7627527920313915\n",
      "0.7628243814121907\n",
      "0.7628959276018099\n",
      "0.7629674306393245\n",
      "0.7630388905637624\n",
      "0.7628089210367691\n",
      "0.7628803856583308\n",
      "0.7629518072289156\n",
      "0.7630231857874135\n",
      "0.7630945213726671\n",
      "0.7631658140234727\n",
      "0.762936221419976\n",
      "0.7630075187969925\n",
      "0.7630787733012627\n",
      "0.7631499849714457\n",
      "0.7629206730769231\n",
      "0.7626914989486332\n",
      "0.7627627627627628\n",
      "0.7628339837886521\n",
      "0.762905162064826\n",
      "0.7629762976297629\n",
      "0.7630473905218956\n",
      "0.7631184407796102\n",
      "0.763189448441247\n",
      "0.7632604135451004\n",
      "0.7630317555422409\n",
      "0.7631027253668763\n",
      "0.7631736526946108\n",
      "0.7632445375636037\n",
      "0.7633153800119689\n",
      "0.7633861800777745\n",
      "0.7634569377990431\n",
      "0.7635276532137518\n",
      "0.7635983263598326\n",
      "0.7636689572751718\n",
      "0.7634408602150538\n",
      "0.763511495968946\n",
      "0.7635820895522388\n",
      "0.7633542226201134\n",
      "0.7634248210023866\n",
      "0.7634953772740829\n",
      "0.7632677400119261\n",
      "0.7633383010432191\n",
      "0.7634088200238379\n",
      "0.7634792969913613\n",
      "0.7635497319833234\n",
      "0.7636201250372134\n",
      "0.7636904761904761\n",
      "0.7634632549836359\n",
      "0.7632361689470554\n",
      "0.7630092179601546\n",
      "0.762782401902497\n",
      "0.762555720653789\n",
      "0.7626262626262627\n",
      "0.7623997623997624\n",
      "0.7624703087885986\n",
      "0.7622439893143366\n",
      "0.7623145400593472\n",
      "0.7623850489469001\n",
      "0.7624555160142349\n",
      "0.7625259412985473\n",
      "0.7622999407231772\n",
      "0.7623703703703704\n",
      "0.7624407582938388\n",
      "0.7625111045306485\n",
      "0.7622853759621078\n",
      "0.7623557265463154\n",
      "0.7624260355029586\n",
      "0.7624963028689736\n",
      "0.7625665286812537\n",
      "0.7623411173514631\n",
      "0.7624113475177305\n",
      "0.7624815361890694\n",
      "0.7625516834022446\n",
      "0.7626217891939769\n",
      "0.762396694214876\n",
      "0.7624668043670699\n",
      "0.7625368731563422\n",
      "0.7626069006192864\n",
      "0.7626768867924528\n",
      "0.7627468317123489\n",
      "0.762816735415439\n",
      "0.7628865979381443\n",
      "0.7629564193168433\n",
      "0.7627318221960554\n",
      "0.7628016480282519\n",
      "0.7625772285966461\n",
      "0.7626470588235295\n",
      "0.7627168479858865\n",
      "0.7627865961199295\n",
      "0.7628563032618279\n",
      "0.7629259694477086\n",
      "0.7629955947136564\n",
      "0.7630651790957135\n",
      "0.762841209275022\n",
      "0.7629107981220657\n",
      "0.7629803461425638\n",
      "0.763049853372434\n",
      "0.763119319847552\n",
      "0.7631887456037515\n",
      "0.7632581306768239\n",
      "0.7633274751025191\n",
      "0.7633967789165447\n",
      "0.7634660421545667\n",
      "0.7635352648522096\n",
      "0.7636044470450556\n",
      "0.7633811055864288\n",
      "0.7634502923976608\n",
      "0.7635194387605964\n",
      "0.7635885447106955\n",
      "0.7633654688869412\n",
      "0.7634345794392523\n",
      "0.7632116788321168\n",
      "0.7632807939287799\n",
      "0.7633498686898161\n",
      "0.7631271878646441\n",
      "0.763196267133275\n",
      "0.763265306122449\n",
      "0.7633343048673856\n",
      "0.7634032634032634\n",
      "0.7631808913486746\n",
      "0.7629586488060571\n",
      "0.7627365356622998\n",
      "0.7628055878928988\n",
      "0.7625836485306954\n",
      "0.7626527050610821\n",
      "0.7627217214306484\n",
      "0.7627906976744186\n",
      "0.7628596338273758\n",
      "0.7629285299244625\n",
      "0.7629973860005809\n",
      "0.7630662020905923\n",
      "0.7631349782293179\n",
      "0.763203714451538\n",
      "0.7629823034522774\n",
      "0.7630510440835266\n",
      "0.7631197448535807\n",
      "0.7631884057971015\n",
      "0.7629672558678644\n",
      "0.7630359212050984\n",
      "0.7628149435273676\n",
      "0.7628836132020845\n",
      "0.7629522431259045\n",
      "0.7630208333333334\n",
      "0.7630893838588372\n",
      "0.7631578947368421\n",
      "0.7632263660017347\n",
      "0.7630057803468208\n",
      "0.763074255995377\n",
      "0.7631426920854997\n",
      "0.7632110886514583\n",
      "0.7629907621247113\n",
      "0.7630591630591631\n",
      "0.7628390075014426\n",
      "0.7626189789443323\n",
      "0.7626874279123415\n",
      "0.762755837417123\n",
      "0.7628242074927953\n",
      "0.762892538173437\n",
      "0.7626728110599078\n",
      "0.7627411459832998\n",
      "0.7628094415659182\n",
      "0.762589928057554\n",
      "0.7626582278481012\n",
      "0.7627264883520276\n",
      "0.7627947096032203\n",
      "0.7628628916355275\n",
      "0.7626436781609195\n",
      "0.7627118644067796\n",
      "0.7624928202182654\n",
      "0.7625610106230262\n",
      "0.7626291618828932\n",
      "0.7626972740315638\n",
      "0.7624784853700516\n",
      "0.7625466016633209\n",
      "0.7623279816513762\n",
      "0.762396102034967\n",
      "0.7624641833810888\n",
      "0.7625322257232885\n",
      "0.7626002290950744\n",
      "0.7623819066704838\n",
      "0.7624499141385231\n",
      "0.7625178826895566\n",
      "0.7625858123569794\n",
      "0.7626537031741493\n",
      "0.7624356775300172\n",
      "0.7625035724492712\n",
      "0.7622857142857142\n",
      "0.7623536132533562\n",
      "0.7624214734437464\n",
      "0.7624892948900942\n",
      "0.7625570776255708\n",
      "0.7626248216833096\n",
      "0.7626925270964061\n",
      "0.7627601938979185\n",
      "0.7628278221208666\n",
      "0.7628954117982332\n",
      "0.762962962962963\n",
      "0.7630304756479636\n",
      "0.7628132118451025\n",
      "0.762596071733561\n",
      "0.7626636311895276\n",
      "0.7624466571834992\n",
      "0.762514220705347\n",
      "0.7625817458060847\n",
      "0.7626492325184764\n",
      "0.7627166808752487\n",
      "0.7627840909090909\n",
      "0.7628514626526555\n",
      "0.7629187961385576\n",
      "0.7629860913993756\n",
      "0.7630533484676504\n",
      "0.7631205673758865\n",
      "0.7631877481565513\n",
      "0.7632548908420754\n",
      "0.7633219954648526\n",
      "0.76338906205724\n",
      "0.7634560906515581\n",
      "0.7635230812800906\n",
      "0.763590033975085\n",
      "0.7636569487687518\n",
      "0.7637238256932655\n",
      "0.7635077793493635\n",
      "0.7632918552036199\n",
      "0.7633587786259542\n",
      "0.7631430186546071\n",
      "0.7632099463125177\n",
      "0.7632768361581921\n",
      "0.7633436882236656\n",
      "0.7634105025409373\n",
      "0.76347727914197\n",
      "0.7635440180586908\n",
      "0.7633286318758815\n",
      "0.7633953750705019\n",
      "0.7634620806315195\n",
      "0.7635287485907554\n",
      "0.7633136094674556\n",
      "0.7633802816901408\n",
      "0.7634469163615882\n",
      "0.7635135135135135\n",
      "0.7635800731775964\n",
      "0.7636465953854812\n",
      "0.7637130801687764\n",
      "0.7634983127109112\n",
      "0.763564801799269\n",
      "0.7636312535132097\n",
      "0.7636976678842371\n",
      "0.7637640449438202\n",
      "0.7638303847233923\n",
      "0.7638966872543514\n",
      "0.7639629525680606\n",
      "0.7640291806958474\n",
      "0.7640953716690042\n",
      "0.7641615255187886\n",
      "0.7642276422764228\n",
      "0.7642937219730942\n",
      "0.7640795741103951\n",
      "0.7641456582633053\n",
      "0.7642117054046486\n",
      "0.7639977603583427\n",
      "0.764063811922754\n",
      "0.7638500279798545\n",
      "0.7639160839160839\n",
      "0.7637024608501118\n",
      "0.7634889572267263\n",
      "0.7635550586920067\n",
      "0.7636211232187762\n",
      "0.7636871508379889\n",
      "0.7637531415805641\n",
      "0.7638190954773869\n",
      "0.7636059168294725\n",
      "0.7633928571428571\n",
      "0.7634588563458856\n",
      "0.7632459564974903\n",
      "0.7633119598550321\n",
      "0.7630992196209587\n",
      "0.7631652270827528\n",
      "0.7629526462395543\n",
      "0.7630186577554998\n",
      "0.7630846325167038\n",
      "0.7631505705538547\n",
      "0.7629382303839732\n",
      "0.7627260083449235\n",
      "0.7627919911012235\n",
      "0.7628579371698637\n",
      "0.7629238465814341\n",
      "0.7629897193664907\n",
      "0.7630555555555556\n",
      "0.763121355179117\n",
      "0.7631871182676291\n",
      "0.7629752983624757\n",
      "0.7627635960044395\n",
      "0.7628294036061026\n",
      "0.7628951747088186\n",
      "0.7629609093429442\n",
      "0.7627494456762749\n",
      "0.7625380991964533\n",
      "0.7626038781163434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7626696206037109\n",
      "0.762735326688815\n",
      "0.7625242181013009\n",
      "0.762589928057554\n",
      "0.762655601659751\n",
      "0.7627212389380531\n",
      "0.7625103677080454\n",
      "0.7622996130458817\n",
      "0.7620889748549323\n",
      "0.7621546961325967\n",
      "0.7622203811101905\n",
      "0.7622860298177803\n",
      "0.7623516422853989\n",
      "0.7624172185430463\n",
      "0.7622068965517241\n",
      "0.7622724765581909\n",
      "0.7623380204025365\n",
      "0.7624035281146637\n",
      "0.762193441719482\n",
      "0.762258953168044\n",
      "0.7623244285320848\n",
      "0.7623898678414097\n",
      "0.7624552711257914\n",
      "0.7625206384149698\n",
      "0.7623108665749656\n",
      "0.7623762376237624\n",
      "0.7624415727247732\n",
      "0.7625068719076415\n",
      "0.7625721352019785\n",
      "0.7626373626373626\n",
      "0.7627025542433398\n",
      "0.7624931356397584\n",
      "0.7625583310458414\n",
      "0.7623490669593853\n",
      "0.7624142661179698\n",
      "0.7622051563357104\n",
      "0.7622703591993419\n",
      "0.7620614035087719\n",
      "0.7621266100301453\n",
      "0.7619178082191781\n",
      "0.7619830183511367\n",
      "0.7617743702081051\n",
      "0.7618395839036408\n",
      "0.7619047619047619\n",
      "0.761969904240766\n",
      "0.7620350109409191\n",
      "0.7621000820344545\n",
      "0.762165117550574\n",
      "0.7622301175184477\n",
      "0.7622950819672131\n",
      "0.7623600109259765\n",
      "0.7624249044238122\n",
      "0.7624897624897625\n",
      "0.7625545851528385\n",
      "0.762619372442019\n",
      "0.762684124386252\n",
      "0.7627488410144533\n",
      "0.7625408942202835\n",
      "0.7623330607795039\n",
      "0.762125340599455\n",
      "0.7619177335875783\n",
      "0.7619825708061002\n",
      "0.7620473727198476\n",
      "0.7621121393576483\n",
      "0.7621768707482993\n",
      "0.7622415669205659\n",
      "0.7623062279031819\n",
      "0.7623708537248505\n",
      "0.762435444414243\n",
      "0.7625\n",
      "0.7625645205107308\n",
      "0.7626290059750136\n",
      "0.7624219386369807\n",
      "0.762214983713355\n",
      "0.7622795115332429\n",
      "0.7623440043407488\n",
      "0.7624084621643613\n",
      "0.762472885032538\n",
      "0.7625372729737057\n",
      "0.7626016260162601\n",
      "0.7626659441885668\n",
      "0.7627302275189599\n",
      "0.7627944760357434\n",
      "0.7628586897671901\n",
      "0.7629228687415426\n",
      "0.762987012987013\n",
      "0.7627806329456316\n",
      "0.7628447809626825\n",
      "0.7629088942957556\n",
      "0.762972972972973\n",
      "0.7627668197784383\n",
      "0.7628309022150189\n",
      "0.7626248987307589\n",
      "0.7626889848812095\n",
      "0.762753036437247\n",
      "0.7628170534268753\n",
      "0.7628810358780685\n",
      "0.7629449838187702\n",
      "0.763008897276894\n",
      "0.7630727762803234\n",
      "0.7631366208569119\n",
      "0.7632004310344828\n",
      "0.7632642068408295\n",
      "0.7633279483037156\n",
      "0.7631224764468372\n",
      "0.7631862217438106\n",
      "0.7632499327414581\n",
      "0.7633136094674556\n",
      "0.7633772519494488\n",
      "0.7634408602150538\n",
      "0.763504434291857\n",
      "0.7632993014508329\n",
      "0.7633628793983347\n",
      "0.7634264232008593\n",
      "0.763489932885906\n",
      "0.7632850241545893\n",
      "0.7630802253823451\n",
      "0.7631437768240343\n",
      "0.7632072941807455\n",
      "0.7632707774798928\n",
      "0.7633342267488609\n",
      "0.7631296891747053\n",
      "0.7631931422448432\n",
      "0.7632565613283342\n",
      "0.7633199464524766\n",
      "0.7633832976445396\n",
      "0.7634466149317635\n",
      "0.763509898341359\n",
      "0.7633056967103503\n",
      "0.7633689839572193\n",
      "0.7634322373696872\n",
      "0.7632282202031\n",
      "0.7632914774245257\n",
      "0.7633547008547008\n",
      "0.7631508678237651\n",
      "0.7632140950347037\n",
      "0.7632772884974647\n",
      "0.7633404482390609\n",
      "0.7634035742864764\n",
      "0.7632\n",
      "0.7632631298320448\n",
      "0.7633262260127932\n",
      "0.7631228350652811\n",
      "0.7631859350026639\n",
      "0.762982689747004\n",
      "0.7630457933972311\n",
      "0.7628426936385414\n",
      "0.7626397019691326\n",
      "0.7624368183027401\n",
      "0.7625\n",
      "0.7625631480989099\n",
      "0.7626262626262627\n",
      "0.7626893436088228\n",
      "0.7624867162592986\n",
      "0.7622841965471447\n",
      "0.7623473181093999\n",
      "0.762410406158747\n",
      "0.7622080679405521\n",
      "0.7622711594587424\n",
      "0.7623342175066313\n",
      "0.7623972421108459\n",
      "0.7624602332979852\n",
      "0.762258150013252\n",
      "0.7623211446740858\n",
      "0.7623841059602648\n",
      "0.762447033898305\n",
      "0.7625099285146942\n",
      "0.7623080995235575\n",
      "0.7623709976184175\n",
      "0.7621693121693122\n",
      "0.7622322137000793\n",
      "0.7622950819672131\n",
      "0.7623579169970922\n",
      "0.7624207188160677\n",
      "0.7624834874504623\n",
      "0.7625462229265716\n",
      "0.7626089252706628\n",
      "0.7626715945089757\n",
      "0.7627342306677224\n",
      "0.762796833773087\n",
      "0.7628594038512266\n",
      "0.76292194092827\n",
      "0.762984445030319\n",
      "0.7627833421191355\n",
      "0.7628458498023716\n",
      "0.7629083245521602\n",
      "0.7627074005794048\n",
      "0.7627698788836229\n",
      "0.7628323242958673\n",
      "0.7626315789473684\n",
      "0.762694027887398\n",
      "0.7627564439768543\n",
      "0.7628188272416513\n",
      "0.7628811777076762\n",
      "0.7629434954007884\n",
      "0.7630057803468208\n",
      "0.7630680325715786\n",
      "0.7631302521008403\n",
      "0.763192438960357\n",
      "0.763254593175853\n",
      "0.7633167147730254\n",
      "0.7633788037775446\n",
      "0.7634408602150538\n",
      "0.7632406921866807\n",
      "0.7630406290956749\n",
      "0.7628406708595388\n",
      "0.7629028032486246\n",
      "0.7629649030906234\n",
      "0.7630269704111023\n",
      "0.7630890052356021\n",
      "0.7631510075896362\n",
      "0.7629513343799058\n",
      "0.7630133403086581\n",
      "0.7630753138075314\n",
      "0.7631372549019608\n",
      "0.763199163617355\n",
      "0.7632610399790959\n",
      "0.7633228840125392\n",
      "0.7633846957430138\n",
      "0.7634464751958224\n",
      "0.7635082223962412\n",
      "0.7635699373695198\n",
      "0.7636316201408818\n",
      "0.7636932707355243\n",
      "0.763754889178618\n",
      "0.7638164754953076\n",
      "0.7638780297107115\n",
      "0.7639395518499218\n",
      "0.7640010419380047\n",
      "0.7638020833333333\n",
      "0.7638635771934392\n",
      "0.7639250390421656\n",
      "0.7639864689045017\n",
      "0.7637877211238293\n",
      "0.7638491547464239\n",
      "0.7639105564222569\n",
      "0.7639719261762412\n",
      "0.764033264033264\n",
      "0.7640945700181866\n",
      "0.7638961038961039\n",
      "0.7639574136587899\n",
      "0.764018691588785\n",
      "0.7640799377108747\n",
      "0.7641411520498184\n",
      "0.7642023346303501\n",
      "0.7642634854771784\n",
      "0.7643246046149857\n",
      "0.7643856920684292\n",
      "0.7644467478621404\n",
      "0.7645077720207254\n",
      "0.7645687645687645\n",
      "0.7646297255308131\n",
      "0.7646906549314004\n",
      "0.764751552795031\n",
      "0.7648124191461837\n",
      "0.7646145887221935\n",
      "0.7646754590121542\n",
      "0.7647362978283351\n",
      "0.7645386404755751\n",
      "0.7645994832041344\n",
      "0.7646602944975458\n",
      "0.7647210743801653\n",
      "0.7647818228763232\n",
      "0.7645844088797109\n",
      "0.7646451612903226\n",
      "0.7647058823529411\n",
      "0.7645086407015734\n",
      "0.7645693656523982\n",
      "0.7646300592936324\n",
      "0.7644329896907216\n",
      "0.7644936871940221\n",
      "0.764554353426069\n",
      "0.7646149884110224\n",
      "0.7646755921730175\n",
      "0.7647361647361647\n",
      "0.7647967061245496\n",
      "0.7648572163622331\n",
      "0.7649176954732511\n",
      "0.7649781434816149\n",
      "0.765038560411311\n",
      "0.7650989462863017\n",
      "0.7651593011305241\n",
      "0.7652196249678911\n",
      "0.7650231124807396\n",
      "0.764826700898588\n",
      "0.7648870636550308\n",
      "0.7649473954323839\n",
      "0.7650076962544895\n",
      "0.7650679661451655\n",
      "0.7648717948717949\n",
      "0.7649320687003333\n",
      "0.7647360328036904\n",
      "0.7647963105303612\n",
      "0.7648565573770492\n",
      "0.7649167733674775\n",
      "0.764720942140297\n",
      "0.7645252111594574\n",
      "0.7645854657113613\n",
      "0.7646456894346381\n",
      "0.7647058823529411\n",
      "0.7645103554078241\n",
      "0.7643149284253579\n",
      "0.7643751597239969\n",
      "0.7644353602452734\n",
      "0.7644955300127714\n",
      "0.7645556690500511\n",
      "0.7646157773806485\n",
      "0.7646758550280756\n",
      "0.7647359020158203\n",
      "0.764795918367347\n",
      "0.7646008671257333\n",
      "0.7646608873023968\n",
      "0.7647208768799388\n",
      "0.764525993883792\n",
      "0.7645859872611465\n",
      "0.7646459500764137\n",
      "0.7647058823529411\n",
      "0.7647657841140529\n",
      "0.7648256553830491\n",
      "0.7648854961832061\n",
      "0.7649453065377766\n",
      "0.7650050864699899\n",
      "0.7650648360030511\n",
      "0.7651245551601423\n",
      "0.7651842439644219\n",
      "0.7652439024390244\n",
      "0.7653035306070612\n",
      "0.7653631284916201\n",
      "0.7654226961157654\n",
      "0.7652284263959391\n",
      "0.7650342552651611\n",
      "0.7650938609842719\n",
      "0.7651534364696931\n",
      "0.7652129817444219\n",
      "0.7650190114068441\n",
      "0.7650785605676634\n",
      "0.7651380795540917\n",
      "0.7651975683890577\n",
      "0.7652570270954672\n",
      "0.7653164556962025\n",
      "0.765375854214123\n",
      "0.7651821862348178\n",
      "0.7652415886668353\n",
      "0.765048052604957\n",
      "0.7651074589127687\n",
      "0.7651668351870576\n",
      "0.7652261814505938\n",
      "0.7652854977261243\n",
      "0.765092194998737\n",
      "0.7651515151515151\n",
      "0.7649583438525625\n",
      "0.765017667844523\n",
      "0.7650769618975524\n",
      "0.7651362260343088\n",
      "0.7651954602774275\n",
      "0.765254664649521\n",
      "0.7650617595160071\n",
      "0.7651209677419355\n",
      "0.765180146132527\n",
      "0.7652392947103275\n",
      "0.7652984134978594\n",
      "0.7653575025176234\n",
      "0.7654165617920966\n",
      "0.7654755913437342\n",
      "0.7655345911949686\n",
      "0.7655935613682092\n",
      "0.7656525018858436\n",
      "0.7657114127702364\n",
      "0.7657702940437295\n",
      "0.7658291457286432\n",
      "0.7656367746797287\n",
      "0.7656956303365143\n",
      "0.7657544564398694\n",
      "0.7658132530120482\n",
      "0.7658720200752823\n",
      "0.7659307576517812\n",
      "0.7659894657637322\n",
      "0.7657973921765295\n",
      "0.7658561042867886\n",
      "0.7659147869674185\n",
      "0.7657228764720622\n",
      "0.7655310621242485\n",
      "0.7655897821187078\n",
      "0.7656484727090636\n",
      "0.7657071339173968\n",
      "0.7655155155155156\n",
      "0.7655741806354766\n",
      "0.7653826913456728\n",
      "0.765441360340085\n",
      "0.7655\n",
      "0.7655586103474131\n",
      "0.7653673163418291\n",
      "0.765425930552086\n",
      "0.7654845154845155\n",
      "0.7655430711610487\n",
      "0.7656015976035946\n",
      "0.7656600948340404\n",
      "0.7657185628742516\n",
      "0.7657770017460713\n",
      "0.7655860349127181\n",
      "0.7656444776863625\n",
      "0.765702891326022\n",
      "0.7657612758534762\n",
      "0.7658196312904834\n",
      "0.7656288916562889\n",
      "0.765687250996016\n",
      "0.7657455812795618\n",
      "0.7658038825286212\n",
      "0.7658621547648669\n",
      "0.7659203980099503\n",
      "0.7659786122855011\n",
      "0.7660367976131278\n",
      "0.7660949540144171\n",
      "0.7661530815109344\n",
      "0.7662111801242236\n",
      "0.7662692498758072\n",
      "0.7663272907871865\n",
      "0.7663853028798411\n",
      "0.7664432861752296\n",
      "0.7662531017369727\n",
      "0.7663110890597866\n",
      "0.7663690476190477\n",
      "0.7664269774361517\n",
      "0.7664848785324739\n",
      "0.7665427509293681\n",
      "0.7666005946481665\n",
      "0.7664107010156056\n",
      "0.7662209014363547\n",
      "0.7662787818767022\n",
      "0.7663366336633664\n",
      "0.7663944568176194\n",
      "0.7662048490846116\n",
      "0.7662626762305219\n",
      "0.766320474777448\n",
      "0.7663782447466008\n",
      "0.7664359861591695\n",
      "0.7664936990363233\n",
      "0.7665513833992095\n",
      "0.7666090392689553\n",
      "0.7666666666666667\n",
      "0.7667242656134288\n",
      "0.766781836130306\n",
      "0.766592647421663\n",
      "0.7666502220029601\n",
      "0.7664611590628854\n",
      "0.7665187376725838\n",
      "0.7663298003450826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7663873829472647\n",
      "0.7664449371766445\n",
      "0.766256157635468\n",
      "0.7660674710662398\n",
      "0.7658788774002954\n",
      "0.7659365001230618\n",
      "0.765994094488189\n",
      "0.7660516605166051\n",
      "0.7661091982292179\n",
      "0.7661667076469142\n",
      "0.7659783677482792\n",
      "0.7660358810518555\n",
      "0.7658476658476658\n",
      "0.7659051830017195\n",
      "0.7657170923379175\n",
      "0.7655290940338817\n",
      "0.7653411880216003\n",
      "0.7651533742331288\n",
      "0.7649656526005888\n",
      "0.7647780230561687\n",
      "0.7645904855321236\n",
      "0.7646481980877666\n",
      "0.7647058823529411\n",
      "0.764518500367557\n",
      "0.7645761881430672\n",
      "0.7643889297085477\n",
      "0.7642017629774731\n",
      "0.7640146878824969\n",
      "0.7640724424865394\n",
      "0.7641301688279912\n",
      "0.7639432485322897\n",
      "0.7640009782342871\n",
      "0.7638141809290954\n",
      "0.7638719139574676\n",
      "0.7639296187683284\n",
      "0.7639872953823601\n",
      "0.7640449438202247\n",
      "0.764102564102564\n",
      "0.76416015625\n",
      "0.7639736392482304\n",
      "0.7640312347486579\n",
      "0.7640888021468651\n",
      "0.7639024390243903\n",
      "0.7639600097537186\n",
      "0.7640175524134568\n",
      "0.7638313429198148\n",
      "0.7638888888888888\n",
      "0.76394640682095\n",
      "0.7637603507062835\n",
      "0.76381787192598\n",
      "0.7638753651411879\n",
      "0.7636894621562424\n",
      "0.7637469586374696\n",
      "0.7638044271466796\n",
      "0.7638618677042801\n",
      "0.7639192803306589\n",
      "0.7639766650461838\n",
      "0.764034021871203\n",
      "0.7640913508260447\n",
      "0.7639057566188973\n",
      "0.7639630888780962\n",
      "0.7640203932993445\n",
      "0.7638349514563106\n",
      "0.7638922591603979\n",
      "0.7639495390587093\n",
      "0.7640067911714771\n",
      "0.7638215324927256\n",
      "0.7638787878787878\n",
      "0.7636936500242365\n",
      "0.7637509086503513\n",
      "0.7638081395348837\n",
      "0.7638653426979898\n",
      "0.763680387409201\n",
      "0.7637375938029533\n",
      "0.7637947725072604\n",
      "0.7638519235422212\n",
      "0.7639090469279148\n",
      "0.7639661426844014\n",
      "0.7640232108317214\n",
      "0.7640802513898961\n",
      "0.764137264378927\n",
      "0.7641942498187968\n",
      "0.7642512077294686\n",
      "0.7640666505674958\n",
      "0.7641236117817479\n",
      "0.7641805454984311\n",
      "0.763996138996139\n",
      "0.7638118214716526\n",
      "0.7636275928605886\n",
      "0.7636845912707981\n",
      "0.7635004821600772\n",
      "0.7635574837310195\n",
      "0.7636144578313253\n",
      "0.763671404480848\n",
      "0.763728323699422\n",
      "0.7635444257163496\n",
      "0.7636013480982186\n",
      "0.7636582430806258\n",
      "0.7637151106833494\n",
      "0.7635313928313687\n",
      "0.7635882635882636\n",
      "0.7636451069968743\n",
      "0.7637019230769231\n",
      "0.7637587118481134\n",
      "0.7635752042287361\n",
      "0.7633917847705981\n",
      "0.7634486071085494\n",
      "0.763265306122449\n",
      "0.7630820931349016\n",
      "0.7628989680825534\n",
      "0.7629558541266794\n",
      "0.7627728472055649\n",
      "0.7628297362110312\n",
      "0.7628865979381443\n",
      "0.7629434324065196\n",
      "0.7630002396357537\n",
      "0.763057019645424\n",
      "0.7631137724550898\n",
      "0.7631704980842912\n",
      "0.7632271965525497\n",
      "0.7632838678793681\n",
      "0.7631012203876526\n",
      "0.7631578947368421\n",
      "0.7632145419756039\n",
      "0.763271162123386\n",
      "0.7633277551996175\n",
      "0.7631453154875717\n",
      "0.7632019115890084\n",
      "0.763258480649785\n",
      "0.7633150226892763\n",
      "0.7633715377268386\n",
      "0.7634280257818095\n",
      "0.7634844868735083\n",
      "0.7635409210212359\n",
      "0.7635973282442748\n",
      "0.7636537085618889\n",
      "0.7634716261325704\n",
      "0.7635280095351609\n",
      "0.763584366062917\n",
      "0.7636406957350489\n",
      "0.763696998570748\n",
      "0.7637532745891878\n",
      "0.7638095238095238\n",
      "0.7636277076886455\n",
      "0.7636839600190386\n",
      "0.7637401855817273\n",
      "0.7637963843958135\n",
      "0.7638525564803805\n",
      "0.7636709462672373\n",
      "0.7637271214642263\n",
      "0.7637832699619772\n",
      "0.7638393917795201\n",
      "0.763895486935867\n",
      "0.7639515554500119\n",
      "0.7640075973409307\n",
      "0.7640636126275813\n",
      "0.7641196013289037\n",
      "0.7641755634638197\n",
      "0.7642314990512334\n",
      "0.7642874081100308\n",
      "0.7643432906590801\n",
      "0.7643991467172315\n",
      "0.7644549763033175\n",
      "0.7642738687514807\n",
      "0.7643297015632402\n",
      "0.7643855079327492\n",
      "0.7644412878787878\n",
      "0.7644970414201183\n",
      "0.764552768575485\n",
      "0.7643718949609652\n",
      "0.7644276253547777\n",
      "0.7644833293922914\n",
      "0.7643026004728133\n",
      "0.7643583077286693\n",
      "0.764413988657845\n",
      "0.7644696432789984\n",
      "0.764289088332546\n",
      "0.764344746162928\n",
      "0.7644003777148253\n",
      "0.7644559830068445\n",
      "0.7645115620575743\n",
      "0.7643312101910829\n",
      "0.7643867924528301\n",
      "0.7642065550577694\n",
      "0.7642621404997643\n",
      "0.7643176997407495\n",
      "0.764373232799246\n",
      "0.7644287396937574\n",
      "0.7644842204427696\n",
      "0.7645396750647516\n",
      "0.7645951035781544\n",
      "0.764650506001412\n",
      "0.7644705882352941\n",
      "0.764525993883792\n",
      "0.7645813734713076\n",
      "0.7646367270162239\n",
      "0.764456981664316\n",
      "0.7645123384253819\n",
      "0.7645676691729323\n",
      "0.7646229739252995\n",
      "0.7646782527007985\n",
      "0.7647335055177271\n",
      "0.7647887323943662\n",
      "0.7648439333489792\n",
      "0.7648991083998123\n",
      "0.764954257565095\n",
      "0.7647748592870544\n",
      "0.7648300117233294\n",
      "0.7648851383028599\n",
      "0.7649402390438247\n",
      "0.7649953139643861\n",
      "0.7648161161864605\n",
      "0.7646370023419203\n",
      "0.7644579723718099\n",
      "0.7645131086142322\n",
      "0.7643341914345892\n",
      "0.7641553579784744\n",
      "0.7639766081871345\n",
      "0.7640318054256314\n",
      "0.7640869768529343\n",
      "0.7641421224871435\n",
      "0.7641972423463426\n",
      "0.7642523364485981\n",
      "0.7643074048119598\n",
      "0.7643624474544606\n",
      "0.7644174643941163\n",
      "0.7644724556489262\n",
      "0.7645274212368728\n",
      "0.7645823611759216\n",
      "0.7646372754840215\n",
      "0.7646921641791045\n",
      "0.7645138726975985\n",
      "0.7643356643356644\n",
      "0.7643905849452343\n",
      "0.7642124883504194\n",
      "0.7642674120661542\n",
      "0.7643223102002794\n",
      "0.7643771827706636\n",
      "0.7644320297951583\n",
      "0.7642541307889225\n",
      "0.7643089809213588\n",
      "0.7643638055361712\n",
      "0.7644186046511627\n",
      "0.76447337828412\n",
      "0.7645281264528127\n",
      "0.7645828491749942\n",
      "0.7646375464684015\n",
      "0.764692218350755\n",
      "0.7647468648397585\n",
      "0.7648014859530996\n",
      "0.7648560817084494\n",
      "0.7649106521234625\n",
      "0.7649651972157773\n",
      "0.7647877522616562\n",
      "0.7648423005565863\n",
      "0.7648968235566891\n",
      "0.7649513212795549\n",
      "0.764774044032445\n",
      "0.7648285449490269\n",
      "0.7648830206161686\n",
      "0.7647058823529411\n",
      "0.764760361194721\n",
      "0.7648148148148148\n",
      "0.7648692432307336\n",
      "0.7649236464599722\n",
      "0.7649780245200093\n",
      "0.7650323774283071\n",
      "0.7650867052023121\n",
      "0.7651410078594545\n",
      "0.764964178414606\n",
      "0.765018484288355\n",
      "0.7650727650727651\n",
      "0.7651270207852194\n",
      "0.7651812514430847\n",
      "0.7652354570637119\n",
      "0.7652896376644357\n",
      "0.765343793262575\n",
      "0.7651672433679354\n",
      "0.7652214022140221\n",
      "0.7652755360848513\n",
      "0.7650991240202858\n",
      "0.7651532611200738\n",
      "0.7649769585253456\n",
      "0.7650310988251555\n",
      "0.7650852141870106\n",
      "0.7651393046281373\n",
      "0.7651933701657458\n",
      "0.7652474108170311\n",
      "0.7653014265991717\n",
      "0.765125373821026\n",
      "0.765179392824287\n",
      "0.7652333869855139\n",
      "0.7650574712643678\n",
      "0.7651114686279016\n",
      "0.7649356617647058\n",
      "0.7649896623018608\n",
      "0.7650436380339918\n",
      "0.7648679678530425\n",
      "0.7649219467401286\n",
      "0.7647463851273812\n",
      "0.7645709040844424\n",
      "0.7646249139710943\n",
      "0.7646788990825688\n",
      "0.7647328594359092\n",
      "0.7647867950481431\n",
      "0.7648407059362824\n",
      "0.7648945921173236\n",
      "0.7649484536082474\n",
      "0.7647732478240953\n",
      "0.764598122280742\n",
      "0.7646520146520146\n",
      "0.7647058823529411\n",
      "0.7647597254004577\n",
      "0.7648135438114848\n",
      "0.7648673376029277\n",
      "0.7649211067916762\n",
      "0.7649748513946045\n",
      "0.7650285714285714\n",
      "0.7650822669104205\n",
      "0.7649074708704592\n",
      "0.7649611694837826\n",
      "0.7650148435715917\n",
      "0.7648401826484018\n",
      "0.7646656014608537\n",
      "0.7647193062528526\n",
      "0.7647729865389002\n",
      "0.7648266423357665\n",
      "0.7648802736602053\n",
      "0.7649338805289557\n",
      "0.7649874629587418\n",
      "0.7650410209662717\n",
      "0.7650945545682388\n",
      "0.7651480637813212\n",
      "0.7649738100660441\n",
      "0.7647996357012751\n",
      "0.7648531755064876\n",
      "0.7649066909421939\n",
      "0.7649601820250285\n",
      "0.7650136487716106\n",
      "0.7650670911985444\n",
      "0.7651205093224193\n",
      "0.765173903159809\n",
      "0.765\n",
      "0.7650533969552374\n",
      "0.765106769650159\n",
      "0.7651601181012946\n",
      "0.765213442325159\n",
      "0.7650397275822929\n",
      "0.7650930549251022\n",
      "0.765146358066712\n",
      "0.7651996370235935\n",
      "0.7652528918122024\n",
      "0.7653061224489796\n",
      "0.7653593289503514\n",
      "0.7654125113327289\n",
      "0.7652390663947428\n",
      "0.765292251925691\n",
      "0.7653454133635335\n",
      "0.7653985507246377\n",
      "0.7652252660176591\n",
      "0.7652784065187868\n",
      "0.7653315229689975\n",
      "0.7653846153846153\n",
      "0.7654376837819498\n",
      "0.7654907281772954\n",
      "0.765543748586932\n",
      "0.765370705244123\n",
      "0.7651977401129944\n",
      "0.7650248531405333\n",
      "0.7648520442737745\n",
      "0.7649051490514905\n",
      "0.7649582298487243\n",
      "0.7650112866817156\n",
      "0.7650643195666892\n",
      "0.7648916967509025\n",
      "0.7649447326866682\n",
      "0.7649977447000451\n",
      "0.7648252536640361\n",
      "0.7646528403967539\n",
      "0.7647058823529411\n",
      "0.7645335736818387\n",
      "0.7645866186077945\n",
      "0.7646396396396397\n",
      "0.7644674622832696\n",
      "0.7645204862674471\n",
      "0.7645734863830745\n",
      "0.7644014401440145\n",
      "0.7644544431946007\n",
      "0.7645074224021593\n",
      "0.7645603777827749\n",
      "0.7643884892086331\n",
      "0.7642166779051472\n",
      "0.7642696629213483\n",
      "0.7640979555156144\n",
      "0.7641509433962265\n",
      "0.7642039074781046\n",
      "0.7642568477772789\n",
      "0.7640852974186307\n",
      "0.7641382405745063\n",
      "0.7641911599730761\n",
      "0.7640197397936295\n",
      "0.7640726620318457\n",
      "0.7639013452914798\n",
      "0.7639542703429725\n",
      "0.7637830569251457\n",
      "0.7638359847636119\n",
      "0.7636648745519713\n",
      "0.7637178051511758\n",
      "0.7635467980295566\n",
      "0.7633758674725767\n",
      "0.7632050134288272\n",
      "0.7632579995524726\n",
      "0.7630872483221477\n",
      "0.7629165734734958\n",
      "0.7629695885509838\n",
      "0.7630225799239884\n",
      "0.7630755476084041\n",
      "0.7631284916201118\n",
      "0.7631814119749777\n",
      "0.7632343086888541\n",
      "0.7630638677981242\n",
      "0.7631167671355213\n",
      "0.7629464285714286\n",
      "0.7629993305065833\n",
      "0.7630522088353414\n",
      "0.7631050635734999\n",
      "0.7631578947368421\n",
      "0.7632107023411371\n",
      "0.76326348640214\n",
      "0.7633162469355917\n",
      "0.7633689839572193\n",
      "0.7634216974827356\n",
      "0.7634743875278397\n",
      "0.7633043865508795\n",
      "0.7633570792520036\n",
      "0.7634097484976631\n",
      "0.7634623943035158\n",
      "0.7635150166852058\n",
      "0.763567615658363\n",
      "0.7633978207694018\n",
      "0.76345042240996\n",
      "0.7635030006668149\n",
      "0.7635555555555555\n",
      "0.7636080870917574\n",
      "0.7634384717903154\n",
      "0.7634910059960026\n",
      "0.7635435168738899\n",
      "0.7633740288568257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7634265423879272\n",
      "0.7634790326159308\n",
      "0.7633096716947648\n",
      "0.7633621645597694\n",
      "0.7634146341463415\n",
      "0.7634670804699624\n",
      "0.7632978723404256\n",
      "0.7633503212940395\n",
      "0.7634027470093044\n",
      "0.7634551495016612\n",
      "0.7632860938883969\n",
      "0.7633384990037636\n",
      "0.7633908809207613\n",
      "0.7634432396547909\n",
      "0.7634955752212389\n",
      "0.7635478876354789\n",
      "0.7633790358248562\n",
      "0.7634313508733142\n",
      "0.7634836427939876\n",
      "0.7635359116022099\n",
      "0.7635881573133009\n",
      "0.7636403799425668\n",
      "0.7636925795053003\n",
      "0.7637447560167807\n",
      "0.7637969094922737\n",
      "0.7636283381152064\n",
      "0.7636804942630185\n",
      "0.7637326273990734\n",
      "0.7637847375385972\n",
      "0.7638368246968027\n",
      "0.7638888888888888\n",
      "0.7639409301300418\n",
      "0.7637725870427501\n",
      "0.7638246309759859\n",
      "0.7638766519823789\n",
      "0.7639286500770756\n",
      "0.7639806252752092\n",
      "0.7640325775918996\n",
      "0.7640845070422535\n",
      "0.7639163916391639\n",
      "0.7639683238011439\n",
      "0.764020233120739\n",
      "0.7638522427440633\n",
      "0.7639041547592877\n",
      "0.763956043956044\n",
      "0.7640079103493738\n",
      "0.7638400702987698\n",
      "0.7638919393806282\n",
      "0.7639437856829161\n",
      "0.7639956092206367\n",
      "0.7640474100087796\n",
      "0.7640991880623217\n",
      "0.7641509433962265\n",
      "0.7642026760254442\n",
      "0.7642543859649122\n",
      "0.7643060732295549\n",
      "0.7643577378342832\n",
      "0.7644093797939951\n",
      "0.7644609991235758\n",
      "0.764512595837897\n",
      "0.7645641699518178\n",
      "0.764615721480184\n",
      "0.7646672504378283\n",
      "0.764718756839571\n",
      "0.7647702407002188\n",
      "0.7646029315248305\n",
      "0.7646544181977253\n",
      "0.7644872075224142\n",
      "0.7643200699606472\n",
      "0.7643715846994535\n",
      "0.7644230769230769\n",
      "0.7644745466462749\n",
      "0.764525993883792\n",
      "0.7645774186503603\n",
      "0.7646288209606987\n",
      "0.7646802008295132\n",
      "0.7647315582714972\n",
      "0.7645646956142265\n",
      "0.7643979057591623\n",
      "0.7644492911668485\n",
      "0.7645006541648496\n",
      "0.7643339873555701\n",
      "0.7643853530950305\n",
      "0.7644366964480279\n",
      "0.7644880174291939\n",
      "0.7643214985841864\n",
      "0.7643728222996515\n",
      "0.764424123666449\n",
      "0.7644754026991728\n",
      "0.7645266594124048\n",
      "0.7645778938207136\n",
      "0.7646291059386556\n",
      "0.7644628099173554\n",
      "0.7645140247879973\n",
      "0.7645652173913043\n",
      "0.7646163877417953\n",
      "0.7646675358539765\n",
      "0.7647186617423419\n",
      "0.7645525629887054\n",
      "0.7646036916395222\n",
      "0.7646547980894486\n",
      "0.7647058823529411\n",
      "0.7647569444444444\n",
      "0.7645910175743111\n",
      "0.764642082429501\n",
      "0.7646931251355454\n",
      "0.7647441457068517\n",
      "0.764578365488836\n",
      "0.764629388816645\n",
      "0.7644637053087757\n",
      "0.7645147313691508\n",
      "0.7645657353259693\n",
      "0.7644001732351667\n",
      "0.7642346828317818\n",
      "0.7640692640692641\n",
      "0.7639039169011037\n",
      "0.7639549978364344\n",
      "0.7637897469175859\n",
      "0.763840830449827\n",
      "0.7638918918918919\n",
      "0.7639429312581063\n",
      "0.7639939485627837\n",
      "0.7640449438202247\n",
      "0.7640959170447181\n",
      "0.76414686825054\n",
      "0.7641977974519543\n",
      "0.7642487046632125\n",
      "0.7642995898985538\n",
      "0.7641346568839016\n",
      "0.764185544768069\n",
      "0.7642364106988784\n",
      "0.7642872546905327\n",
      "0.764338076757223\n",
      "0.7643888769131278\n",
      "0.7644396551724137\n",
      "0.764490411549235\n",
      "0.7645411460577337\n",
      "0.7645918587120396\n",
      "0.7646425495262704\n",
      "0.7646932185145318\n",
      "0.7647438656909169\n",
      "0.7647944910695073\n",
      "0.7646299483648882\n",
      "0.7646805764680576\n",
      "0.764731182795699\n",
      "0.7647817673618577\n",
      "0.7648323301805675\n",
      "0.76488287126585\n",
      "0.7647185217017619\n",
      "0.7645542427497315\n",
      "0.7646048109965635\n",
      "0.7646553575263045\n",
      "0.7647058823529411\n",
      "0.7647563854904486\n",
      "0.7648068669527897\n",
      "0.7648573267539155\n",
      "0.7649077649077649\n",
      "0.7647437272142398\n",
      "0.7645797598627787\n",
      "0.7646302250803858\n",
      "0.7646806686669524\n",
      "0.7647310906363831\n",
      "0.7647814910025706\n",
      "0.764831869779396\n",
      "0.764882226980728\n",
      "0.7649325626204239\n",
      "0.7647688356164384\n",
      "0.7648191739781725\n",
      "0.7648694908001712\n",
      "0.7649197860962567\n",
      "0.7649700598802395\n",
      "0.7650203121659184\n",
      "0.764856776400171\n",
      "0.7649070314169695\n",
      "0.7649572649572649\n",
      "0.7650074770348216\n",
      "0.7650576676633917\n",
      "0.7651078368567158\n",
      "0.7651579846285226\n",
      "0.7652081109925294\n",
      "0.765044814340589\n",
      "0.7650949434606358\n",
      "0.7651450511945392\n",
      "0.7651951375559821\n",
      "0.7650319829424307\n",
      "0.7650820720528672\n",
      "0.7651321398124468\n",
      "0.7651821862348178\n",
      "0.7652322113336174\n",
      "0.7652822151224707\n",
      "0.7653321976149915\n",
      "0.7653821588247818\n",
      "0.7654320987654321\n",
      "0.7654820174505214\n",
      "0.765531914893617\n",
      "0.7655817911082748\n",
      "0.7654189706507869\n",
      "0.7654688496704232\n",
      "0.7653061224489796\n",
      "0.765356004250797\n",
      "0.7651933701657458\n",
      "0.7652432547270024\n",
      "0.7650807136788446\n",
      "0.7651306009768528\n",
      "0.7649681528662421\n",
      "0.7650180428783697\n",
      "0.7650679117147708\n",
      "0.7649055803097815\n",
      "0.7649554518455663\n",
      "0.7650053022269353\n",
      "0.7650551314673452\n",
      "0.7651049395802417\n",
      "0.7651547265790589\n",
      "0.7652044924772198\n",
      "0.7652542372881356\n",
      "0.7650921414954459\n",
      "0.765141889030072\n",
      "0.7651916154986238\n",
      "0.7650296359017782\n",
      "0.765079365079365\n",
      "0.7651290732120186\n",
      "0.765178760313095\n",
      "0.7652284263959391\n",
      "0.7652780714738845\n",
      "0.7653276955602537\n",
      "0.7651659268653561\n",
      "0.7652155536770922\n",
      "0.7652651595182759\n",
      "0.7653147444021968\n",
      "0.7653643083421331\n",
      "0.7654138513513513\n",
      "0.7654633734431074\n",
      "0.7655128746306459\n",
      "0.7655623549271998\n",
      "0.7656118143459916\n",
      "0.7654503269352457\n",
      "0.7654997891185154\n",
      "0.7655492304448661\n",
      "0.7655986509274874\n",
      "0.7656480505795574\n",
      "0.7656974294142436\n",
      "0.7657467874447019\n",
      "0.7657961246840775\n",
      "0.7658454411455043\n",
      "0.7656842105263157\n",
      "0.7657335297832035\n",
      "0.7657828282828283\n",
      "0.7658321060382917\n",
      "0.7658813630626841\n",
      "0.7659305993690851\n",
      "0.7659798149705634\n",
      "0.7660290098801766\n",
      "0.766078184110971\n",
      "0.7659172094977936\n",
      "0.7657563025210085\n",
      "0.7655954631379962\n",
      "0.7654346913061739\n",
      "0.7652739869829939\n",
      "0.7653232577665827\n",
      "0.7653725078698845\n",
      "0.7654217373059169\n",
      "0.7654709460876862\n",
      "0.7655201342281879\n",
      "0.7655693017404068\n",
      "0.7654088050314466\n",
      "0.7654579752672396\n",
      "0.7652975691533948\n",
      "0.7651372302535093\n",
      "0.7651864264767491\n",
      "0.7650261780104712\n",
      "0.7650753768844221\n",
      "0.7651245551601423\n",
      "0.7651737128505651\n",
      "0.7650136011717933\n",
      "0.7650627615062762\n",
      "0.7651119012758837\n",
      "0.7651610204935173\n",
      "0.7652101191720677\n",
      "0.7652591973244147\n",
      "0.7653082549634274\n",
      "0.7651483493522775\n",
      "0.7651974096511385\n",
      "0.7652464494569757\n",
      "0.7652954687826269\n",
      "0.7653444676409186\n",
      "0.765184721352536\n",
      "0.7652337228714524\n",
      "0.7652827039432506\n",
      "0.7653316645807259\n",
      "0.7653806047966631\n",
      "0.7654295246038365\n",
      "0.7654784240150094\n",
      "0.7655273030429346\n",
      "0.7655761617003543\n",
      "0.765625\n",
      "0.7656738179545928\n",
      "0.765722615576843\n",
      "0.765563189673121\n",
      "0.7656119900083264\n",
      "0.7656607700312175\n",
      "0.7657095297544736\n",
      "0.7657582691907635\n",
      "0.7658069883527454\n",
      "0.7658556872530672\n",
      "0.7656964656964657\n",
      "0.7657451673248805\n",
      "0.7657938487115544\n",
      "0.7658425098691045\n",
      "0.7658911508101371\n",
      "0.7659397715472482\n",
      "0.7659883720930233\n",
      "0.7660369524600373\n",
      "0.7660855126608551\n",
      "0.7661340527080307\n",
      "0.7661825726141079\n",
      "0.76623107239162\n",
      "0.7660721692243883\n",
      "0.7661206717810491\n",
      "0.7659618573797679\n",
      "0.7660103626943006\n",
      "0.7660588479071695\n",
      "0.766107313030868\n",
      "0.7659486329743165\n",
      "0.765997100849037\n",
      "0.7660455486542443\n",
      "0.7660939764024012\n",
      "0.7659354304635762\n",
      "0.765983860955928\n",
      "0.7660322714108398\n",
      "0.7658738366080662\n",
      "0.7657154673283706\n",
      "0.7655571635311144\n",
      "0.7656056221579165\n",
      "0.765447406488944\n",
      "0.765495867768595\n",
      "0.7655443090270605\n",
      "0.7653862040479141\n",
      "0.7654346479454883\n",
      "0.7652766308835673\n",
      "0.7653250773993808\n",
      "0.7653735039207594\n",
      "0.765215597276666\n",
      "0.7650577557755776\n",
      "0.7651062074654568\n",
      "0.7651546391752577\n",
      "0.7652030509173366\n",
      "0.7652514427040396\n",
      "0.7652998145477025\n",
      "0.765142150803461\n",
      "0.7651905252317199\n",
      "0.7652388797364086\n",
      "0.7650813259213506\n",
      "0.7651296829971181\n",
      "0.7649722165054538\n",
      "0.7650205761316873\n",
      "0.765068915860934\n",
      "0.765117235705471\n",
      "0.7651655356775653\n",
      "0.7652138157894737\n",
      "0.765262076053443\n",
      "0.7651048088779285\n",
      "0.7651530717074173\n",
      "0.7652013147082991\n",
      "0.7652495378927912\n",
      "0.7652977412731006\n",
      "0.7653459248614247\n",
      "0.7653940886699507\n",
      "0.7654422327108558\n",
      "0.765490356996307\n",
      "0.7655384615384615\n",
      "0.7655865463494668\n",
      "0.7654295673569818\n",
      "0.7654776547765477\n",
      "0.7655257224841155\n",
      "0.7655737704918033\n",
      "0.7654169227617291\n",
      "0.765464973371569\n",
      "0.7653082121646528\n",
      "0.7651515151515151\n",
      "0.7651995905834186\n",
      "0.7652476463364716\n",
      "0.7650910579087374\n",
      "0.765139116202946\n",
      "0.7651871548373901\n",
      "0.7652351738241309\n",
      "0.7652831731752198\n",
      "0.7653311529026983\n",
      "0.765379113018598\n",
      "0.7654270535349408\n",
      "0.7654749744637385\n",
      "0.7655228758169934\n",
      "0.765570757606698\n",
      "0.7656186198448346\n",
      "0.7656664625433762\n",
      "0.7657142857142857\n",
      "0.765558049377678\n",
      "0.7654018767849857\n",
      "0.7654497246583725\n",
      "0.7654975530179445\n",
      "0.7655453618756372\n",
      "0.7655931512433755\n",
      "0.7654371306297126\n",
      "0.7654849225753871\n",
      "0.7653289875738439\n",
      "0.7653767820773931\n",
      "0.7654245571166769\n",
      "0.7652687296416938\n",
      "0.7651129656014655\n",
      "0.7651607651607651\n",
      "0.7652085452695829\n",
      "0.765052888527258\n",
      "0.7651006711409396\n",
      "0.7651484343228955\n",
      "0.7651961780849766\n",
      "0.7652439024390244\n",
      "0.7652916073968705\n",
      "0.7653392929703373\n",
      "0.765386959171237\n",
      "0.7654346060113729\n",
      "0.7654822335025381\n",
      "0.7655298416565165\n",
      "0.7653744672214329\n",
      "0.765422077922078\n",
      "0.7654696693041185\n",
      "0.7655172413793103\n",
      "0.7655647941593997\n",
      "0.7654095701540957\n",
      "0.7652544090816947\n",
      "0.7653019862180787\n",
      "0.7653495440729483\n",
      "0.7653970826580226\n",
      "0.7652420498278306\n",
      "0.7650870797893884\n",
      "0.7649321725045556\n",
      "0.7649797570850202\n",
      "0.7648249342238413\n",
      "0.7648725212464589\n",
      "0.7649200890147684\n",
      "0.764967637540453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7650151668351871\n",
      "0.7648604933279418\n",
      "0.7649080250656963\n",
      "0.7647534357316087\n",
      "0.7648009698929077\n",
      "0.7648484848484849\n",
      "0.7648959806099778\n",
      "0.7649434571890146\n",
      "0.7647890167575206\n",
      "0.7648364957610012\n",
      "0.7646821392532795\n",
      "0.764729620661824\n",
      "0.7645753479927375\n",
      "0.7646228317870108\n",
      "0.764670296430732\n",
      "0.7647177419354839\n",
      "0.7647651683128401\n",
      "0.7648125755743652\n",
      "0.7646584726979649\n",
      "0.7647058823529411\n",
      "0.764551863041289\n",
      "0.7645992750704793\n",
      "0.7646466680088585\n",
      "0.7644927536231884\n",
      "0.7645401489233246\n",
      "0.7645875251509054\n",
      "0.7644337155501911\n",
      "0.7642799678197908\n",
      "0.7643273677860446\n",
      "0.7643747486932047\n",
      "0.7644221105527638\n",
      "0.7644694533762058\n",
      "0.764516777175005\n",
      "0.7645640819606268\n",
      "0.764611367744527\n",
      "0.7644578313253012\n",
      "0.764505119453925\n",
      "0.7643516659975913\n",
      "0.7643989564519366\n",
      "0.764446227929374\n",
      "0.7642928786359077\n",
      "0.764340152426795\n",
      "0.7641868859033487\n",
      "0.7640336808340016\n",
      "0.7640809781519342\n",
      "0.7641282565130261\n",
      "0.7641755159286716\n",
      "0.7640224358974359\n",
      "0.7640696975766073\n",
      "0.7641169403283941\n",
      "0.7639639639639639\n",
      "0.7640112089671738\n",
      "0.7640584350610367\n",
      "0.7641056422569028\n",
      "0.7641528305661133\n",
      "0.7642\n",
      "0.764247150569886\n",
      "0.7642942822870852\n",
      "0.7641415150909454\n",
      "0.7641886490807354\n",
      "0.7642357642357642\n",
      "0.7640831002796644\n",
      "0.7641302176952267\n",
      "0.7641773162939297\n",
      "0.7640247554402076\n",
      "0.7640718562874251\n",
      "0.7641189383356616\n",
      "0.7641660015961692\n",
      "0.7640135647316976\n",
      "0.7638611886717191\n",
      "0.7639082751744766\n",
      "0.7639553429027113\n",
      "0.76400239186765\n",
      "0.7640494220805102\n",
      "0.7640964335525005\n",
      "0.7641434262948207\n",
      "0.7639912368054173\n",
      "0.7640382317801673\n",
      "0.7640852080430022\n",
      "0.7641321656050956\n",
      "0.7639800995024876\n",
      "0.7640270592916832\n",
      "0.7640740003978516\n",
      "0.7641209228321401\n",
      "0.764167826605687\n",
      "0.7640159045725646\n",
      "0.7638640429338104\n",
      "0.7637122416534181\n",
      "0.7637591893502881\n",
      "0.7638061183949146\n",
      "0.7638530287984111\n",
      "0.7638999205718825\n",
      "0.7639467937264245\n",
      "0.7639936482731242\n",
      "0.763842032149236\n",
      "0.7638888888888888\n",
      "0.763935727038286\n",
      "0.7639825466084887\n",
      "0.7640293476105493\n",
      "0.7640761300555114\n",
      "0.7641228939544104\n",
      "0.7641696393182719\n",
      "0.7642163661581137\n",
      "0.7642630744849446\n",
      "0.7643097643097643\n",
      "0.7641584158415842\n",
      "0.7642051078994259\n",
      "0.7642517814726841\n",
      "0.7642984365723333\n",
      "0.7641472101305896\n",
      "0.7641938674579624\n",
      "0.7642405063291139\n",
      "0.7642871267549931\n",
      "0.7641360221431396\n",
      "0.7641826447914608\n",
      "0.7642292490118577\n",
      "0.7640782454060462\n",
      "0.7641248518372185\n",
      "0.7641714398577918\n",
      "0.764218009478673\n",
      "0.7642645607107601\n",
      "0.7643110935649428\n",
      "0.7643576080521018\n",
      "0.7644041041831097\n",
      "0.7644505819688301\n",
      "0.7642998027613412\n",
      "0.7643462827844607\n",
      "0.7643927444794952\n",
      "0.7642420658387542\n",
      "0.7642885297595585\n",
      "0.7643349753694582\n",
      "0.7641843971631206\n",
      "0.7640338782745716\n",
      "0.7640803465931469\n",
      "0.7641267966135066\n",
      "0.7641732283464567\n",
      "0.7642196418027948\n",
      "0.7642660369933097\n",
      "0.7643124139287822\n",
      "0.7643587726199843\n",
      "0.7644051130776794\n",
      "0.7644514353126229\n",
      "0.7644977393355612\n",
      "0.7645440251572327\n",
      "0.7645902927883671\n",
      "0.7646365422396857\n",
      "0.7644863484580633\n",
      "0.7645326001571092\n",
      "0.7645788336933045\n",
      "0.7646250490773459\n",
      "0.7644749754661433\n",
      "0.7643249607535322\n",
      "0.7643711987443594\n",
      "0.7644174185955277\n",
      "0.7644636203177093\n",
      "0.764313725490196\n",
      "0.7643599294256028\n",
      "0.764406115248922\n",
      "0.7644522829708015\n",
      "0.7644984326018809\n",
      "0.7643486777668952\n",
      "0.7643948296122209\n",
      "0.7644409633835911\n",
      "0.764487079091621\n",
      "0.7643374437267567\n",
      "0.7643835616438356\n",
      "0.7644296615143807\n",
      "0.7644757433489828\n",
      "0.7645218071582242\n",
      "0.7645678529526789\n",
      "0.764613880742913\n",
      "0.764659890539484\n",
      "0.7647058823529411\n",
      "0.7645564673700664\n",
      "0.7644071107638211\n",
      "0.764453125\n",
      "0.7643038469049014\n",
      "0.764349863334635\n",
      "0.7642006636736287\n",
      "0.7642466822794691\n",
      "0.7642926829268293\n",
      "0.7643386656262193\n",
      "0.7643846303881412\n",
      "0.7644305772230889\n",
      "0.7642815363618639\n",
      "0.764327485380117\n",
      "0.7641785227051257\n",
      "0.7640296180826188\n",
      "0.7640755893239821\n",
      "0.7641215426567978\n",
      "0.7641674780915287\n",
      "0.7642133956386293\n",
      "0.7642592953085459\n",
      "0.7641105488516933\n",
      "0.7641564506713369\n",
      "0.7642023346303501\n",
      "0.7642482007391558\n",
      "0.7640995721509141\n",
      "0.7639510013610733\n",
      "0.7639968895800933\n",
      "0.7638483965014577\n",
      "0.763699961134862\n",
      "0.7637458713813872\n",
      "0.7635975135975136\n",
      "0.7636434259079433\n",
      "0.7634951456310679\n",
      "0.7633469229275869\n",
      "0.7633928571428571\n",
      "0.7634387735299826\n",
      "0.763290648040357\n",
      "0.7633365664403492\n",
      "0.7633824670287044\n",
      "0.7634283498157843\n",
      "0.7634742148119427\n",
      "0.7635200620275248\n",
      "0.7633720930232558\n",
      "0.7634179422592521\n",
      "0.763463773731112\n",
      "0.7633159016075924\n",
      "0.7633617350890782\n",
      "0.7634075508228461\n",
      "0.7634533488192025\n",
      "0.7634991290884459\n",
      "0.7635448916408669\n",
      "0.763590636486748\n",
      "0.7636363636363637\n",
      "0.7636820730999807\n",
      "0.7637277648878577\n",
      "0.7637734390102455\n",
      "0.7638190954773869\n",
      "0.763864734299517\n",
      "0.7639103554868625\n",
      "0.7639559590496426\n",
      "0.7640015449980687\n",
      "0.7640471133423441\n",
      "0.7640926640926641\n",
      "0.7641381972592164\n",
      "0.7641837128521807\n",
      "0.7642292108817287\n",
      "0.7642746913580247\n",
      "0.7643201542912247\n",
      "0.7643655996914771\n",
      "0.7642182379024485\n",
      "0.7642636854279106\n",
      "0.7641164000770861\n",
      "0.7639691714836223\n",
      "0.7638219996147178\n",
      "0.7638674884437596\n",
      "0.7639129597535144\n",
      "0.7637658837119754\n",
      "0.7638113570741097\n",
      "0.7638568129330254\n",
      "0.7639022512988263\n",
      "0.7639476721816083\n",
      "0.7639930755914599\n",
      "0.7638461538461538\n",
      "0.7638915593155162\n",
      "0.7639369473279508\n",
      "0.76379012108399\n",
      "0.7636433512682552\n",
      "0.7634966378482229\n",
      "0.7635420668459469\n",
      "0.7633954292298829\n",
      "0.7634408602150538\n",
      "0.7632942983298138\n",
      "0.7633397312859885\n",
      "0.7633851468048359\n",
      "0.763430544896393\n",
      "0.7634759255706887\n",
      "0.7633294975067128\n",
      "0.7633748801534036\n",
      "0.763420245398773\n",
      "0.7634655932528273\n",
      "0.7633192794174013\n",
      "0.7631730216516575\n",
      "0.7632183908045977\n",
      "0.7632637425780502\n",
      "0.7631175794714669\n",
      "0.7631629331801647\n",
      "0.763208269525268\n",
      "0.7632535885167464\n",
      "0.7632988901645618\n",
      "0.7633441744786684\n",
      "0.763389441469013\n",
      "0.7634346911455345\n",
      "0.7632887189292543\n",
      "0.7633339705601223\n",
      "0.7633792048929664\n",
      "0.763424421937703\n",
      "0.7632785632403516\n",
      "0.763323782234957\n",
      "0.7633689839572193\n",
      "0.7634141684170327\n",
      "0.763459335624284\n",
      "0.7635044855888529\n",
      "0.7633587786259542\n",
      "0.7632131272657889\n",
      "0.7630675314765357\n",
      "0.7631127217242037\n",
      "0.7629672006102212\n",
      "0.7630123927550048\n",
      "0.7630575676706062\n",
      "0.7631027253668763\n",
      "0.7631478658536586\n",
      "0.7630024766622214\n",
      "0.7628571428571429\n",
      "0.7629023043229861\n",
      "0.7629474485910129\n",
      "0.7629925756710452\n",
      "0.7630376855728969\n",
      "0.7628924833491912\n",
      "0.762937595129376\n",
      "0.762982689747004\n",
      "0.7630277672118676\n",
      "0.7630728275337517\n",
      "0.7631178707224334\n",
      "0.7629728188557309\n",
      "0.7630178639300647\n",
      "0.7628728861865856\n",
      "0.7629179331306991\n",
      "0.762962962962963\n",
      "0.7628180782377516\n",
      "0.7628631099297513\n",
      "0.7629081245254365\n",
      "0.7629531220345417\n",
      "0.7629981024667931\n",
      "0.7630430658319105\n",
      "0.7628983308042488\n",
      "0.7629432960364119\n",
      "0.7627986348122867\n",
      "0.7626540284360189\n",
      "0.7625094768764216\n",
      "0.7625544817130946\n",
      "0.7624100037893141\n",
      "0.76245501041864\n",
      "0.7623106060606061\n",
      "0.762166256390835\n",
      "0.7620219613782658\n",
      "0.7620670073821693\n",
      "0.7619227857683573\n",
      "0.7619678334910123\n",
      "0.7620128641695043\n",
      "0.7620578778135049\n",
      "0.7621028744326778\n",
      "0.76214785403668\n",
      "0.7621928166351607\n",
      "0.762048762048762\n",
      "0.7620937263794406\n",
      "0.7621386737200075\n",
      "0.7621836040800907\n",
      "0.7622285174693106\n",
      "0.762273413897281\n",
      "0.762129507268265\n",
      "0.7621744054360136\n",
      "0.76221928665786\n",
      "0.7622641509433963\n",
      "0.7623089983022071\n",
      "0.7623538287438703\n",
      "0.7623986422779558\n",
      "0.7622549019607843\n",
      "0.7622997172478794\n",
      "0.7623445156426687\n",
      "0.7623892971547014\n",
      "0.7624340617935192\n",
      "0.762478809568657\n",
      "0.7625235404896422\n",
      "0.7625682545659951\n",
      "0.7626129518072289\n",
      "0.7624694146433277\n",
      "0.7625141136620248\n",
      "0.7623706491063029\n",
      "0.7624153498871332\n",
      "0.7624600338536769\n",
      "0.7625047010154193\n",
      "0.7623613461176912\n",
      "0.762406015037594\n",
      "0.7624506671678256\n",
      "0.7624953025178505\n",
      "0.7625399210971257\n",
      "0.7625845229151015\n",
      "0.7626291079812206\n",
      "0.7624859181374389\n",
      "0.7625305049746574\n",
      "0.762575075075075\n",
      "0.7624319759804842\n",
      "0.7624765478424015\n",
      "0.7623335209154005\n",
      "0.7621905476369092\n",
      "0.762235139696231\n",
      "0.7620922384701913\n",
      "0.762136832239925\n",
      "0.7621814092953523\n",
      "0.7622259696458684\n",
      "0.7622705133008617\n",
      "0.7621277392770182\n",
      "0.7619850187265917\n",
      "0.7618423516195469\n",
      "0.7618869337326843\n",
      "0.7617443383866741\n",
      "0.7617889221556886\n",
      "0.7618334892422826\n",
      "0.7618780396558175\n",
      "0.7619225734056481\n",
      "0.7619670905011219\n",
      "0.7618246401196486\n",
      "0.7618691588785047\n",
      "0.7619136609979443\n",
      "0.7617713004484304\n",
      "0.7618158042219316\n",
      "0.7618602913709376\n",
      "0.7619047619047619\n",
      "0.7617625093353249\n",
      "0.7618069815195072\n",
      "0.7618514371033968\n",
      "0.7618958760962866\n",
      "0.7619402985074627\n",
      "0.761984704346204\n",
      "0.762029093621783\n",
      "0.7618870035427933\n",
      "0.7619313944817301\n",
      "0.7619757688723205\n",
      "0.7620201267238166\n",
      "0.762064468045463\n",
      "0.7619225037257824\n",
      "0.7619668467126094\n",
      "0.7620111731843575\n",
      "0.7620554831502514\n",
      "0.7620997766195086\n",
      "0.7621440536013401\n",
      "0.7621883141049498\n",
      "0.7622325581395348\n",
      "0.7620907738095238\n",
      "0.7621350195276176\n",
      "0.761993306061733\n",
      "0.7620375534485964\n",
      "0.7620817843866171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7621259988849656\n",
      "0.7621701969528056\n",
      "0.7620286085825748\n",
      "0.7618870728083209\n",
      "0.7619312906220984\n",
      "0.7619754920163386\n",
      "0.7618340449229627\n",
      "0.7618782479584262\n",
      "0.7619224345889776\n",
      "0.7619666048237477\n",
      "0.7620107586718605\n",
      "0.7620548961424333\n",
      "0.7620990172445763\n",
      "0.7621431219873934\n",
      "0.7621872103799815\n",
      "0.7622312824314307\n",
      "0.7622753381508245\n",
      "0.7623193775472398\n",
      "0.7623634006297463\n",
      "0.7624074074074074\n",
      "0.7624513978892797\n",
      "0.7624953720844132\n",
      "0.7623542476401999\n",
      "0.7623982235381199\n",
      "0.7622571692876966\n",
      "0.7623011468738439\n",
      "0.762345108193083\n",
      "0.7623890532544378\n",
      "0.7624329820669254\n",
      "0.7624768946395564\n",
      "0.7623359822583626\n",
      "0.7621951219512195\n",
      "0.7622390541289489\n",
      "0.7622829700775766\n",
      "0.7623268698060942\n",
      "0.7623707533234859\n",
      "0.7624146206387299\n",
      "0.7624584717607974\n",
      "0.7625023066986529\n",
      "0.7623616236162362\n",
      "0.7624054602471869\n",
      "0.7622648469199558\n",
      "0.7623086852295777\n",
      "0.7623525073746312\n",
      "0.7623963133640553\n",
      "0.7624401032067821\n",
      "0.7624838769117376\n",
      "0.7625276344878408\n",
      "0.7625713759440044\n",
      "0.7624309392265194\n",
      "0.7624746823789358\n",
      "0.7625184094256259\n",
      "0.7623780600036812\n",
      "0.7624217887375783\n",
      "0.7622815087396504\n",
      "0.7623252391464312\n",
      "0.7621850285083686\n",
      "0.7622287605737403\n",
      "0.7622724765581909\n",
      "0.7621323529411764\n",
      "0.7619922808307297\n",
      "0.7620360161705255\n",
      "0.7620797354400147\n",
      "0.7621234386480529\n",
      "0.7621671258034894\n",
      "0.7622107969151671\n",
      "0.7622544519919222\n",
      "0.762114537444934\n",
      "0.7619746742521564\n",
      "0.7620183486238532\n",
      "0.762062006971198\n",
      "0.7621056493030081\n",
      "0.7621492756280946\n",
      "0.762009534286762\n",
      "0.7620531622364803\n",
      "0.7619134897360704\n",
      "0.7619571192963167\n",
      "0.7618175155734701\n",
      "0.7618611467301704\n",
      "0.7619047619047619\n",
      "0.7619483611060245\n",
      "0.7619919443427317\n",
      "0.7618524620172067\n",
      "0.761896046852123\n",
      "0.761939615736505\n",
      "0.7618002195389681\n",
      "0.7618437900128041\n",
      "0.7618873445501098\n",
      "0.761930883159627\n",
      "0.7617915904936015\n",
      "0.7618351306890879\n",
      "0.7616959064327485\n",
      "0.7615567330531701\n",
      "0.7616002922908294\n",
      "0.7616438356164383\n",
      "0.7616873630387144\n",
      "0.7617308745663685\n",
      "0.7617743702081051\n",
      "0.7616353349151305\n",
      "0.7614963503649635\n",
      "0.7613574165298304\n",
      "0.7614009485589202\n",
      "0.7614444647091009\n",
      "0.7614879649890591\n",
      "0.761531449407475\n",
      "0.7615749179730222\n",
      "0.7616183706943686\n",
      "0.7616618075801749\n",
      "0.7617052286390964\n",
      "0.7617486338797814\n",
      "0.7617920233108724\n",
      "0.7616533139111434\n",
      "0.7616967048971418\n",
      "0.761740080087368\n",
      "0.7616014558689718\n",
      "0.7614628820960698\n",
      "0.7615062761506276\n",
      "0.7613677700982175\n",
      "0.7614111656664848\n",
      "0.7614545454545455\n",
      "0.7614979094710053\n",
      "0.7615412577244638\n",
      "0.7615845902235144\n",
      "0.7616279069767442\n",
      "0.7616712079927339\n",
      "0.7615328732292045\n",
      "0.7613945887052842\n",
      "0.7612563543936093\n",
      "0.761118170266836\n",
      "0.7611615245009075\n",
      "0.7612048630012702\n",
      "0.7612481857764877\n",
      "0.761291492835117\n",
      "0.7613347841857091\n",
      "0.7613780598368087\n",
      "0.7614213197969543\n",
      "0.7612833061446438\n",
      "0.7613265675969554\n",
      "0.7613698133719877\n",
      "0.7614130434782609\n",
      "0.7614562579242891\n",
      "0.7614994567185802\n",
      "0.7615426398696361\n",
      "0.7615858073859522\n",
      "0.7616289592760181\n",
      "0.761672095548317\n",
      "0.7615342862312285\n",
      "0.761396526772793\n",
      "0.7614396816784229\n",
      "0.7614828209764919\n",
      "0.7615259446754655\n",
      "0.7613882863340564\n",
      "0.7614314115308151\n",
      "0.7614745211420311\n",
      "0.7613369467028004\n",
      "0.7613800578034682\n",
      "0.7612425501173921\n",
      "0.7612856626941134\n",
      "0.7613287597039177\n",
      "0.7611913357400723\n",
      "0.7612344342176502\n",
      "0.761277517141826\n",
      "0.7613205845210175\n",
      "0.7613636363636364\n",
      "0.7614066726780884\n",
      "0.7614496934727731\n",
      "0.7613124211285379\n",
      "0.7613554434030281\n",
      "0.761398450171202\n",
      "0.7614414414414414\n",
      "0.7614844172221221\n",
      "0.7615273775216138\n",
      "0.7615703223482803\n",
      "0.7616132517104789\n",
      "0.7616561656165617\n",
      "0.7615190784737221\n",
      "0.7613820406694259\n",
      "0.7614249730118747\n",
      "0.7614678899082569\n",
      "0.7615107913669065\n",
      "0.761373853623449\n",
      "0.7614167565623876\n",
      "0.7614596440769369\n",
      "0.7613227893601725\n",
      "0.7613656783468105\n",
      "0.7614085519223859\n",
      "0.7614514100952039\n",
      "0.7614942528735632\n",
      "0.7615370802657568\n",
      "0.7615798922800718\n",
      "0.7616226889247891\n",
      "0.7616654702081838\n",
      "0.7615287995693523\n",
      "0.7615715823466093\n",
      "0.7616143497757848\n",
      "0.7616571018651362\n",
      "0.7616998386229156\n",
      "0.7617425600573682\n",
      "0.7617852661767341\n",
      "0.7618279569892473\n",
      "0.7616914531445977\n",
      "0.7615549982085275\n",
      "0.7615977073258104\n",
      "0.7616404011461319\n",
      "0.7616830796777081\n",
      "0.761546723952739\n",
      "0.7615894039735099\n",
      "0.7614531138153185\n",
      "0.7614957953122204\n",
      "0.7615384615384615\n",
      "0.7615811125022357\n",
      "0.7616237482117311\n",
      "0.7616663686751296\n",
      "0.7617089739006078\n",
      "0.761751563896336\n",
      "0.7616154395997141\n",
      "0.7616580310880829\n",
      "0.7615219721329046\n",
      "0.7615645651009109\n",
      "0.7614285714285715\n",
      "0.7614711658632387\n",
      "0.7615137450910389\n",
      "0.7615563091201142\n",
      "0.7614204139900071\n",
      "0.7614629794826048\n",
      "0.7613271494826971\n",
      "0.7613697164258962\n",
      "0.7614122681883024\n",
      "0.7612765198787663\n",
      "0.7611408199643493\n",
      "0.7611833897700945\n",
      "0.7612259444048468\n",
      "0.7610903260288616\n",
      "0.761132882080513\n",
      "0.7611754229741763\n",
      "0.761039886039886\n",
      "0.7610824283425316\n",
      "0.7609469562121751\n",
      "0.7609894999110162\n",
      "0.7610320284697509\n",
      "0.7608966376089664\n",
      "0.7609391675560299\n",
      "0.7609816823759559\n",
      "0.7610241820768137\n",
      "0.7610666666666667\n",
      "0.7611091361535727\n",
      "0.7611515905455838\n",
      "0.7611940298507462\n",
      "0.7612364540771007\n",
      "0.761278863232682\n",
      "0.7613212573255195\n",
      "0.7613636363636364\n",
      "0.7614060003550506\n",
      "0.7612708555200568\n",
      "0.7613132209405501\n",
      "0.7611781405251952\n",
      "0.7610431080361895\n",
      "0.7610854913089748\n",
      "0.7611278595495655\n",
      "0.7611702127659574\n",
      "0.7610352774330792\n",
      "0.7610776320453739\n",
      "0.7611199716462874\n",
      "0.7609851169383416\n",
      "0.7610274579273694\n",
      "0.7610697839178179\n",
      "0.7609350097396848\n",
      "0.7608002832861189\n",
      "0.7606656045317756\n",
      "0.7607079646017699\n",
      "0.7607503096797027\n",
      "0.7607926397735315\n",
      "0.7608349548912082\n",
      "0.7607003891050583\n",
      "0.7605658709106985\n",
      "0.7606082036775106\n",
      "0.7606505214778151\n",
      "0.7606928243195475\n",
      "0.7607351122106379\n",
      "0.7607773851590106\n",
      "0.7608196431725843\n",
      "0.7608618862592723\n",
      "0.7609041144269821\n",
      "0.7609463276836158\n",
      "0.7609885260370697\n",
      "0.760854218143311\n",
      "0.7608964178577731\n",
      "0.760938602681722\n",
      "0.7609807726230375\n",
      "0.7608465608465609\n",
      "0.7608887321460059\n",
      "0.7607545839210155\n",
      "0.7607967565661907\n",
      "0.7608389143461403\n",
      "0.7608810572687225\n",
      "0.7607470049330515\n",
      "0.7607891491985204\n",
      "0.7606551602676999\n",
      "0.7606973058637084\n",
      "0.7607394366197183\n",
      "0.7607815525435663\n",
      "0.7608236536430835\n",
      "0.7608657399260954\n",
      "0.7609078114004222\n",
      "0.7609498680738787\n",
      "0.7609919099542737\n",
      "0.7610339370494109\n",
      "0.7610759493670886\n",
      "0.7611179469150993\n",
      "0.7609841827768014\n",
      "0.7610261816903884\n",
      "0.7610681658468025\n",
      "0.7611101352538204\n",
      "0.7611520899192132\n",
      "0.7611940298507462\n",
      "0.7612359550561798\n",
      "0.7611023345620502\n",
      "0.7611442611442611\n",
      "0.7611861730128092\n",
      "0.7612280701754386\n",
      "0.7612699526398877\n",
      "0.7613118204138899\n",
      "0.7613536735051727\n",
      "0.7613955119214586\n",
      "0.7614373356704645\n",
      "0.7614791447599019\n",
      "0.7613457157876292\n",
      "0.7613875262789068\n",
      "0.7614293221229638\n",
      "0.7614711033274956\n",
      "0.7613377692172999\n",
      "0.7613795518207283\n",
      "0.7614213197969543\n",
      "0.7614630731536577\n",
      "0.7615048118985127\n",
      "0.761371588523443\n",
      "0.7614133286688822\n",
      "0.7614550542147605\n",
      "0.7613219094247246\n",
      "0.7613636363636364\n",
      "0.7614053487152596\n",
      "0.7614470464872423\n",
      "0.761488729687227\n",
      "0.7615303983228512\n",
      "0.7615720524017467\n",
      "0.7616136919315404\n",
      "0.7616553169198533\n",
      "0.7616969273743017\n",
      "0.761738523302496\n",
      "0.7617801047120419\n",
      "0.7616471819926715\n",
      "0.76168876482903\n",
      "0.7617303331589046\n",
      "0.7617718869898848\n",
      "0.7618134263295554\n",
      "0.7618549511854951\n",
      "0.761896461565278\n",
      "0.7619379574764726\n",
      "0.7619794389266423\n",
      "0.7620209059233449\n",
      "0.7618881727921964\n",
      "0.7617554858934169\n",
      "0.7617969702246212\n",
      "0.7618384401114207\n",
      "0.7618798955613577\n",
      "0.7617473024712844\n",
      "0.7617887593527057\n",
      "0.761830201809325\n",
      "0.7618716298486693\n",
      "0.7619130434782608\n",
      "0.7619544427056164\n",
      "0.7619958275382476\n",
      "0.761863375630106\n",
      "0.7617309697601669\n",
      "0.7617723718505647\n",
      "0.7618137595552467\n",
      "0.7618551328817093\n",
      "0.7617228204237583\n",
      "0.761764195172773\n",
      "0.7618055555555555\n",
      "0.7618469015795869\n",
      "0.7618882332523429\n",
      "0.7619295505812944\n",
      "0.761970853573907\n",
      "0.7620121422376409\n",
      "0.7618799861255636\n",
      "0.7619212762268077\n",
      "0.7619625520110958\n",
      "0.7620038134858728\n",
      "0.7620450606585789\n",
      "0.7620862935366488\n",
      "0.7621275121275122\n",
      "0.7619954962757665\n",
      "0.7618635261517146\n",
      "0.7617316017316017\n",
      "0.7617728531855956\n",
      "0.7616409901332871\n",
      "0.7616822429906542\n",
      "0.7615504412528119\n",
      "0.7615916955017301\n",
      "0.7616329354782909\n",
      "0.7616741611898997\n",
      "0.7615424520145253\n",
      "0.7615836791147994\n",
      "0.7616248919619706\n",
      "0.7616660905634289\n",
      "0.7617072749265595\n",
      "0.7617484450587422\n",
      "0.7617896009673518\n",
      "0.7618307426597583\n",
      "0.7618718701433258\n",
      "0.7617403314917127\n",
      "0.761608838253064\n",
      "0.7616499827407663\n",
      "0.7616911130284728\n",
      "0.7617322291235334\n",
      "0.7616008280144902\n",
      "0.7614694722318041\n",
      "0.7615106052767718\n",
      "0.7613793103448275\n",
      "0.761420444750905\n",
      "0.761461564977594\n",
      "0.7615026710322247\n",
      "0.7615437629221227\n",
      "0.7614125753660638\n",
      "0.7614536686186704\n",
      "0.7614947477182711\n",
      "0.7615358126721763\n",
      "0.7614047168187296\n",
      "0.7612736660929432\n",
      "0.7613147478919291\n",
      "0.7613558155540262\n",
      "0.7613968690865301\n",
      "0.761265909872721\n",
      "0.7613069647463456\n",
      "0.7613480055020633\n",
      "0.7613890321471549\n",
      "0.7614300446888965\n",
      "0.7614710431345592\n",
      "0.7615120274914089\n",
      "0.7615529977667067\n",
      "0.7615939539677087\n",
      "0.7616348961016658\n",
      "0.7616758241758241\n",
      "0.7615450643776824\n",
      "0.7614143494679025\n",
      "0.761455294319547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7614962251201098\n",
      "0.7615371418768228\n",
      "0.7614065180102916\n",
      "0.761447436117304\n",
      "0.7614883401920439\n",
      "0.7615292302417281\n",
      "0.7615701062735687\n",
      "0.7616109682947729\n",
      "0.7616518163125429\n",
      "0.7616926503340757\n",
      "0.7617334703665639\n",
      "0.7616030142147628\n",
      "0.7616438356164383\n",
      "0.7616846430405753\n",
      "0.7617254364943512\n",
      "0.7615950710251583\n",
      "0.7616358658453114\n",
      "0.7616766467065869\n",
      "0.7617174136161478\n",
      "0.7617581665811527\n",
      "0.7617989056087552\n",
      "0.7618396307061036\n",
      "0.7618803418803419\n",
      "0.7619210391386088\n",
      "0.7617908407382091\n",
      "0.7618315393815137\n",
      "0.7618722241202597\n",
      "0.7619128949615713\n",
      "0.7619535519125683\n",
      "0.7618234591087587\n",
      "0.7618641174462274\n",
      "0.7619047619047619\n",
      "0.7617747440273037\n",
      "0.7616447705169767\n",
      "0.7616854315933128\n",
      "0.7617260787992496\n",
      "0.7615961800818554\n",
      "0.7616368286445013\n",
      "0.7616774633481077\n",
      "0.7617180841997614\n",
      "0.7615882753919564\n",
      "0.7616288975975465\n",
      "0.7616695059625213\n",
      "0.7617101004939534\n",
      "0.7617506811989101\n",
      "0.7617912480844543\n",
      "0.7618318011576438\n",
      "0.7618723404255319\n",
      "0.7619128658951668\n",
      "0.7619533775735919\n",
      "0.7619938754678462\n",
      "0.761864262629699\n",
      "0.7619047619047619\n",
      "0.7619452474069036\n",
      "0.7619857191431486\n",
      "0.7620261771205168\n",
      "0.7620666213460231\n",
      "0.762107051826678\n",
      "0.7621474685694869\n",
      "0.7620180057754374\n",
      "0.7620584239130435\n",
      "0.7620988283239939\n",
      "0.7621392190152801\n",
      "0.762179595993889\n",
      "0.7620502376103191\n",
      "0.7619209231291363\n",
      "0.7617916525279945\n",
      "0.7616624257845632\n",
      "0.7615332428765265\n",
      "0.7615736815329829\n",
      "0.7616141064767717\n",
      "0.7614849974571961\n",
      "0.7615254237288136\n",
      "0.7615658362989324\n",
      "0.7614368010843782\n",
      "0.7614772149754362\n",
      "0.7615176151761518\n",
      "0.7615580016934801\n",
      "0.7615983745343718\n",
      "0.7614694430336888\n",
      "0.761509817197021\n",
      "0.7615501776950414\n",
      "0.761590524534687\n",
      "0.7616308577228895\n",
      "0.7616711772665764\n",
      "0.7617114831726703\n",
      "0.7617517754480893\n",
      "0.7617920540997464\n",
      "0.7618323191345504\n",
      "0.761872570559405\n",
      "0.7619128083812099\n",
      "0.7617840851495185\n",
      "0.7616554054054054\n",
      "0.7616956595169735\n",
      "0.7617359000337723\n",
      "0.7617761269626878\n",
      "0.761647535449021\n",
      "0.7616877637130802\n",
      "0.76172797840027\n",
      "0.7617681795174625\n",
      "0.761808367071525\n",
      "0.7618485410693203\n",
      "0.7618887015177066\n",
      "0.7619288484235374\n",
      "0.7618004045853001\n",
      "0.7618405528400471\n",
      "0.7618806875631952\n",
      "0.7619208087615839\n",
      "0.7617924528301887\n",
      "0.7618325753747685\n",
      "0.7618726844055237\n",
      "0.761912779929281\n",
      "0.7617845117845118\n",
      "0.7618246086517422\n",
      "0.7618646920228879\n",
      "0.7619047619047619\n",
      "0.7619448183041723\n",
      "0.7618166526492851\n",
      "0.7618567103935419\n",
      "0.7618967546662183\n",
      "0.761768661735037\n",
      "0.7618087073457724\n",
      "0.7618487394957983\n",
      "0.7618887581919005\n",
      "0.761760752688172\n",
      "0.7618007727196372\n",
      "0.7618407793080282\n",
      "0.7618807724601175\n",
      "0.7617528542646071\n",
      "0.7617928487493705\n",
      "0.7618328298086606\n",
      "0.7618727974492364\n",
      "0.7619127516778523\n",
      "0.7619526925012582\n",
      "0.7619926199261993\n",
      "0.7620325339594164\n",
      "0.7619047619047619\n",
      "0.7619446772841576\n",
      "0.7619845792826014\n",
      "0.7620244679068209\n",
      "0.761896782841823\n",
      "0.7619366728095158\n",
      "0.7619765494137354\n",
      "0.761848936526545\n",
      "0.7618888144675151\n",
      "0.7619286790557509\n",
      "0.7618011382658185\n",
      "0.7618410041841004\n",
      "0.7618808567603749\n",
      "0.7619206960013385\n",
      "0.7617932418869187\n",
      "0.76183308245526\n",
      "0.7618729096989967\n",
      "0.7619127236248119\n",
      "0.7619525242393849\n",
      "0.7618251713187364\n",
      "0.7618649732620321\n",
      "0.7619047619047619\n",
      "0.7619445372535917\n",
      "0.7619842993151829\n",
      "0.7620240480961924\n",
      "0.7618968108198364\n",
      "0.7619365609348915\n",
      "0.7618093807377734\n",
      "0.761849132176235\n",
      "0.7618888703487402\n",
      "0.7619285952619286\n",
      "0.7618015012510425\n",
      "0.76184122748499\n",
      "0.7618809404702351\n",
      "0.7619206402134044\n",
      "0.7617936322720453\n",
      "0.7618333333333334\n",
      "0.7618730211631395\n",
      "0.7619126957680773\n",
      "0.761952357154756\n",
      "0.7619920053297802\n",
      "0.7620316402997502\n",
      "0.7620712620712621\n",
      "0.7621108706509073\n",
      "0.7621504660452729\n",
      "0.7621900482609419\n",
      "0.7622296173044925\n",
      "0.7622691731824988\n",
      "0.762142381902861\n",
      "0.7620156327956095\n",
      "0.7620552045227802\n",
      "0.7620947630922693\n",
      "0.7621343085106383\n",
      "0.7621738407844441\n",
      "0.7622133599202393\n",
      "0.7622528659245722\n",
      "0.7622923588039867\n",
      "0.7623318385650224\n",
      "0.7622052474261043\n",
      "0.7622447285405944\n",
      "0.7622841965471447\n",
      "0.7621576763485477\n",
      "0.7620311981413873\n",
      "0.7620706819313091\n",
      "0.7621101526211015\n",
      "0.7621496102172831\n",
      "0.7621890547263681\n",
      "0.7622284861548665\n",
      "0.7621021220159151\n",
      "0.7619757997679429\n",
      "0.7620152469340404\n",
      "0.7620546810273405\n",
      "0.7620941020543406\n",
      "0.7621335100215338\n",
      "0.7621729049354091\n",
      "0.7622122868024508\n",
      "0.7622516556291391\n",
      "0.76229101142195\n",
      "0.7623303541873552\n",
      "0.7623696839318219\n",
      "0.7624090006618134\n",
      "0.7624483043837883\n",
      "0.7624875951042012\n",
      "0.762361501571027\n",
      "0.7624007936507936\n",
      "0.7624400727392957\n",
      "0.7624793388429753\n",
      "0.7623533300280946\n",
      "0.7623925974884336\n",
      "0.7624318519742277\n",
      "0.7624710934919062\n",
      "0.7625103220478943\n",
      "0.762549537648613\n",
      "0.7625887403004787\n",
      "0.7626279300099043\n",
      "0.7626671067832975\n",
      "0.7627062706270628\n",
      "0.7627454215475994\n",
      "0.7627845595513032\n",
      "0.7628236846445654\n",
      "0.762697889182058\n",
      "0.7627370156636438\n",
      "0.762776129244972\n",
      "0.7628152299324213\n",
      "0.7628543177323666\n",
      "0.7628933926511782\n",
      "0.7629324546952224\n",
      "0.7629715038708614\n",
      "0.7630105401844532\n",
      "0.7630495636423514\n",
      "0.7630885742509055\n",
      "0.762962962962963\n",
      "0.7630019749835418\n",
      "0.763040974164884\n",
      "0.7629154327081277\n",
      "0.7629544332949498\n",
      "0.7629934210526316\n",
      "0.763032395987502\n",
      "0.7630713581058862\n",
      "0.7631103074141049\n",
      "0.7631492439184747\n",
      "0.7631881676253082\n",
      "0.7632270785409135\n",
      "0.7631016921307705\n",
      "0.7631406044678055\n",
      "0.7631795040236492\n",
      "0.7632183908045977\n",
      "0.7630930881628633\n",
      "0.7631319763624426\n",
      "0.7631708517971443\n",
      "0.7632097144732524\n",
      "0.763084495488105\n",
      "0.7631233595800525\n",
      "0.763162210923405\n",
      "0.7632010495244342\n",
      "0.7632398753894081\n",
      "0.7632786885245901\n",
      "0.7631535813801016\n",
      "0.7631923959357587\n",
      "0.7630673439292152\n",
      "0.7629423328964613\n",
      "0.762981162981163\n",
      "0.7630199803471994\n",
      "0.7630587850008187\n",
      "0.7630975769482645\n",
      "0.7631363561957767\n",
      "0.7630114566284779\n",
      "0.7630502372770414\n",
      "0.7630890052356021\n",
      "0.7631277605103877\n",
      "0.7631665031076218\n",
      "0.7632052330335242\n",
      "0.76324395029431\n",
      "0.763282654896191\n",
      "0.7633213468453743\n",
      "0.7633600261480634\n",
      "0.763235294117647\n",
      "0.7632739748407124\n",
      "0.763312642927148\n",
      "0.7631879797484893\n",
      "0.7630633572828217\n",
      "0.7629387755102041\n",
      "0.7628142344107085\n",
      "0.7628529459768238\n",
      "0.7628916449086162\n",
      "0.7627671724588024\n",
      "0.7626427406199021\n",
      "0.7625183493720437\n",
      "0.7625570776255708\n",
      "0.762432740909832\n",
      "0.7624714704923378\n",
      "0.7625101874490627\n",
      "0.7625488917861799\n",
      "0.7624246374450057\n",
      "0.7624633431085044\n",
      "0.7625020361622414\n",
      "0.7623778501628664\n",
      "0.7624165445367204\n",
      "0.762455226310648\n",
      "0.7623311085788702\n",
      "0.7623697916666666\n",
      "0.7622457282343369\n",
      "0.7622844126260983\n",
      "0.7623230844314299\n",
      "0.7623617436564737\n",
      "0.7624003903073671\n",
      "0.7624390243902439\n",
      "0.7624776459112339\n",
      "0.7625162548764629\n",
      "0.7625548512920527\n",
      "0.7625934351641209\n",
      "0.7626320064987815\n",
      "0.762508122157245\n",
      "0.7625466948189054\n",
      "0.7625852549529067\n",
      "0.7626238025653516\n",
      "0.7626623376623377\n",
      "0.7625385489368609\n",
      "0.762414800389484\n",
      "0.762291092000649\n",
      "0.7623296560674886\n",
      "0.7623682076236821\n",
      "0.7624067466753163\n",
      "0.7624452732284741\n",
      "0.7624837872892347\n",
      "0.762360188036959\n",
      "0.7623987034035656\n",
      "0.7624372062874737\n",
      "0.7624756966947505\n",
      "0.7625141746314595\n",
      "0.7625526401036605\n",
      "0.7625910931174089\n",
      "0.7626295336787565\n",
      "0.762667961793751\n",
      "0.7627063774684364\n",
      "0.7627447807088525\n",
      "0.7627831715210356\n",
      "0.7628215499110176\n",
      "0.7628599158848269\n",
      "0.7627365356622998\n",
      "0.7627749029754204\n",
      "0.7628132578819725\n",
      "0.7628516003879728\n",
      "0.7627283012768709\n",
      "0.7626050420168067\n",
      "0.7626433995798998\n",
      "0.7626817447495962\n",
      "0.7625585527378452\n",
      "0.7625968992248062\n",
      "0.7626352333279509\n",
      "0.7626735550532774\n",
      "0.7627118644067796\n",
      "0.7627501613944481\n",
      "0.7626270776182024\n",
      "0.7626653759277187\n",
      "0.7627036618809485\n",
      "0.7627419354838709\n",
      "0.7627801967424609\n",
      "0.7628184456626894\n",
      "0.7628566822505239\n",
      "0.7628949065119278\n",
      "0.7629331184528606\n",
      "0.7629713180792781\n",
      "0.7630095053971323\n",
      "0.7628865979381443\n",
      "0.7629247866000967\n",
      "0.762962962962963\n",
      "0.763001127032684\n",
      "0.7630392788151964\n",
      "0.7630774183164333\n",
      "0.7631155455423237\n",
      "0.7631536604987933\n",
      "0.7631917631917632\n",
      "0.7632298536271513\n",
      "0.7632679318108717\n",
      "0.7633059977488342\n",
      "0.7631832797427652\n",
      "0.7632213470503134\n",
      "0.7630986820957891\n",
      "0.7631367507632975\n",
      "0.7630141388174807\n",
      "0.7628915662650603\n",
      "0.7629296498554449\n",
      "0.7628071302392806\n",
      "0.7628452151573539\n",
      "0.7628832878471665\n",
      "0.7629213483146068\n",
      "0.7629593965655593\n",
      "0.762997432605905\n",
      "0.7628750200545483\n",
      "0.7629130574270132\n",
      "0.7629510825982357\n",
      "0.7628287363694676\n",
      "0.7628667628667629\n",
      "0.7629047771721705\n",
      "0.7629427792915532\n",
      "0.7629807692307692\n",
      "0.7628585162634193\n",
      "0.762736302467158\n",
      "0.762614127823162\n",
      "0.7626521460602178\n",
      "0.7626901521216973\n",
      "0.7627281460134486\n",
      "0.7627661277413158\n",
      "0.7628040973111395\n",
      "0.7628420547287565\n",
      "0.76272\n",
      "0.7627579587266038\n",
      "0.7627959053103007\n",
      "0.7628338397569167\n",
      "0.7628717620722737\n",
      "0.7627498001598721\n",
      "0.7627877237851662\n",
      "0.7628256352884769\n",
      "0.7628635346756152\n",
      "0.7629014219523885\n",
      "0.7629392971246006\n",
      "0.7629771601980514\n",
      "0.7628553177898435\n",
      "0.7628931821810634\n",
      "0.7629310344827587\n",
      "0.7629688747007183\n",
      "0.7628471113948292\n",
      "0.7628849529280357\n",
      "0.7629227823867263\n",
      "0.7629605997766788\n",
      "0.7628389154704944\n",
      "0.7628767341731781\n",
      "0.7627551020408163\n",
      "0.762633508688028\n",
      "0.7626713420465413\n",
      "0.7625498007968128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7625876354365838\n",
      "0.7626254580213477\n",
      "0.7626632685568653\n",
      "0.7627010670488932\n",
      "0.7627388535031847\n",
      "0.7626174176086611\n",
      "0.7626552053486151\n",
      "0.7626929810600032\n",
      "0.7625716104392107\n",
      "0.7626093874303899\n",
      "0.7624880687241489\n",
      "0.7625258469858438\n",
      "0.7624045801526718\n",
      "0.7624423596756241\n",
      "0.7623211446740858\n",
      "0.7623589254490543\n",
      "0.762396694214876\n",
      "0.762275544255522\n",
      "0.7623133142675564\n",
      "0.762351072279587\n",
      "0.7623888182973316\n",
      "0.7622677465459743\n",
      "0.762305493807558\n",
      "0.7623432290839816\n",
      "0.7623809523809524\n",
      "0.762418663704174\n",
      "0.7624563630593463\n",
      "0.7623353958432493\n",
      "0.7623730964467005\n",
      "0.7624107850911974\n",
      "0.7624484617824294\n",
      "0.7624861265260822\n",
      "0.7625237793278377\n",
      "0.7625614201933746\n",
      "0.7624405705229794\n",
      "0.7624782126445888\n",
      "0.7625158428390367\n",
      "0.7625534611119912\n",
      "0.7625910674691162\n",
      "0.7626286619160728\n",
      "0.762666244458518\n",
      "0.7627038151021054\n",
      "0.762741373852485\n",
      "0.7627789207153031\n",
      "0.7628164556962025\n",
      "0.7628539788008226\n",
      "0.762733312242961\n",
      "0.7626126838526016\n",
      "0.7624920936116382\n",
      "0.7625296442687747\n",
      "0.7625671830540626\n",
      "0.762604709973131\n",
      "0.7624841972187105\n",
      "0.762521725391057\n",
      "0.7625592417061612\n",
      "0.7624387932396146\n",
      "0.7624763108022742\n",
      "0.7625138165166587\n",
      "0.7625513103883802\n",
      "0.7625887924230466\n",
      "0.7626262626262627\n",
      "0.7626637210036294\n",
      "0.7627011675607447\n",
      "0.7627386023032025\n",
      "0.7626182965299685\n",
      "0.7626557325343006\n",
      "0.7626931567328918\n",
      "0.7625729150244364\n",
      "0.762610340479193\n",
      "0.7626477541371158\n",
      "0.762685156003782\n",
      "0.7625649913344887\n",
      "0.7624448645242596\n",
      "0.7624822806741219\n",
      "0.7625196850393701\n",
      "0.7625570776255708\n",
      "0.7624370277078085\n",
      "0.7624744215331339\n",
      "0.7625118035882908\n",
      "0.7625491738788356\n",
      "0.7624292007551919\n",
      "0.76230926537675\n",
      "0.7623466498899025\n",
      "0.7622267652146564\n",
      "0.7621069182389937\n",
      "0.7621443169313001\n",
      "0.7621817038667086\n",
      "0.7620619204777621\n",
      "0.7620993086109366\n",
      "0.7621366849960722\n",
      "0.7620169651272385\n",
      "0.7620543427045704\n",
      "0.7619346733668342\n",
      "0.7619720521274925\n",
      "0.7620094191522763\n",
      "0.7620467744467116\n",
      "0.7620841180163214\n",
      "0.7621214498666248\n",
      "0.7621587700031377\n",
      "0.7621960784313726\n",
      "0.7620765370138017\n",
      "0.7619570330876587\n",
      "0.761994355597366\n",
      "0.7618749020222605\n",
      "0.7617554858934169\n",
      "0.7617928224416236\n",
      "0.761830147289251\n",
      "0.7618674604417985\n",
      "0.7619047619047619\n",
      "0.7619420516836335\n",
      "0.7619793297839023\n",
      "0.7620165962110537\n",
      "0.7618973074514716\n",
      "0.7617780560338081\n",
      "0.7616588419405321\n",
      "0.7616961351901111\n",
      "0.7617334167709637\n",
      "0.7617706866885656\n",
      "0.7618079449483891\n",
      "0.761845191555903\n",
      "0.7618824265165729\n",
      "0.7619196498358606\n",
      "0.7619568615192247\n",
      "0.7619940615721207\n",
      "0.76203125\n",
      "0.7619122012185596\n",
      "0.7617931896282412\n",
      "0.7616742152116196\n",
      "0.7617114303560275\n",
      "0.7617486338797814\n",
      "0.7617858257883234\n",
      "0.7618230060870922\n",
      "0.7618601747815231\n",
      "0.7618973318770479\n",
      "0.7619344773790951\n",
      "0.76197161129309\n",
      "0.7618527760449157\n",
      "0.7618899111180415\n",
      "0.7619270346117867\n",
      "0.7619641465315666\n",
      "0.762001246882793\n",
      "0.7620383356708742\n",
      "0.7620754129012153\n",
      "0.7621124785792179\n",
      "0.7621495327102804\n",
      "0.7621865752997975\n",
      "0.762223606353161\n",
      "0.762260625875759\n",
      "0.7622976338729763\n",
      "0.7621789883268483\n",
      "0.7622159975101152\n",
      "0.7622529951765987\n",
      "0.7622899813316739\n",
      "0.7623269559807124\n",
      "0.7623639191290824\n",
      "0.7622453739698336\n",
      "0.7622823383084577\n",
      "0.7623192911549821\n",
      "0.7623562325147654\n",
      "0.7623931623931623\n",
      "0.7622747047855811\n",
      "0.7623116358552121\n",
      "0.762193227710469\n",
      "0.7622301599627271\n",
      "0.7622670807453417\n",
      "0.7623039900636547\n",
      "0.7621856566283762\n",
      "0.7620673599255006\n",
      "0.7621042830540037\n",
      "0.7621411947245927\n",
      "0.7620229599751784\n",
      "0.7620598728090585\n",
      "0.7619416873449132\n",
      "0.7619786013335401\n",
      "0.762015503875969\n",
      "0.7620523949775229\n",
      "0.7619342839429635\n",
      "0.7618162095149543\n",
      "0.7616981716764797\n",
      "0.7615801704105345\n",
      "0.7616171003717472\n",
      "0.7616540188942234\n",
      "0.7616909259832766\n",
      "0.7617278216442174\n",
      "0.7617647058823529\n",
      "0.7618015787029871\n",
      "0.7618384401114207\n",
      "0.7617205632059415\n",
      "0.7617574257425742\n",
      "0.7616395978344934\n",
      "0.7615218063717909\n",
      "0.761558682542137\n",
      "0.761595547309833\n",
      "0.7614778172824239\n",
      "0.761514683153014\n",
      "0.7615515376294236\n",
      "0.7615883807169345\n",
      "0.7616252124208249\n",
      "0.7615075687364844\n",
      "0.7615444015444015\n",
      "0.7615812229771464\n",
      "0.7616180330399877\n",
      "0.7616548317381908\n",
      "0.7616916190770181\n",
      "0.7617283950617284\n",
      "0.7617651596975775\n",
      "0.7618019129898179\n",
      "0.761838654943699\n",
      "0.7618753855644664\n",
      "0.7619121048573632\n",
      "0.7619488128276287\n",
      "0.7619855094804995\n",
      "0.7620221948212084\n",
      "0.7620588688549853\n",
      "0.762095531587057\n",
      "0.7621321830226467\n",
      "0.7621688231669748\n",
      "0.762205452025258\n",
      "0.7622420696027102\n",
      "0.7621247113163973\n",
      "0.7621613300492611\n",
      "0.7621979375096198\n",
      "0.7622345337026778\n",
      "0.762271118633636\n",
      "0.7621538461538462\n",
      "0.7621904322411936\n",
      "0.7620732082436174\n",
      "0.7621097954790097\n",
      "0.7621463714637147\n",
      "0.7621829362029209\n",
      "0.7622194897018137\n",
      "0.7622560319655756\n",
      "0.7622925629993854\n",
      "0.7623290828084192\n",
      "0.7623655913978494\n",
      "0.762402088772846\n",
      "0.7622850122850123\n",
      "0.7621679717488101\n",
      "0.7622044826527479\n",
      "0.7622409823484267\n",
      "0.7622774708410067\n",
      "0.7623139481356452\n",
      "0.7623504142374962\n",
      "0.7623868691517104\n",
      "0.7624233128834356\n",
      "0.7624597454378162\n",
      "0.7624961668199939\n",
      "0.7625325770351066\n",
      "0.7624156958920908\n",
      "0.7622988505747127\n",
      "0.7623352742874655\n",
      "0.762371686839283\n",
      "0.7624080882352942\n",
      "0.7622913156685557\n",
      "0.7623277182235835\n",
      "0.7623641096309907\n",
      "0.7624004898958971\n",
      "0.7624368590234195\n",
      "0.7624732170186715\n",
      "0.7625095638867636\n",
      "0.762545899632803\n",
      "0.7624292488909286\n",
      "0.7623126338329764\n",
      "0.7623489830249274\n",
      "0.7623853211009174\n",
      "0.762421648066045\n",
      "0.7624579639254051\n",
      "0.7624942686840899\n",
      "0.7625305623471883\n",
      "0.7625668449197861\n",
      "0.7624503513596089\n",
      "0.7623338933862838\n",
      "0.7622174709835065\n",
      "0.7621010841349825\n",
      "0.7621374045801527\n",
      "0.7621737139368036\n",
      "0.7622100122100122\n",
      "0.7622462994048528\n",
      "0.7622825755263961\n",
      "0.7623188405797101\n",
      "0.7623550945698597\n",
      "0.7623913375019064\n",
      "0.7624275693809088\n",
      "0.7624637902119226\n",
      "0.7625\n",
      "0.7625361987501905\n",
      "0.7625723864675403\n",
      "0.7626085631570928\n",
      "0.7626447288238879\n",
      "0.7626808834729627\n",
      "0.7627170271093512\n",
      "0.7627531597380843\n",
      "0.76278928136419\n",
      "0.762825391992693\n",
      "0.7627092846270929\n",
      "0.7627453964388982\n",
      "0.7627814972611078\n",
      "0.7628175870987373\n",
      "0.7628536659567995\n",
      "0.7628897338403042\n",
      "0.7627737226277372\n",
      "0.7628097916983427\n",
      "0.7628458498023716\n",
      "0.7627298981608147\n",
      "0.7627659574468085\n",
      "0.7626500531834067\n",
      "0.7626861136432695\n",
      "0.7627221631475012\n",
      "0.7627582017010935\n",
      "0.7626423690205011\n",
      "0.7626784087458245\n",
      "0.7627144375284651\n",
      "0.7627504553734062\n",
      "0.7627864622856275\n",
      "0.7628224582701062\n",
      "0.7628584433318161\n",
      "0.7628944174757282\n",
      "0.7629303807068103\n",
      "0.7629663330300273\n",
      "0.7630022744503412\n",
      "0.7628865979381443\n",
      "0.7627709564953767\n",
      "0.7628069111852076\n",
      "0.762842854978027\n",
      "0.7628787878787879\n",
      "0.7629147098924406\n",
      "0.7629506210239322\n",
      "0.7629865212782069\n",
      "0.7630224106602059\n",
      "0.7629068887206661\n",
      "0.7629427792915532\n",
      "0.7629786589980324\n",
      "0.7630145278450363\n",
      "0.7630503858374943\n",
      "0.7630862329803328\n",
      "0.7631220692784753\n",
      "0.7631578947368421\n",
      "0.7630424920610918\n",
      "0.7630783187178711\n",
      "0.762962962962963\n",
      "0.7629987908101572\n",
      "0.763034607828321\n",
      "0.7630704140223632\n",
      "0.7631062093971899\n",
      "0.7629909365558912\n",
      "0.763026733121885\n",
      "0.7629115070975536\n",
      "0.7627963158689416\n",
      "0.7628321256038647\n",
      "0.7627169811320754\n",
      "0.7627527920313915\n",
      "0.7627885921231327\n",
      "0.7628243814121907\n",
      "0.7628601599034546\n",
      "0.7628959276018099\n",
      "0.7627808776956718\n",
      "0.762816646562123\n",
      "0.7628524046434494\n",
      "0.762737413325294\n",
      "0.7626224566691786\n",
      "0.7625075346594334\n",
      "0.76254331776405\n",
      "0.7624284423018982\n",
      "0.7624642265401416\n",
      "0.7625\n",
      "0.7623851829543743\n",
      "0.7624209575429087\n",
      "0.762456721360831\n",
      "0.7624924744130042\n",
      "0.7625282167042889\n",
      "0.7625639482395425\n",
      "0.7625996690236196\n",
      "0.7626353790613718\n",
      "0.7626710783576478\n",
      "0.7627067669172932\n",
      "0.7625920914148249\n",
      "0.7624774503908599\n",
      "0.7625131519615211\n",
      "0.7625488428013225\n",
      "0.7625845229151015\n",
      "0.7624699519230769\n",
      "0.7625056331680937\n",
      "0.7623911084409732\n",
      "0.7624267908094309\n",
      "0.7624624624624624\n",
      "0.762347995796427\n",
      "0.7622335634944462\n",
      "0.7622692480864476\n",
      "0.7623049219687875\n",
      "0.7623405851462866\n",
      "0.7623762376237624\n",
      "0.7624118794060297\n",
      "0.7624475104979004\n",
      "0.7624831309041835\n",
      "0.7625187406296852\n",
      "0.7625543396792085\n",
      "0.762589928057554\n",
      "0.7626255057695189\n",
      "0.7626610728198981\n",
      "0.7626966292134831\n",
      "0.762732174955063\n",
      "0.7626179421896061\n",
      "0.7626534890685834\n",
      "0.762689025303189\n",
      "0.7625748502994012\n",
      "0.762610387666517\n",
      "0.7626459143968871\n",
      "0.7626814304952866\n",
      "0.7625673249551167\n",
      "0.762602842183994\n",
      "0.7624887825306611\n",
      "0.7625243008823089\n",
      "0.7624102870813397\n",
      "0.7624458065480639\n",
      "0.7624813153961136\n",
      "0.7625168136302496\n",
      "0.7625523012552301\n",
      "0.7624383684446436\n",
      "0.7624738571855393\n",
      "0.7625093353248693\n",
      "0.7623954599761051\n",
      "0.7622816186352098\n",
      "0.7623171095849507\n",
      "0.7623525899387968\n",
      "0.7623880597014925\n",
      "0.7624235188777795\n",
      "0.7624589674723963\n",
      "0.7624944054900791\n",
      "0.7625298329355609\n",
      "0.762565249813572\n",
      "0.7626006561288399\n",
      "0.7626360518860892\n",
      "0.7626714370900417\n",
      "0.7627068117454167\n",
      "0.7627421758569299\n",
      "0.7627775294292952\n",
      "0.7628128724672228\n",
      "0.7628482049754208\n",
      "0.762883526958594\n",
      "0.7627699180938198\n",
      "0.7628052412150089\n",
      "0.762840553818669\n",
      "0.7628758559094969\n",
      "0.7627623158208067\n",
      "0.7626488095238095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7625353370034221\n",
      "0.7625706634930081\n",
      "0.7626059794734493\n",
      "0.7626412849494348\n",
      "0.7625278810408922\n",
      "0.7625631876300922\n",
      "0.7624498290471236\n",
      "0.7624851367419738\n",
      "0.7625204339426364\n",
      "0.762407132243685\n",
      "0.7624424305452384\n",
      "0.76232917409388\n",
      "0.7623644734887866\n",
      "0.7623997623997624\n",
      "0.762286562731997\n",
      "0.7623218527315915\n",
      "0.7623571322547128\n",
      "0.7622439893143366\n",
      "0.7621308799525152\n",
      "0.7620178041543026\n",
      "0.7620531078475004\n",
      "0.7620884010679324\n",
      "0.7621236838202581\n",
      "0.7620106761565836\n",
      "0.7620459599703484\n",
      "0.7620812333234509\n",
      "0.7619682821994961\n",
      "0.76185536455246\n",
      "0.7618906504667358\n",
      "0.7619259259259259\n",
      "0.761813064731151\n",
      "0.7618483412322274\n",
      "0.7618836072856509\n",
      "0.7619188628960616\n",
      "0.7618060695780903\n",
      "0.7618413262285376\n",
      "0.761876572443392\n",
      "0.7619118082272862\n",
      "0.7619470335848498\n",
      "0.7619822485207101\n",
      "0.7620174530394912\n",
      "0.7620526471458149\n",
      "0.7619399674700577\n",
      "0.7619751626256653\n",
      "0.762010347376201\n",
      "0.7620455217262785\n",
      "0.7620806856805084\n",
      "0.7621158392434988\n",
      "0.7620032501107993\n",
      "0.7620384047267356\n",
      "0.7620735489587949\n",
      "0.7621086828115771\n",
      "0.761996161228407\n",
      "0.7620312961322705\n",
      "0.7620664206642066\n",
      "0.7621015348288076\n",
      "0.7621366386306625\n",
      "0.7621717320743582\n",
      "0.7622068151644785\n",
      "0.7622418879056048\n",
      "0.7622769503023153\n",
      "0.7623120023591861\n",
      "0.7623470440807902\n",
      "0.7623820754716981\n",
      "0.7624170965364775\n",
      "0.7624521072796935\n",
      "0.7624871077059083\n",
      "0.7625220978196818\n",
      "0.7624097805273237\n",
      "0.7624447717231222\n",
      "0.7623324988955971\n",
      "0.7623674911660777\n",
      "0.7624024731341087\n",
      "0.762437444804239\n",
      "0.7624724061810154\n",
      "0.7625073572689818\n",
      "0.7625422980726791\n",
      "0.7625772285966461\n",
      "0.7624650683924107\n",
      "0.7623529411764706\n",
      "0.7622408469342744\n",
      "0.7622758012349309\n",
      "0.7621637512861973\n",
      "0.762198706643151\n",
      "0.76208670095518\n",
      "0.7621216573611519\n",
      "0.7621566034964008\n",
      "0.7620446533490012\n",
      "0.762079600528712\n",
      "0.762114537444934\n",
      "0.7621494641021876\n",
      "0.7620375807398708\n",
      "0.7619257302216351\n",
      "0.761960669210449\n",
      "0.761995597945708\n",
      "0.7620305164319249\n",
      "0.76206542467361\n",
      "0.7621003226752714\n",
      "0.7621352104414137\n",
      "0.7621700879765396\n",
      "0.7620583492156575\n",
      "0.7620932277924363\n",
      "0.7621280961453906\n",
      "0.7621629542790153\n",
      "0.7621978021978022\n",
      "0.7622326399062408\n",
      "0.7621209901860261\n",
      "0.7621558289396603\n",
      "0.7621906574901157\n",
      "0.7620790629575402\n",
      "0.7621138925486751\n",
      "0.762148711943794\n",
      "0.7621835211473731\n",
      "0.7622183201638865\n",
      "0.7622531089978054\n",
      "0.7622878876535986\n",
      "0.762322656135732\n",
      "0.7622111728575607\n",
      "0.7622459423892382\n",
      "0.7621345029239766\n",
      "0.7620230960385909\n",
      "0.7620578778135049\n",
      "0.7620926494227678\n",
      "0.7619812974868498\n",
      "0.7620160701241783\n",
      "0.7620508326029798\n",
      "0.7619395355630203\n",
      "0.7619742990654206\n",
      "0.7618630457001022\n",
      "0.7618978102189781\n",
      "0.7619325645891111\n",
      "0.7619673088149446\n",
      "0.7618561214066832\n",
      "0.7618908666472133\n",
      "0.761925601750547\n",
      "0.7618144690781797\n",
      "0.7618492051917748\n",
      "0.7618839311752698\n",
      "0.7619186470330952\n",
      "0.7619533527696793\n",
      "0.7618422970412476\n",
      "0.7618770037889828\n",
      "0.7619117004225557\n",
      "0.7619463869463869\n",
      "0.7619810633648944\n",
      "0.7620157296824934\n",
      "0.7619047619047619\n",
      "0.7619394292370414\n",
      "0.7619740864754695\n",
      "0.7620087336244541\n",
      "0.76189783146558\n",
      "0.7617869615832363\n",
      "0.761821620835152\n",
      "0.7617107942973523\n",
      "0.7616\n",
      "0.7616346713205352\n",
      "0.7616693325578013\n",
      "0.7617039837161965\n",
      "0.7615932548335513\n",
      "0.7616279069767442\n",
      "0.7616625490481035\n",
      "0.7616971810520198\n",
      "0.761731802992881\n",
      "0.7617664148750727\n",
      "0.7618010167029775\n",
      "0.7618356084809759\n",
      "0.7618701902134456\n",
      "0.7619047619047619\n",
      "0.7619393235592974\n",
      "0.7619738751814223\n",
      "0.7620084167755042\n",
      "0.7618978525827046\n",
      "0.7619323951835195\n",
      "0.7618218740934145\n",
      "0.7617113850616388\n",
      "0.761600928074246\n",
      "0.76163549369291\n",
      "0.7616700492896492\n",
      "0.7617045948688216\n",
      "0.7617391304347826\n",
      "0.7617736559918852\n",
      "0.7618081715444799\n",
      "0.7618426770969143\n",
      "0.7618771726535342\n",
      "0.7619116582186821\n",
      "0.7619461337966985\n",
      "0.7619805993919212\n",
      "0.7618702953097858\n",
      "0.7619047619047619\n",
      "0.7619392185238785\n",
      "0.7619736651714658\n",
      "0.7620081018518519\n",
      "0.7618978735715319\n",
      "0.7619323112525311\n",
      "0.7619667389732465\n",
      "0.7620011567379988\n",
      "0.762035564551106\n",
      "0.7620699624168835\n",
      "0.7619598207833502\n",
      "0.7619942196531792\n",
      "0.7620286085825748\n",
      "0.7619185206587692\n",
      "0.7619529105878954\n",
      "0.761842865395725\n",
      "0.7618772563176895\n",
      "0.7617672538261623\n",
      "0.761801645734084\n",
      "0.7618360277136259\n",
      "0.7617260787992496\n",
      "0.7616161616161616\n",
      "0.7615062761506276\n",
      "0.7613964223889209\n",
      "0.761286600317323\n",
      "0.7613210268243438\n",
      "0.761211247296323\n",
      "0.7611014994232987\n",
      "0.7609917831915813\n",
      "0.7608820985874892\n",
      "0.7607724455973484\n",
      "0.760806916426513\n",
      "0.7608413773231523\n",
      "0.7608758282915586\n",
      "0.7609102693360219\n",
      "0.7609447004608295\n",
      "0.7609791216702664\n",
      "0.7610135329686151\n",
      "0.7610479343601555\n",
      "0.7610823258491652\n",
      "0.7609728018419917\n",
      "0.760863309352518\n",
      "0.7607538483671414\n",
      "0.7607882623705409\n",
      "0.7608226664749029\n",
      "0.7608570606844981\n",
      "0.7608914450035945\n",
      "0.7609258194364578\n",
      "0.7609601839873509\n",
      "0.7609945386605347\n",
      "0.7610288834602673\n",
      "0.7610632183908046\n",
      "0.7609538859359287\n",
      "0.7608445848893995\n",
      "0.7607353152376849\n",
      "0.7607696726019529\n",
      "0.7608040201005025\n",
      "0.7608383577375826\n",
      "0.7608726855174394\n",
      "0.7607634902411022\n",
      "0.7606543263021954\n",
      "0.7606886657101866\n",
      "0.7605795438244154\n",
      "0.76061388410786\n",
      "0.7606482145418041\n",
      "0.7605391453971896\n",
      "0.760573476702509\n",
      "0.7604644495412844\n",
      "0.7604987817113372\n",
      "0.7603897965032961\n",
      "0.7604241295314514\n",
      "0.760458452722063\n",
      "0.7604927660793582\n",
      "0.7605270696075623\n",
      "0.7605613633108979\n",
      "0.7605956471935853\n",
      "0.7606299212598425\n",
      "0.7606641855138849\n",
      "0.7606984399599256\n",
      "0.7607326846021751\n",
      "0.760623837458864\n",
      "0.7606580829756795\n",
      "0.7606923186954656\n",
      "0.7607265446224256\n",
      "0.7607607607607607\n",
      "0.7607949671146698\n",
      "0.7608291636883489\n",
      "0.760863350485992\n",
      "0.7607546091181935\n",
      "0.7606458988282366\n",
      "0.7606800971567367\n",
      "0.7607142857142857\n",
      "0.7607484645050707\n",
      "0.7607826335332762\n",
      "0.7608167928030843\n",
      "0.7607081667618504\n",
      "0.7607423269093505\n",
      "0.760633742506423\n",
      "0.7606679035250464\n",
      "0.7607020547945206\n",
      "0.7607361963190185\n",
      "0.7607703281027104\n",
      "0.7608044501497646\n",
      "0.7608385624643468\n",
      "0.7608726650506202\n",
      "0.760906757912746\n",
      "0.7609408410548824\n",
      "0.7609749144811858\n",
      "0.7610089781958101\n",
      "0.7609005414648048\n",
      "0.7609346060692407\n",
      "0.760968660968661\n",
      "0.7610027061672127\n",
      "0.7610367416690401\n",
      "0.7610707674782856\n",
      "0.7609624145785877\n",
      "0.7609964412811387\n",
      "0.7610304582977512\n",
      "0.7610644656325601\n",
      "0.7609561752988048\n",
      "0.7609901835253948\n",
      "0.7610241820768137\n",
      "0.7610581709571896\n",
      "0.7610921501706485\n",
      "0.7611261197213138\n",
      "0.7611600796133068\n",
      "0.7611940298507462\n",
      "0.761085844229676\n",
      "0.7611197953673441\n",
      "0.7611537368570617\n",
      "0.7611876687029407\n",
      "0.7610795454545455\n",
      "0.7611134781991195\n",
      "0.761147401306447\n",
      "0.7611813147806332\n",
      "0.7612152186257808\n",
      "0.7612491128459901\n",
      "0.7612829974453591\n",
      "0.7613168724279835\n",
      "0.7613507377979569\n",
      "0.7613845935593702\n",
      "0.7612765957446809\n",
      "0.7613104524180967\n",
      "0.7613442994895065\n",
      "0.7613781369629945\n",
      "0.7614119648426425\n",
      "0.7613040396881644\n",
      "0.7611961451247166\n",
      "0.7612299844126399\n",
      "0.7612638141116463\n",
      "0.7611559711007225\n",
      "0.7611898016997167\n",
      "0.761081999716754\n",
      "0.7611158312092892\n",
      "0.7610080702251169\n",
      "0.7610419026047565\n",
      "0.7610757254069356\n",
      "0.7611095386357204\n",
      "0.7611433422951748\n",
      "0.7611771363893605\n",
      "0.7610694581977648\n",
      "0.7611032531824611\n",
      "0.7611370386084005\n",
      "0.761170814479638\n",
      "0.7612045808002262\n",
      "0.7610969748374329\n",
      "0.76113074204947\n",
      "0.7610231769361221\n",
      "0.7610569450332062\n",
      "0.7610907035885843\n",
      "0.7611244526063004\n",
      "0.7611581920903955\n",
      "0.7611919220449089\n",
      "0.7612256424738775\n",
      "0.7612593533813355\n",
      "0.7611518915866742\n",
      "0.761044460127029\n",
      "0.7610781823313576\n",
      "0.7609707915902356\n",
      "0.7610045146726863\n",
      "0.7610382282409367\n",
      "0.7610719322990127\n",
      "0.7611056268509379\n",
      "0.7611393119007333\n",
      "0.7611729874524179\n",
      "0.7612066535100085\n",
      "0.7612403100775194\n",
      "0.7612739571589628\n",
      "0.7611666901507679\n",
      "0.761200338123415\n",
      "0.7612339766164249\n",
      "0.7611267605633802\n",
      "0.7611603999436699\n",
      "0.7611940298507462\n",
      "0.7612276502886104\n",
      "0.7612612612612613\n",
      "0.7612948627726953\n",
      "0.7613284548269068\n",
      "0.761362037427888\n",
      "0.7613956105796286\n",
      "0.7614291742861162\n",
      "0.7613220815752462\n",
      "0.7613556461819716\n",
      "0.7613892013498312\n",
      "0.7614227470828061\n",
      "0.761315715490582\n",
      "0.7613492621222768\n",
      "0.7613827993254637\n",
      "0.7614163271041169\n",
      "0.7614498454622085\n",
      "0.7614833544037084\n",
      "0.761376404494382\n",
      "0.7614099143378739\n",
      "0.7614434147711318\n",
      "0.7614769057981188\n",
      "0.7615103874227962\n",
      "0.7614035087719299\n",
      "0.7614369912994667\n",
      "0.7614704644310369\n",
      "0.7615039281705949\n",
      "0.7615373825220929\n",
      "0.761570827489481\n",
      "0.7616042630767074\n",
      "0.761497476163769\n",
      "0.76153091265947\n",
      "0.7615643397813289\n",
      "0.7615977575332866\n",
      "0.7616311659192825\n",
      "0.7616645649432535\n",
      "0.7616979546091343\n",
      "0.7617313349208573\n",
      "0.7617647058823529\n",
      "0.7617980674975494\n",
      "0.7618314197703725\n",
      "0.7618647627047459\n",
      "0.7618980963045913\n",
      "0.7619314205738279\n",
      "0.7619647355163728\n",
      "0.761998041136141\n",
      "0.7618914381645215\n",
      "0.7619247447195412\n",
      "0.7618181818181818\n",
      "0.7617116487204587\n",
      "0.761744966442953\n",
      "0.7617782748497134\n",
      "0.761671792004473\n",
      "0.7615653389238295\n",
      "0.7615986584684181\n",
      "0.7614922453541987\n",
      "0.7615255658005029\n",
      "0.7615588769381199\n",
      "0.7615921787709498\n",
      "0.761485826001955\n",
      "0.7615191287349903\n",
      "0.761552422169482\n",
      "0.7615857063093244\n",
      "0.761618981158409\n",
      "0.7616522467206251\n",
      "0.7616855029998605\n",
      "0.7615792410714286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7616124982563817\n",
      "0.7616457461645746\n",
      "0.7615395342351137\n",
      "0.7614333519241495\n",
      "0.7614666109019936\n",
      "0.7614998606077502\n",
      "0.7615331010452961\n",
      "0.7615663322185061\n",
      "0.761460220147694\n",
      "0.7614934522151017\n",
      "0.7615266750243767\n",
      "0.7615598885793872\n",
      "0.7615930928839995\n",
      "0.7616262879420774\n",
      "0.7615202561603787\n",
      "0.7614142538975501\n",
      "0.7614474599860821\n",
      "0.7613414973559699\n",
      "0.7613747043272575\n",
      "0.7612687813021702\n",
      "0.7613019891500904\n",
      "0.7613351877607789\n",
      "0.7612293144208038\n",
      "0.7612625139043382\n",
      "0.7612957041568191\n",
      "0.7613288851820962\n",
      "0.7612230715774844\n",
      "0.7611172873818788\n",
      "0.7611504793664027\n",
      "0.761183662128369\n",
      "0.761216835671621\n",
      "0.76125\n",
      "0.7612831551173448\n",
      "0.7611774507081366\n",
      "0.7610717756490352\n",
      "0.7611049416990561\n",
      "0.7611380985426787\n",
      "0.7611712461837358\n",
      "0.761204384626058\n",
      "0.7612375138734739\n",
      "0.76127063392981\n",
      "0.7613037447988904\n",
      "0.7613368464845375\n",
      "0.7613699389905713\n",
      "0.7614030223208097\n",
      "0.761297477127807\n",
      "0.7613305613305613\n",
      "0.7613636363636364\n",
      "0.7612581405015935\n",
      "0.7612912164034359\n",
      "0.7613242831417094\n",
      "0.7613573407202217\n",
      "0.761390389142778\n",
      "0.7614234284131819\n",
      "0.7613180119064101\n",
      "0.7613510520487264\n",
      "0.7613840830449827\n",
      "0.761417104898976\n",
      "0.7614501176145012\n",
      "0.7613447703375761\n",
      "0.7613777839258542\n",
      "0.7614107883817427\n",
      "0.7613054902503111\n",
      "0.7613384955752213\n",
      "0.7613714917738145\n",
      "0.7614044788498756\n",
      "0.7614374568071873\n",
      "0.7614704256495302\n",
      "0.7615033853806826\n",
      "0.7615363360044212\n",
      "0.7614311368973615\n",
      "0.76146408839779\n",
      "0.7614970307968513\n",
      "0.7615299640983154\n",
      "0.7614248239679691\n",
      "0.7614577581446714\n",
      "0.7614906832298136\n",
      "0.7615235992271598\n",
      "0.7614185180074513\n",
      "0.7614514348785872\n",
      "0.7614843426679542\n",
      "0.7615172413793103\n",
      "0.7615501310164116\n",
      "0.7615830115830116\n",
      "0.7616158830828622\n",
      "0.7616487455197133\n",
      "0.7616815988973122\n",
      "0.7615766262403528\n",
      "0.7616094805015847\n",
      "0.7616423257095619\n",
      "0.7616751618680259\n",
      "0.7617079889807162\n",
      "0.7617408070513704\n",
      "0.7617736160837235\n",
      "0.761806416081509\n",
      "0.7618392070484582\n",
      "0.7617343427391604\n",
      "0.7617671345995045\n",
      "0.7616623090683914\n",
      "0.7616951018161805\n",
      "0.7615903150364561\n",
      "0.7616231086657497\n",
      "0.7616558932746528\n",
      "0.7615511551155115\n",
      "0.7615839406022274\n",
      "0.761616717074512\n",
      "0.7616494845360825\n",
      "0.7616822429906542\n",
      "0.7615775731757592\n",
      "0.7616103325089311\n",
      "0.7616430828410496\n",
      "0.7615384615384615\n",
      "0.761571212745502\n",
      "0.7616039549574293\n",
      "0.7616366881779486\n",
      "0.7615321252059308\n",
      "0.7615648592999313\n",
      "0.7615975844084546\n",
      "0.7616303005351996\n",
      "0.7616630076838639\n",
      "0.7616957058581424\n",
      "0.7617283950617284\n",
      "0.7617610752983129\n",
      "0.7616566099835436\n",
      "0.7616892911010558\n",
      "0.7617219632574719\n",
      "0.7617546264564771\n",
      "0.7616502192982456\n",
      "0.7616828833767302\n",
      "0.7617155385036997\n",
      "0.7616111796136457\n",
      "0.7616438356164383\n",
      "0.7616764826736063\n",
      "0.761709120788825\n",
      "0.7616048199370122\n",
      "0.7616374589266156\n",
      "0.7616700889801505\n",
      "0.7617027101012867\n",
      "0.7615984672232107\n",
      "0.7616310892172962\n",
      "0.7616637022848542\n",
      "0.7616963064295486\n",
      "0.7615921214608125\n",
      "0.7614879649890591\n",
      "0.7615205797894161\n",
      "0.7615531856713152\n",
      "0.7614490772385509\n",
      "0.7614816839803171\n",
      "0.7615142818094848\n",
      "0.7615468707297076\n",
      "0.7615794507446373\n",
      "0.7616120218579235\n",
      "0.761644584073214\n",
      "0.7616771373941547\n",
      "0.7617096818243889\n",
      "0.7617422173675588\n",
      "0.7617747440273037\n",
      "0.7618072618072618\n",
      "0.7618397707110687\n",
      "0.7618722707423581\n",
      "0.7619047619047619\n",
      "0.7618008185538881\n",
      "0.7618333105988269\n",
      "0.7617294053464266\n",
      "0.7616255284331106\n",
      "0.7616580310880829\n",
      "0.7616905248807089\n",
      "0.7617230098146128\n",
      "0.7617554858934169\n",
      "0.7617879531207413\n",
      "0.7618204115002044\n",
      "0.7618528610354224\n",
      "0.7618853017300096\n",
      "0.7619177335875783\n",
      "0.761950156611739\n",
      "0.7619825708061002\n",
      "0.7620149761742682\n",
      "0.7620473727198476\n",
      "0.7620797604464408\n",
      "0.7621121393576483\n",
      "0.762144509457069\n",
      "0.7621768707482993\n",
      "0.7620731873214529\n",
      "0.7621055495103374\n",
      "0.7621379028967769\n",
      "0.7621702474843622\n",
      "0.7622025832766826\n",
      "0.7620989668297988\n",
      "0.7621313035204567\n",
      "0.7620277249252514\n",
      "0.762060062508493\n",
      "0.7620923913043478\n",
      "0.7621247113163973\n",
      "0.7621570225482206\n",
      "0.7621893250033953\n",
      "0.762221618685497\n",
      "0.7621181262729124\n",
      "0.7621504208525658\n",
      "0.7621827066648568\n",
      "0.7620792616720955\n",
      "0.7621115483783417\n",
      "0.7621438263229308\n",
      "0.7620404287070953\n",
      "0.762072707542051\n",
      "0.7619693476196935\n",
      "0.7618660157309466\n",
      "0.7617627118644068\n",
      "0.761795010845987\n",
      "0.761827301070896\n",
      "0.7617240444564922\n",
      "0.7617563355468221\n",
      "0.7617886178861789\n",
      "0.7618208914781195\n",
      "0.7618531563261989\n",
      "0.7618854124339699\n",
      "0.7619176598049837\n",
      "0.7619498984427895\n",
      "0.7619821283509342\n",
      "0.7620143495329633\n",
      "0.7620465619924202\n",
      "0.7620787657328462\n",
      "0.7621109607577807\n",
      "0.7621431470707617\n",
      "0.7620400432900433\n",
      "0.7619369674015961\n",
      "0.7618339193941033\n",
      "0.761866125760649\n",
      "0.7618983234180638\n",
      "0.7619305123698796\n",
      "0.7618275209516085\n",
      "0.7618597107717259\n",
      "0.7617567567567568\n",
      "0.7617889474395352\n",
      "0.7618211294244799\n",
      "0.7618533027151155\n",
      "0.7618854673149649\n",
      "0.761917623227549\n",
      "0.7619497704563867\n",
      "0.7618469015795869\n",
      "0.7618790496760259\n",
      "0.7619111890943447\n",
      "0.761808367071525\n",
      "0.7618405073539334\n",
      "0.7618726389638424\n",
      "0.7619047619047619\n",
      "0.7619368761801997\n",
      "0.7619689817936615\n",
      "0.7620010787486515\n",
      "0.762033167048672\n",
      "0.762065246697223\n",
      "0.7619625286426742\n",
      "0.7618598382749326\n",
      "0.7618919283115483\n",
      "0.7619240097008892\n",
      "0.7619560824464502\n",
      "0.7619881465517241\n",
      "0.762020202020202\n",
      "0.762052248855373\n",
      "0.7620842870607244\n",
      "0.7619816908992999\n",
      "0.7620137299771167\n",
      "0.7620457604306864\n",
      "0.7620777822634908\n",
      "0.7621097954790097\n",
      "0.7620072648997713\n",
      "0.7620392789884315\n",
      "0.7620712844653665\n",
      "0.7621032813340506\n",
      "0.7621352695979562\n",
      "0.7621672492605539\n",
      "0.7620647936550612\n",
      "0.7619623655913978\n",
      "0.761994355597366\n",
      "0.7620263370061812\n",
      "0.7620583098213086\n",
      "0.7620902740462118\n",
      "0.762122229684352\n",
      "0.7620198764437282\n",
      "0.7619175506915536\n",
      "0.7619495166487648\n",
      "0.7619814740233588\n",
      "0.7620134228187919\n",
      "0.7620453630385183\n",
      "0.7620772946859904\n",
      "0.7619750436066014\n",
      "0.762006976120204\n",
      "0.7620389000670691\n",
      "0.7620708154506438\n",
      "0.762102722274373\n",
      "0.7621346205417002\n",
      "0.7621665102560665\n",
      "0.7621983914209115\n",
      "0.7620962337488273\n",
      "0.7621281157866524\n",
      "0.7621599892804503\n",
      "0.7621918542336549\n",
      "0.7622237106496986\n",
      "0.7622555585320118\n",
      "0.762287397884023\n",
      "0.762319228709159\n",
      "0.7623510510108448\n",
      "0.7623828647925034\n",
      "0.7624146700575559\n",
      "0.7624464668094219\n",
      "0.7623444399839422\n",
      "0.7623762376237624\n",
      "0.7624080267558528\n",
      "0.7624398073836276\n",
      "0.7624715795104988\n",
      "0.762503343139877\n",
      "0.7624013905602354\n",
      "0.7624331550802139\n",
      "0.7624649111081406\n",
      "0.7624966586474204\n",
      "0.7625283977014566\n",
      "0.7625601282736505\n",
      "0.7625918503674015\n",
      "0.7624899812984237\n",
      "0.7623881394416989\n",
      "0.7624198717948718\n",
      "0.7624515956736547\n",
      "0.762483311081442\n",
      "0.7623815244960619\n",
      "0.7624132407901761\n",
      "0.7624449486187108\n",
      "0.762343207899653\n",
      "0.762374916611074\n",
      "0.7624066168623266\n",
      "0.7623049219687875\n",
      "0.7622032542011203\n",
      "0.762234964661955\n",
      "0.7622666666666666\n",
      "0.7622983602186375\n",
      "0.7623300453212477\n",
      "0.7623617219778756\n",
      "0.7623933901918977\n",
      "0.7622918054630247\n",
      "0.7623234745536904\n",
      "0.76235513520714\n",
      "0.7623867874267448\n",
      "0.7624184312158743\n",
      "0.7624500665778962\n",
      "0.7624816935161762\n",
      "0.7625133120340788\n",
      "0.762544922134966\n",
      "0.7625765238221985\n",
      "0.762608117099135\n",
      "0.7625066524747206\n",
      "0.7625382466409472\n",
      "0.7625698324022346\n",
      "0.7624684133528394\n",
      "0.7625\n",
      "0.7623986172051589\n",
      "0.7624302047327839\n",
      "0.7624617838628207\n",
      "0.7624933545986178\n",
      "0.762392026578073\n",
      "0.7624235981929312\n",
      "0.762455161418892\n",
      "0.7624867162592986\n",
      "0.7625182627174923\n",
      "0.7624169986719788\n",
      "0.7623157615190546\n",
      "0.7623473181093999\n",
      "0.7622461170848268\n",
      "0.7621449429254048\n",
      "0.7621765096217651\n",
      "0.7622080679405521\n",
      "0.7622396178851002\n",
      "0.7622711594587424\n",
      "0.7623026926648097\n",
      "0.7623342175066313\n",
      "0.7623657339875348\n",
      "0.7623972421108459\n",
      "0.7624287418798886\n",
      "0.7623276776246023\n",
      "0.7622266401590457\n",
      "0.762258150013252\n",
      "0.7622896515171591\n",
      "0.7623211446740858\n",
      "0.7623526294873493\n",
      "0.7623841059602648\n",
      "0.7622831413057873\n",
      "0.7621822033898306\n",
      "0.7622136899245333\n",
      "0.7621127879269262\n",
      "0.7621442753143614\n",
      "0.7621757543673902\n",
      "0.7620748974460765\n",
      "0.762106377348505\n",
      "0.762137848921815\n",
      "0.7621693121693122\n",
      "0.7620685094564211\n",
      "0.7620999735519703\n",
      "0.7621314293269866\n",
      "0.76216287678477\n",
      "0.7621943159286186\n",
      "0.7622257467618292\n",
      "0.762125016519096\n",
      "0.7621564482029598\n",
      "0.7621878715814506\n",
      "0.7620871862615588\n",
      "0.7619865275392946\n",
      "0.7618858954041204\n",
      "0.7619173379109996\n",
      "0.7618167414840243\n",
      "0.7617161716171618\n",
      "0.7616156282998944\n",
      "0.7615151115217105\n",
      "0.7615465822116654\n",
      "0.7614461010687426\n",
      "0.7614775725593668\n",
      "0.7615090357472629\n",
      "0.7615404906357162\n",
      "0.76157193722801\n",
      "0.7616033755274262\n",
      "0.7616348055372446\n",
      "0.7616662272607435\n",
      "0.7615658362989324\n",
      "0.7615972588297312\n",
      "0.7614969034128344\n",
      "0.761528326745718\n",
      "0.7614280068502174\n",
      "0.7614594309799789\n",
      "0.7613591465823785\n",
      "0.7613905715038188\n",
      "0.7614219881500988\n",
      "0.7613217482885729\n",
      "0.7613531657233118\n",
      "0.7612529613056067\n",
      "0.7612843795236215\n",
      "0.7611842105263158\n",
      "0.7610840678858045\n",
      "0.7611154959221258\n",
      "0.7611469156911745\n",
      "0.7611783271962125\n",
      "0.7612097304404997\n",
      "0.7612411254272943\n",
      "0.7611410542920993\n",
      "0.7610410094637224\n",
      "0.7610724142462872\n",
      "0.7611038107752957\n",
      "0.7610038102746025\n",
      "0.7610352075669995\n",
      "0.76106659661106\n",
      "0.7610979774100342\n",
      "0.7609980302035456\n",
      "0.7610294117647058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7610607850859918\n",
      "0.7610921501706485\n",
      "0.7611235070219189\n",
      "0.7611548556430446\n",
      "0.7610549796614617\n",
      "0.7609551298871687\n",
      "0.7609864882592156\n",
      "0.7608866736621196\n",
      "0.7607868852459017\n",
      "0.7606871230002623\n",
      "0.7607185000655565\n",
      "0.7607498689040377\n",
      "0.7607812295189409\n",
      "0.7608125819134993\n",
      "0.7608439260909449\n",
      "0.7607442348008385\n",
      "0.7607755797196384\n",
      "0.760806916426513\n",
      "0.7608382449246889\n",
      "0.7608695652173914\n",
      "0.7609008773078434\n",
      "0.7609321811992669\n",
      "0.7609634768948815\n",
      "0.7609947643979058\n",
      "0.7610260437115561\n",
      "0.7609264590421355\n",
      "0.7608269004317676\n",
      "0.760858189429618\n",
      "0.7608894702419883\n",
      "0.76092074287209\n",
      "0.7609520073231333\n",
      "0.7609832635983264\n",
      "0.761014511700876\n",
      "0.7609150326797386\n",
      "0.7609462815318259\n",
      "0.7608468374281233\n",
      "0.7608780870246962\n",
      "0.7609093284557095\n",
      "0.7609405617243632\n",
      "0.7609717868338558\n",
      "0.7608724043359018\n",
      "0.7607730477931575\n",
      "0.7608042825434129\n",
      "0.7608355091383812\n",
      "0.7608667275812557\n",
      "0.7608979378752284\n",
      "0.7607986428291792\n",
      "0.7608298538622129\n",
      "0.7607305936073059\n",
      "0.7607618053743804\n",
      "0.7607930089996087\n",
      "0.7606937923839332\n",
      "0.7607249967401226\n",
      "0.7607561929595827\n",
      "0.760787381045496\n",
      "0.7608185610010427\n",
      "0.7608497328294018\n",
      "0.7608808965337504\n",
      "0.7607817589576548\n",
      "0.7606826472120897\n",
      "0.7607138205028006\n",
      "0.7606147434227664\n",
      "0.7606459174371663\n",
      "0.7606770833333333\n",
      "0.7607082411144382\n",
      "0.76073939078365\n",
      "0.7607705323441364\n",
      "0.7608016657990629\n",
      "0.760832791151594\n",
      "0.760863908404892\n",
      "0.7608950175621179\n",
      "0.7609261186264308\n",
      "0.7609572116009884\n",
      "0.7609882964889467\n",
      "0.7610193732934599\n",
      "0.7610504420176807\n",
      "0.7610815026647602\n",
      "0.7611125552378477\n",
      "0.7611435997400909\n",
      "0.7611746361746362\n",
      "0.7612056645446278\n",
      "0.7612366848532086\n",
      "0.7612676971035199\n",
      "0.7612987012987013\n",
      "0.761199844176081\n",
      "0.7612308491300961\n",
      "0.7612618460340127\n",
      "0.7612928348909658\n",
      "0.7613238157040882\n",
      "0.7613547884765118\n",
      "0.7613857532113663\n",
      "0.76141670991178\n",
      "0.761317940070048\n",
      "0.7613488975356679\n",
      "0.7612501621060822\n",
      "0.7612811203319502\n",
      "0.7613120705302736\n",
      "0.7613430127041743\n",
      "0.7613739468567725\n",
      "0.7614048729911872\n",
      "0.7614357911105352\n",
      "0.7614667012179321\n",
      "0.7614976033164917\n",
      "0.7615284974093264\n",
      "0.7615593834995467\n",
      "0.7615902615902616\n",
      "0.7616211316845786\n",
      "0.7616519937856033\n",
      "0.7616828478964401\n",
      "0.7615842609370955\n",
      "0.7616151158276174\n",
      "0.7616459627329193\n",
      "0.7615474188122655\n",
      "0.7615782664941785\n",
      "0.7614797568231794\n",
      "0.7615106052767718\n",
      "0.761541445751972\n",
      "0.7615722782518748\n",
      "0.7616031027795733\n",
      "0.7615046535677352\n",
      "0.7615354788677782\n",
      "0.7614370638407857\n",
      "0.7614678899082569\n",
      "0.7613695090439276\n",
      "0.7614003358739181\n",
      "0.7614311547403771\n",
      "0.7614619656463902\n",
      "0.7613636363636364\n",
      "0.7612653324725629\n",
      "0.7612961528530855\n",
      "0.7613269652768814\n",
      "0.7613577697470315\n",
      "0.7612595173570783\n",
      "0.7612903225806451\n",
      "0.7611921042446136\n",
      "0.7612229102167183\n",
      "0.7612537082419708\n",
      "0.7612844983234459\n",
      "0.7611863313990973\n",
      "0.7612171222279526\n",
      "0.7612479051179579\n",
      "0.7611497808713586\n",
      "0.7611805645057352\n",
      "0.7610824742268041\n",
      "0.7611132586006958\n",
      "0.7611440350425148\n",
      "0.7611748035553265\n",
      "0.7612055641421948\n",
      "0.7612363168061815\n",
      "0.7612670615503476\n",
      "0.761297798377752\n",
      "0.7613285272914521\n",
      "0.7613592482945037\n",
      "0.7613899613899614\n",
      "0.7614206665808776\n",
      "0.7614513638703037\n",
      "0.761482053261289\n",
      "0.7615127347568819\n",
      "0.7614147909967846\n",
      "0.7614454732510288\n",
      "0.7614761476147615\n",
      "0.7615068140910259\n",
      "0.7615374726828641\n",
      "0.7615681233933161\n",
      "0.7614702480400977\n",
      "0.7613723978411719\n",
      "0.7614030579468072\n",
      "0.7614337101747174\n",
      "0.7614643545279384\n",
      "0.7614949910095042\n",
      "0.7615256196224477\n",
      "0.7615562403697997\n",
      "0.7615868532545899\n",
      "0.761617458279846\n",
      "0.7615197022205108\n",
      "0.7614219712525667\n",
      "0.7613242653663544\n",
      "0.7613548883756736\n",
      "0.7613855035279025\n",
      "0.7614161108260646\n",
      "0.761446710273182\n",
      "0.7614773018722749\n",
      "0.7615078856263624\n",
      "0.7614102564102564\n",
      "0.761440840917831\n",
      "0.7614714175852345\n",
      "0.7613738305779828\n",
      "0.7612762685802152\n",
      "0.7613068545803972\n",
      "0.7613374327440431\n",
      "0.7612399128986806\n",
      "0.7611424180327869\n",
      "0.7610449481367653\n",
      "0.7610755441741357\n",
      "0.760978107796697\n",
      "0.7608806963645673\n",
      "0.7609113016766927\n",
      "0.7609418991553621\n",
      "0.7609724888035828\n",
      "0.7610030706243602\n",
      "0.7610336446206984\n",
      "0.7610642107955999\n",
      "0.7610947691520655\n",
      "0.7611253196930946\n",
      "0.7611558624216852\n",
      "0.7611863973408336\n",
      "0.7610890962546337\n",
      "0.7611196319018405\n",
      "0.7611501597444089\n",
      "0.761180679785331\n",
      "0.7612111920275968\n",
      "0.7612416964741952\n",
      "0.7612721931281135\n",
      "0.7611749680715197\n",
      "0.761205465457796\n",
      "0.7611082737487231\n",
      "0.7611387718626325\n",
      "0.7610416134797039\n",
      "0.7610721123165284\n",
      "0.7611026033690659\n",
      "0.761133086640296\n",
      "0.7611635621331972\n",
      "0.7611940298507462\n",
      "0.7610969387755102\n",
      "0.7611274072184671\n",
      "0.7611578678908442\n",
      "0.7611883207956139\n",
      "0.7612187659357471\n",
      "0.7612492033142129\n",
      "0.761279632933979\n",
      "0.761310054798012\n",
      "0.7613404689092762\n",
      "0.7613708752707351\n",
      "0.7614012738853503\n",
      "0.761431664756082\n",
      "0.761334691798268\n",
      "0.7612377435375016\n",
      "0.7611408199643493\n",
      "0.7611712285168682\n",
      "0.7612016293279023\n",
      "0.7612320224004073\n",
      "0.7612624077373378\n",
      "0.7612927853416466\n",
      "0.761323155216285\n",
      "0.761353517364203\n",
      "0.761383871788349\n",
      "0.761287040569757\n",
      "0.7613173957273652\n",
      "0.761347743165925\n",
      "0.7613780828883804\n",
      "0.761281301639761\n",
      "0.7613116420945603\n",
      "0.7613419748379718\n",
      "0.7613722998729352\n",
      "0.7612755685427519\n",
      "0.7613058943089431\n",
      "0.7613362123713959\n",
      "0.7613665227330455\n",
      "0.7613968253968254\n",
      "0.7614271203656678\n",
      "0.7614574076425035\n",
      "0.7614876872302615\n",
      "0.7615179591318695\n",
      "0.7615482233502539\n",
      "0.7614515924375079\n",
      "0.7613549860441512\n",
      "0.7613852594190029\n",
      "0.7614155251141552\n",
      "0.7614457831325301\n",
      "0.7613492264773015\n",
      "0.7613794852288576\n",
      "0.7612829614604463\n",
      "0.7613132209405501\n",
      "0.7613434727503169\n",
      "0.7613737168926625\n",
      "0.7614039533705018\n",
      "0.7614341821867477\n",
      "0.7614644033443121\n",
      "0.7613679544015199\n",
      "0.7612715298885512\n",
      "0.7613017601620868\n",
      "0.7613319827804508\n",
      "0.7613621977465502\n",
      "0.7612658227848101\n",
      "0.7612960384761422\n",
      "0.7613262465198684\n",
      "0.7613564469188916\n",
      "0.7613866396761133\n",
      "0.761416824794434\n",
      "0.7614470022767519\n",
      "0.7614771721259643\n",
      "0.7615073343449671\n",
      "0.7615374889366544\n",
      "0.7615676359039191\n",
      "0.7614713689799014\n",
      "0.7613751263902933\n",
      "0.7614052824466069\n",
      "0.7614354308819813\n",
      "0.7614655716993051\n",
      "0.7613693784739768\n",
      "0.7613995200202097\n",
      "0.7614296539530184\n",
      "0.7614597802752873\n",
      "0.7614898989898989\n",
      "0.7615200100997349\n",
      "0.7614238828578642\n",
      "0.7614539946989777\n",
      "0.7614840989399293\n",
      "0.7615141955835962\n",
      "0.7614181175876861\n",
      "0.7614482149615239\n",
      "0.7614783047426842\n",
      "0.7615083869340395\n",
      "0.7615384615384615\n",
      "0.7615685285588198\n",
      "0.7614725163893091\n",
      "0.7615025841421909\n",
      "0.7615326443156037\n",
      "0.7615626969124134\n",
      "0.7614667338709677\n",
      "0.7614967871991937\n",
      "0.7615268329554044\n",
      "0.7615568711424613\n",
      "0.7614609571788413\n",
      "0.7614909960962095\n",
      "0.7613951145807102\n",
      "0.7612992572076042\n",
      "0.7613293051359517\n",
      "0.7613593455003147\n",
      "0.7613893783035489\n",
      "0.7614194035485089\n",
      "0.7614494212380473\n",
      "0.7614794313750157\n",
      "0.7613836477987421\n",
      "0.7612878883159351\n",
      "0.7611921529175051\n",
      "0.7612221803093172\n",
      "0.7612522001508675\n",
      "0.7612822124450032\n",
      "0.7611865258924082\n",
      "0.7610908633907252\n",
      "0.7609952249308871\n",
      "0.7608996105038321\n",
      "0.760929648241206\n",
      "0.7609596784323578\n",
      "0.7609897010801306\n",
      "0.7610197161873665\n",
      "0.761049723756906\n",
      "0.7609541745134966\n",
      "0.7609841827768014\n",
      "0.7608886657462031\n",
      "0.7609186746987951\n",
      "0.7609486761199649\n",
      "0.760978670012547\n",
      "0.7610086563793752\n",
      "0.7610386352232815\n",
      "0.7609431832434467\n",
      "0.7609731627790318\n",
      "0.7610031347962383\n",
      "0.7610330992978936\n",
      "0.7610630562868246\n",
      "0.7609676610679368\n",
      "0.7609976187492167\n",
      "0.7610275689223057\n",
      "0.7610575115900263\n",
      "0.7610874467551992\n",
      "0.7611173744206439\n",
      "0.7611472945891784\n",
      "0.7611772072636193\n",
      "0.7612071124467819\n",
      "0.7612370101414799\n",
      "0.7611417125688533\n",
      "0.761171610965077\n",
      "0.7612015018773467\n",
      "0.7611062445250907\n",
      "0.7611361361361362\n",
      "0.7611660202677343\n",
      "0.7611958969226921\n",
      "0.7612257661038149\n",
      "0.761255627813907\n",
      "0.761285482055771\n",
      "0.7613153288322081\n",
      "0.7613451681460183\n",
      "0.761375\n",
      "0.7614048243969503\n",
      "0.7614346413396651\n",
      "0.7614644508309384\n",
      "0.7613693153423289\n",
      "0.7613991255465334\n",
      "0.7613040219835123\n",
      "0.7613338328962158\n",
      "0.7613636363636364\n",
      "0.7613934323885628\n",
      "0.7614232209737828\n",
      "0.7614530021220821\n",
      "0.7613579630554169\n",
      "0.7613877449145139\n",
      "0.761417519341153\n",
      "0.761447286338116\n",
      "0.7614770459081837\n",
      "0.761506798054135\n",
      "0.7615365427787478\n",
      "0.7615662800847987\n",
      "0.7614713216957606\n",
      "0.7613763869841665\n",
      "0.7614061331338818\n",
      "0.7613112302131372\n",
      "0.7612163509471586\n",
      "0.7611214953271028\n",
      "0.761151258410167\n",
      "0.7611810140774885\n",
      "0.7612107623318386\n",
      "0.761240503175987\n",
      "0.7612702366127023\n",
      "0.7612999626447516\n",
      "0.7613296812749004\n",
      "0.7613593925059131\n",
      "0.7612646253422952\n",
      "0.7612943372744244\n",
      "0.7611996017919362\n",
      "0.7612293144208038\n",
      "0.761259019656631\n",
      "0.7612887175021769\n",
      "0.7611940298507462\n",
      "0.761223728391991\n",
      "0.7612534195473762\n",
      "0.7612831033196569\n",
      "0.7613127797115863\n",
      "0.7613424487259167\n",
      "0.7612478250062142\n",
      "0.7611532248042748\n",
      "0.7610586481113321\n",
      "0.7610883339545285\n",
      "0.7609937888198758\n",
      "0.7610234753446776\n",
      "0.7610531544957775\n",
      "0.761082826275922\n",
      "0.7611124906878569\n",
      "0.7610180012414649\n",
      "0.7609235352532274\n",
      "0.7608290927144098\n",
      "0.760734673616282\n",
      "0.7607643628241717\n",
      "0.7606699751861042\n",
      "0.7605756109663814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7606053088563632\n",
      "0.7606349993798834\n",
      "0.7606646825396826\n",
      "0.7606943583384997\n",
      "0.7607240267790727\n",
      "0.7607536878641379\n",
      "0.7607833415964304\n",
      "0.7608129879786838\n",
      "0.7608426270136307\n",
      "0.760872258704002\n",
      "0.7609018830525273\n",
      "0.7609315000619349\n",
      "0.7609611097349517\n",
      "0.7609907120743034\n",
      "0.7608964834076275\n",
      "0.7609260864182246\n",
      "0.7609556820995296\n",
      "0.7609852704542641\n",
      "0.7608910891089109\n",
      "0.7609206781338943\n",
      "0.7609502598366741\n",
      "0.7609798342199678\n",
      "0.7608857001484414\n",
      "0.7609152752009894\n",
      "0.760944842938412\n",
      "0.7608507481142575\n",
      "0.7608803165182987\n",
      "0.7609098776115713\n",
      "0.7609394313967861\n",
      "0.7609689778766531\n",
      "0.7609985170538803\n",
      "0.761028048931175\n",
      "0.7610575735112429\n",
      "0.7610870907967882\n",
      "0.7611166007905138\n",
      "0.7611461034951217\n",
      "0.7611755989133119\n",
      "0.7612050870477837\n",
      "0.7612345679012346\n",
      "0.7611405999259351\n",
      "0.7611700814613676\n",
      "0.7611995557201037\n",
      "0.7612290227048372\n",
      "0.7612584824182603\n",
      "0.7612879348630643\n",
      "0.761317380041939\n",
      "0.7613468179575728\n",
      "0.7613762486126526\n",
      "0.7612823674475956\n",
      "0.7613117987917642\n",
      "0.7613412228796844\n",
      "0.7613706397140392\n",
      "0.7614000492975105\n",
      "0.7614294516327788\n",
      "0.7613356333169049\n",
      "0.7613650363434766\n",
      "0.7613944321261394\n",
      "0.7614238206675699\n",
      "0.7614532019704433\n",
      "0.7614825760374339\n",
      "0.7615119428712139\n",
      "0.7615413024744553\n",
      "0.7614475627769571\n",
      "0.761476923076923\n",
      "0.7615062761506276\n",
      "0.7615356220007383\n",
      "0.7615649606299213\n",
      "0.7615942920408414\n",
      "0.7616236162361624\n",
      "0.7616529332185463\n",
      "0.7616822429906542\n",
      "0.7617115455551458\n",
      "0.7617408409146791\n",
      "0.7617701290719114\n",
      "0.7617994100294986\n",
      "0.7617057883740936\n",
      "0.7617350700417793\n",
      "0.761764344514068\n",
      "0.7617936117936118\n",
      "0.7617000368505098\n",
      "0.7617293048391058\n",
      "0.7617585656391993\n",
      "0.7617878192534381\n",
      "0.761817065684469\n",
      "0.7618463049349374\n",
      "0.7617527924389346\n",
      "0.7617820324005891\n",
      "0.7618112651859124\n",
      "0.7617177914110429\n",
      "0.7617470249049196\n",
      "0.7616535819430814\n",
      "0.7615601619035938\n",
      "0.7614667647780231\n",
      "0.76137339055794\n",
      "0.7614026483570377\n",
      "0.761431898982469\n",
      "0.7613385633733758\n",
      "0.7613678146831719\n",
      "0.7613970588235294\n",
      "0.7613037617938978\n",
      "0.7613330066160254\n",
      "0.7613622442729389\n",
      "0.7612689857912788\n",
      "0.7611757501530925\n",
      "0.7610825373499878\n",
      "0.7611117913554549\n",
      "0.7610186092066601\n",
      "0.7610478638756274\n",
      "0.761077111383109\n",
      "0.7611063517317341\n",
      "0.7611355849241311\n",
      "0.7610424568701823\n",
      "0.7609493516026425\n",
      "0.7609785932721712\n",
      "0.7610078277886497\n",
      "0.7610370551547022\n",
      "0.7610662753729518\n",
      "0.7610954884460203\n",
      "0.7611246943765281\n",
      "0.7610316587214278\n",
      "0.7610608653141041\n",
      "0.7609678601979714\n",
      "0.7609970674486803\n",
      "0.7610262675626145\n",
      "0.7610554605423895\n",
      "0.7610846463906192\n",
      "0.761113825109917\n",
      "0.7611429967028941\n",
      "0.7611721611721611\n",
      "0.7612013185203272\n",
      "0.76123046875\n",
      "0.7612596118637862\n",
      "0.7612887478642909\n",
      "0.761195851128737\n",
      "0.7612249877989263\n",
      "0.7612541173600098\n",
      "0.7612832398145889\n",
      "0.7613123551652641\n",
      "0.761219512195122\n",
      "0.7612486282160712\n",
      "0.7611558156547183\n",
      "0.7611849323418262\n",
      "0.7610921501706485\n",
      "0.761121267519805\n",
      "0.7611503777723617\n",
      "0.7611794809309126\n",
      "0.7612085769980507\n",
      "0.7611158484590084\n",
      "0.7610231425091352\n",
      "0.7610522469857508\n",
      "0.7610813443740867\n",
      "0.761110434676732\n",
      "0.7611395178962747\n",
      "0.7610468654899574\n",
      "0.76095423563778\n",
      "0.7608616283315078\n",
      "0.7608907276709662\n",
      "0.7609198199294318\n",
      "0.7608272506082725\n",
      "0.7608563435105218\n",
      "0.760885429335928\n",
      "0.760792897969111\n",
      "0.7607003891050583\n",
      "0.7607294832826748\n",
      "0.7607585703865791\n",
      "0.7607876504193509\n",
      "0.7608167233835683\n",
      "0.7608457892818082\n",
      "0.7608748481166464\n",
      "0.7609038998906573\n",
      "0.7608114674441205\n",
      "0.7608405198591036\n",
      "0.7608695652173914\n",
      "0.7608986035215544\n",
      "0.7609276347741623\n",
      "0.7609566589777832\n",
      "0.7609856761349842\n",
      "0.7608933122951814\n",
      "0.7609223300970874\n",
      "0.7609513408566921\n",
      "0.760980344576559\n",
      "0.7608880262040519\n",
      "0.7607957302280446\n",
      "0.7607034566403881\n",
      "0.7607324763521708\n",
      "0.7606402328119316\n",
      "0.7606692531522793\n",
      "0.7605770396411686\n",
      "0.7606060606060606\n",
      "0.7606350745364199\n",
      "0.7606640814348037\n",
      "0.7606930813037683\n",
      "0.7607220741458687\n",
      "0.7606299212598425\n",
      "0.7606589147286822\n",
      "0.7605667918130071\n",
      "0.7605957859045773\n",
      "0.7606247729749365\n",
      "0.7605326876513318\n",
      "0.7604406246217165\n",
      "0.7604696199467441\n",
      "0.7603775868328694\n",
      "0.760406582768635\n",
      "0.7603145795523291\n",
      "0.7603435760948464\n",
      "0.7602516027579533\n",
      "0.7602805999032414\n",
      "0.7603095900350707\n",
      "0.760217654171705\n",
      "0.760246644903881\n",
      "0.7601547388781431\n",
      "0.7601837302066965\n",
      "0.7602127145274353\n",
      "0.7602416918429002\n",
      "0.7602706621556308\n",
      "0.7601788087471306\n",
      "0.760207779656922\n",
      "0.7602367435680638\n",
      "0.7602657004830918\n",
      "0.7601738920420239\n",
      "0.760202849553248\n",
      "0.760111070868043\n",
      "0.7600193143408981\n",
      "0.76004828002414\n",
      "0.7600772387159064\n",
      "0.7601061904187282\n",
      "0.7601351351351351\n",
      "0.760043431053203\n",
      "0.7599517490952955\n",
      "0.7599807019659872\n",
      "0.7598890496864448\n",
      "0.7599180031351742\n",
      "0.7599469496021221\n",
      "0.7599758890898132\n",
      "0.7600048216007714\n",
      "0.7600337471375196\n",
      "0.760062665702579\n",
      "0.7599710808531148\n",
      "0.76\n",
      "0.7599084447656909\n",
      "0.7599373644904842\n",
      "0.7599662772491871\n",
      "0.759995183044316\n",
      "0.7600240818783865\n",
      "0.7600529737539128\n",
      "0.7600818586734079\n",
      "0.7601107366393838\n",
      "0.7601396076543507\n",
      "0.7601684717208183\n",
      "0.7601973288412947\n",
      "0.7602261790182868\n",
      "0.7602550222543005\n",
      "0.7602838585518402\n",
      "0.7603126879134094\n",
      "0.7603415103415103\n",
      "0.7603703258386437\n",
      "0.7603991344073094\n",
      "0.760427936050006\n",
      "0.7604567307692308\n",
      "0.7604855185674799\n",
      "0.7605142994472482\n",
      "0.7605430734110297\n",
      "0.7605718404613167\n",
      "0.7606006006006006\n",
      "0.7605092481383617\n",
      "0.7605380088867539\n",
      "0.760566762728146\n",
      "0.7605955096650258\n",
      "0.76062424969988\n",
      "0.7606529828351939\n",
      "0.7606817090734518\n",
      "0.7605904236169447\n",
      "0.7606191504679626\n",
      "0.7606478704259149\n",
      "0.7606765834932822\n",
      "0.760705289672544\n",
      "0.7607339889661789\n",
      "0.7606427629212136\n",
      "0.7606714628297362\n",
      "0.7607001558566119\n",
      "0.7607288420043156\n",
      "0.7607575212753206\n",
      "0.7607861936720998\n",
      "0.760814859197124\n",
      "0.7607236999760364\n",
      "0.7607523661195639\n",
      "0.7607810253953042\n",
      "0.7606899029823931\n",
      "0.7605988023952096\n",
      "0.760507723625913\n",
      "0.7604166666666666\n",
      "0.7603256315096373\n",
      "0.7603543212832177\n",
      "0.7603830041891083\n",
      "0.760411680229775\n",
      "0.7604403494076822\n",
      "0.7604690117252931\n",
      "0.76049766718507\n",
      "0.7605263157894737\n",
      "0.760554957540964\n",
      "0.7605835924419995\n",
      "0.7604926461796007\n",
      "0.7604017216642754\n",
      "0.760430364614465\n",
      "0.7604590007171886\n",
      "0.7604876299749014\n",
      "0.7605162523900574\n",
      "0.7605448679651093\n",
      "0.760573476702509\n",
      "0.7604826185640903\n",
      "0.7605112279025322\n",
      "0.7605398304072615\n",
      "0.7605684260807261\n",
      "0.7604776119402985\n",
      "0.7605062082139447\n",
      "0.7605347976602602\n",
      "0.7605633802816901\n",
      "0.7605919560806779\n",
      "0.7606205250596658\n",
      "0.7606490872210954\n",
      "0.7605583392984968\n",
      "0.7605869020637004\n",
      "0.7606154580152672\n",
      "0.7606440071556351\n",
      "0.7606725494872406\n",
      "0.7605818528675331\n",
      "0.7606103958035288\n",
      "0.7606389319346764\n",
      "0.7606674612634088\n",
      "0.7606959837921583\n",
      "0.7607244995233555\n",
      "0.7607530084594305\n",
      "0.7607815106028115\n",
      "0.7608100059559262\n",
      "0.7608384945212006\n",
      "0.7608669763010599\n",
      "0.7607763753274589\n",
      "0.7606857959280867\n",
      "0.7607142857142857\n",
      "0.7607427687180097\n",
      "0.760652225660557\n",
      "0.7606807092704986\n",
      "0.760590195145169\n",
      "0.7606186793575253\n",
      "0.7606471567927671\n",
      "0.7606756274533127\n",
      "0.7607040913415795\n",
      "0.7607325484599834\n",
      "0.7606420927467301\n",
      "0.7606705504696231\n",
      "0.7606990014265336\n",
      "0.760727445619874\n",
      "0.7607558830520561\n",
      "0.7606654783125372\n",
      "0.7605750950570342\n",
      "0.7606035404538434\n",
      "0.7605131860299359\n",
      "0.7604228530704359\n",
      "0.7604513064133016\n",
      "0.7603610022562641\n",
      "0.760389456186179\n",
      "0.760417903359848\n",
      "0.7604463437796771\n",
      "0.7604747774480712\n",
      "0.7605032043674341\n",
      "0.7605316245401685\n",
      "0.7605600379686759\n",
      "0.7605884446553565\n",
      "0.7606168446026097\n",
      "0.7606452378128336\n",
      "0.760673624288425\n",
      "0.7607020040317799\n",
      "0.7607303770452929\n",
      "0.7607587433313574\n",
      "0.7607871028923661\n",
      "0.7606969301884556\n",
      "0.7606067788575492\n",
      "0.7606351463443536\n",
      "0.7605450236966824\n",
      "0.7605733917782254\n",
      "0.7604832977967306\n",
      "0.7603932251569347\n",
      "0.7604216011369019\n",
      "0.7604499703966844\n",
      "0.7604783329386692\n",
      "0.7605066887652421\n",
      "0.7605350378787878\n",
      "0.7605633802816901\n",
      "0.7605917159763314\n",
      "0.7606200449650928\n",
      "0.7606483672503549\n",
      "0.7606766828344966\n",
      "0.7607049917198959\n",
      "0.7607332939089296\n",
      "0.7607615894039735\n",
      "0.7606716329667731\n",
      "0.7605816978008986\n",
      "0.760491783898806\n",
      "0.7605200945626478\n",
      "0.7605483985344522\n",
      "0.7605766958165918\n",
      "0.760604986411438\n",
      "0.7606332703213611\n",
      "0.76066154754873\n",
      "0.7606898180959131\n",
      "0.7607180819652769\n",
      "0.7607463391591875\n",
      "0.760656511984886\n",
      "0.7606847697756789\n",
      "0.7607130208948176\n",
      "0.7607412653446648\n",
      "0.7607695031275817\n",
      "0.7607977342459287\n",
      "0.7607079646017699\n",
      "0.7606182161396885\n",
      "0.7606464551138374\n",
      "0.7606746874262797\n",
      "0.7607029130793725\n",
      "0.7607311320754717\n",
      "0.7607593444169319\n",
      "0.7607875501061071\n",
      "0.7608157491453496\n",
      "0.7607260726072608\n",
      "0.7607542722451385\n",
      "0.7606646240867311\n",
      "0.7606928243195475\n",
      "0.7607210179076344\n",
      "0.7606314053480976\n",
      "0.7606595995288575\n",
      "0.7606877870686609\n",
      "0.760715967969854\n",
      "0.7607441422347816\n",
      "0.7607723098657876\n",
      "0.7608004708652149\n",
      "0.7608286252354048\n",
      "0.7607390843827233\n",
      "0.7607672393504354\n",
      "0.7607953876926697\n",
      "0.7607058823529412\n",
      "0.7607340312904364\n",
      "0.7607621736062103\n",
      "0.760790309302599\n",
      "0.760818438381938\n",
      "0.7608465608465609\n",
      "0.7608746766988008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7609027859409898\n",
      "0.7609308885754584\n",
      "0.7609589846045364\n",
      "0.7609870740305523\n",
      "0.7610151568558337\n",
      "0.7610432330827067\n",
      "0.7609538353107013\n",
      "0.7608644585388772\n",
      "0.7608925425719318\n",
      "0.7609206200093941\n",
      "0.760948690853587\n",
      "0.7608593566564922\n",
      "0.7608874281018899\n",
      "0.7607981220657277\n",
      "0.7608261941086727\n",
      "0.7607369162168505\n",
      "0.76076498885369\n",
      "0.760793054903801\n",
      "0.7608211143695015\n",
      "0.7608491672531081\n",
      "0.7608772135569368\n",
      "0.7607879924953096\n",
      "0.7608160393950053\n",
      "0.7607268464243845\n",
      "0.7607548939163052\n",
      "0.7607829348335677\n",
      "0.7608109691784836\n",
      "0.760838996953363\n",
      "0.7608670181605155\n",
      "0.7608950328022493\n",
      "0.7609230408808715\n",
      "0.7609510423986883\n",
      "0.7609790373580044\n",
      "0.7610070257611241\n",
      "0.7609179253014869\n",
      "0.7608288457035823\n",
      "0.7608568418588318\n",
      "0.7608848314606742\n",
      "0.7609128145114101\n",
      "0.7609407910133396\n",
      "0.7608517608517609\n",
      "0.7608797379503978\n",
      "0.7609077085039185\n",
      "0.7609356725146199\n",
      "0.7608466845982926\n",
      "0.7608746492048644\n",
      "0.7609026072723021\n",
      "0.7609305588028993\n",
      "0.760958503798948\n",
      "0.7609864422627396\n",
      "0.7608975108098632\n",
      "0.7609254498714653\n",
      "0.7609533824044865\n",
      "0.7609813084112149\n",
      "0.7610092278939377\n",
      "0.7610371408549405\n",
      "0.7610650472965083\n",
      "0.7610929472209248\n",
      "0.7611208406304728\n",
      "0.761148727527434\n",
      "0.7611766079140889\n",
      "0.761204481792717\n",
      "0.7612323491655969\n",
      "0.7611435239206534\n",
      "0.7611713919029285\n",
      "0.7611992533831078\n",
      "0.7612271083634667\n",
      "0.761138325169116\n",
      "0.7611661807580175\n",
      "0.7611940298507462\n",
      "0.7612218724495744\n",
      "0.7611331312660293\n",
      "0.7611609744725493\n",
      "0.7610722610722611\n",
      "0.7611001048828808\n",
      "0.7610114192495921\n",
      "0.7609227542817196\n",
      "0.760950605778192\n",
      "0.7609784507862551\n",
      "0.7610062893081762\n",
      "0.7610341213462211\n",
      "0.7610619469026548\n",
      "0.7609733379904529\n",
      "0.7610011641443539\n",
      "0.7609125829356304\n",
      "0.7609404096834265\n",
      "0.7608518561619924\n",
      "0.7608796835001164\n",
      "0.7609075043630017\n",
      "0.7609353187529083\n",
      "0.7609631266720949\n",
      "0.7608746220051175\n",
      "0.7609024305151761\n",
      "0.7609302325581395\n",
      "0.7608417625857459\n",
      "0.7608695652173914\n",
      "0.7608973613855632\n",
      "0.7609251510925151\n",
      "0.7609529343404997\n",
      "0.7609807111317686\n",
      "0.7610084814685721\n",
      "0.7610362453531598\n",
      "0.7610640027877802\n",
      "0.7610917537746806\n",
      "0.7611194983161073\n",
      "0.7611472364143056\n",
      "0.7610588645071403\n",
      "0.7610866032040864\n",
      "0.7611143354614045\n",
      "0.7611420612813371\n",
      "0.7611697806661251\n",
      "0.7610814574147133\n",
      "0.7611091773987702\n",
      "0.7611368909512761\n",
      "0.7611645980744693\n",
      "0.7610763163999072\n",
      "0.7611040241215354\n",
      "0.7611317254174397\n",
      "0.7611594202898551\n",
      "0.7611871087410156\n",
      "0.761214790773154\n",
      "0.7612424663885026\n",
      "0.7612701355892919\n",
      "0.761297798377752\n",
      "0.7613254547561117\n",
      "0.7613531047265987\n",
      "0.7613807482914399\n",
      "0.7614083854528608\n",
      "0.7614360162130863\n",
      "0.76146364057434\n",
      "0.7613754775963877\n",
      "0.7612873350312572\n",
      "0.7613149670100706\n",
      "0.7613425925925926\n",
      "0.7612544844346719\n",
      "0.761282110622541\n",
      "0.761309730417679\n",
      "0.7613373438223044\n",
      "0.7613649508386351\n",
      "0.7613925514688874\n",
      "0.7614201457152769\n",
      "0.7614477335800185\n",
      "0.7614753150653255\n",
      "0.7613872832369942\n",
      "0.7614148653334875\n",
      "0.7614424410540915\n",
      "0.761470010401017\n",
      "0.7614975733764733\n",
      "0.761525129982669\n",
      "0.7614371534195934\n",
      "0.7614647106387894\n",
      "0.7614922614922615\n",
      "0.7614043192054509\n",
      "0.7614318706697459\n",
      "0.7614594157718508\n",
      "0.7614869545139691\n",
      "0.7613990534456886\n",
      "0.7614265927977839\n",
      "0.7614541257934219\n",
      "0.7614816524348027\n",
      "0.761509172724126\n",
      "0.7614213197969543\n",
      "0.7614488406967355\n",
      "0.7614763552479815\n",
      "0.7615038634528889\n",
      "0.7614160516605166\n",
      "0.7613282601176063\n",
      "0.761355775881946\n",
      "0.7613832853025937\n",
      "0.7614107883817427\n",
      "0.7614382851215858\n",
      "0.7614657755243144\n",
      "0.7614932595921189\n",
      "0.761520737327189\n",
      "0.7615482087317129\n",
      "0.7615756738078784\n",
      "0.7616031325578717\n",
      "0.7615154306771074\n",
      "0.7615428900402994\n",
      "0.7615703430808197\n",
      "0.7615977898008518\n",
      "0.7616252302025782\n",
      "0.7616526642881805\n",
      "0.7616800920598389\n",
      "0.7617075135197331\n",
      "0.7617349286700414\n",
      "0.7616473024272403\n",
      "0.7616747181964574\n",
      "0.7617021276595745\n",
      "0.7617295308187673\n",
      "0.7617569276762102\n",
      "0.7616693492756955\n",
      "0.761581791010461\n",
      "0.7616091954022989\n",
      "0.7616365934950006\n",
      "0.7616639852907378\n",
      "0.761691370791681\n",
      "0.7616038602941176\n",
      "0.7616312464101092\n",
      "0.7616586262347806\n",
      "0.7616859997702997\n",
      "0.7615985300872761\n",
      "0.7616259042369962\n",
      "0.7616532721010333\n",
      "0.7615658362989324\n",
      "0.7614784205693297\n",
      "0.7615057959371054\n",
      "0.761533165021804\n",
      "0.761560527825588\n",
      "0.7615878843506195\n",
      "0.7615005162326488\n",
      "0.7615278733654508\n",
      "0.7615552242229614\n",
      "0.7615825688073394\n",
      "0.7614952413714023\n",
      "0.761522586562715\n",
      "0.76143528602545\n",
      "0.761462631820266\n",
      "0.7614899713467048\n",
      "0.7614027045610818\n",
      "0.7614300446888965\n",
      "0.7614573785517873\n",
      "0.7614847061519074\n",
      "0.761397479954181\n",
      "0.7614248081548506\n",
      "0.7614521300961979\n",
      "0.7614794457803733\n",
      "0.761506755209526\n",
      "0.7615340583858042\n",
      "0.7615613553113553\n",
      "0.7615886459883255\n",
      "0.7616159304188601\n",
      "0.7615287790365031\n",
      "0.7615560640732265\n",
      "0.7614689394806087\n",
      "0.7613818348204072\n",
      "0.7614091273018415\n",
      "0.7614364135407137\n",
      "0.7614636935391652\n",
      "0.7613766293162588\n",
      "0.7612895850005716\n",
      "0.7613168724279835\n",
      "0.7613441536175563\n",
      "0.7613714285714286\n",
      "0.761398697291738\n",
      "0.7613117001828154\n",
      "0.7613389694961727\n",
      "0.7612519990861321\n",
      "0.7612792689891491\n",
      "0.7613065326633166\n",
      "0.7613337901107685\n",
      "0.7613610413336378\n",
      "0.7612741180500057\n",
      "0.7613013698630137\n",
      "0.7613286154548567\n",
      "0.7613558548276649\n",
      "0.7613830879835672\n",
      "0.7614103149246919\n",
      "0.761437535653166\n",
      "0.7614647501711157\n",
      "0.7614919584806661\n",
      "0.761405109489051\n",
      "0.7614323183943437\n",
      "0.7614595210946408\n",
      "0.7614867175920648\n",
      "0.7613999088007296\n",
      "0.761427105893081\n",
      "0.7614542967859586\n",
      "0.7614814814814815\n",
      "0.7613947128532361\n",
      "0.7614218981428734\n",
      "0.7614490772385509\n",
      "0.7614762501423853\n",
      "0.761503416856492\n",
      "0.761530577382986\n",
      "0.7615577317239809\n",
      "0.7615848798815894\n",
      "0.7616120218579235\n",
      "0.7616391576550939\n",
      "0.7616662872752106\n",
      "0.7615796062364857\n",
      "0.7614929449248976\n",
      "0.7614063033337126\n",
      "0.7614334470989761\n",
      "0.7613468319872597\n",
      "0.7613739763421292\n",
      "0.7614011145229159\n",
      "0.7614282465317261\n",
      "0.7614553723706652\n",
      "0.761368804001819\n",
      "0.7612822553143117\n",
      "0.7613093884973857\n",
      "0.7612228662347994\n",
      "0.76125\n",
      "0.761163504147256\n",
      "0.761190638491252\n",
      "0.7611041690332842\n",
      "0.761017719218537\n",
      "0.7610448608745031\n",
      "0.761071996366114\n",
      "0.7610991256954696\n",
      "0.7611262488646685\n",
      "0.7611533658758088\n",
      "0.761066969353008\n",
      "0.7610940869367836\n",
      "0.7610077167498865\n",
      "0.7609213661636219\n",
      "0.7609484910369866\n",
      "0.7609756097560976\n",
      "0.7608892921960072\n",
      "0.7608029942157196\n",
      "0.7608301202086641\n",
      "0.7607438485089012\n",
      "0.7607709750566893\n",
      "0.7607980954540302\n",
      "0.7608252097030151\n",
      "0.7607389776719936\n",
      "0.7606527651858568\n",
      "0.7606798866855524\n",
      "0.7605937004305461\n",
      "0.76050753370341\n",
      "0.7604213864975079\n",
      "0.7604485219164119\n",
      "0.7603624009060023\n",
      "0.7603895368587928\n",
      "0.7604166666666666\n",
      "0.7604437903317106\n",
      "0.7603577088521621\n",
      "0.7603848330503679\n",
      "0.7604119511090991\n",
      "0.7604390630304402\n",
      "0.7604661688164743\n",
      "0.7604932684692839\n",
      "0.7605203619909502\n",
      "0.7605474493835539\n",
      "0.7605745306491744\n",
      "0.7606016057898903\n",
      "0.7606286748077793\n",
      "0.760655737704918\n",
      "0.7606827944833823\n",
      "0.7605968124788064\n",
      "0.760623869801085\n",
      "0.7606509210080235\n",
      "0.7605649717514125\n",
      "0.7605920235001695\n",
      "0.7606190691369182\n",
      "0.7606461086637298\n",
      "0.7606731420826744\n",
      "0.7607001693958215\n",
      "0.7607271906052394\n",
      "0.7606413006661398\n",
      "0.7606683224204109\n",
      "0.7606953380742747\n",
      "0.7606094808126411\n",
      "0.7605236429296919\n",
      "0.7605506657639359\n",
      "0.7605776825002821\n",
      "0.7606046931407943\n",
      "0.7606316976875352\n",
      "0.7606586961425671\n",
      "0.7606856885079508\n",
      "0.7607126747857466\n",
      "0.7607396549780133\n",
      "0.7606538895152198\n",
      "0.7606808702513809\n",
      "0.760595130748422\n",
      "0.7606221120252451\n",
      "0.7606490872210954\n",
      "0.7606760563380282\n",
      "0.7607030193780983\n",
      "0.7607299763433593\n",
      "0.760756927235864\n",
      "0.7607838720576642\n",
      "0.7608108108108108\n",
      "0.7608377434973539\n",
      "0.7608646701193424\n",
      "0.7608915906788247\n",
      "0.7608059432687978\n",
      "0.7607203151378729\n",
      "0.7606347062795409\n",
      "0.7606616405986272\n",
      "0.7605760576057605\n",
      "0.7606029924625942\n",
      "0.7606299212598425\n",
      "0.7606568439995501\n",
      "0.7605713000449843\n",
      "0.7604857753289104\n",
      "0.7605127051945132\n",
      "0.760539629005059\n",
      "0.7604541366906474\n",
      "0.7604810610318085\n",
      "0.7605079793211957\n",
      "0.7605348915608495\n",
      "0.760561797752809\n",
      "0.7604763509718009\n",
      "0.7603909233880027\n",
      "0.7604178366842638\n",
      "0.7604447439353099\n",
      "0.7603593486805166\n",
      "0.7603862564563216\n",
      "0.7603008869428539\n",
      "0.7603277952402335\n",
      "0.7603546974969132\n",
      "0.7603815937149271\n",
      "0.7604084838963079\n",
      "0.760435368043088\n",
      "0.7604622461572983\n",
      "0.7604891182409692\n",
      "0.7605159842961301\n",
      "0.7604306864064603\n",
      "0.7604575529886733\n",
      "0.760484413545638\n",
      "0.7605112680793811\n",
      "0.7604260089686099\n",
      "0.7604528640286964\n",
      "0.7604797130688187\n",
      "0.7605065560910008\n",
      "0.7604213357238906\n",
      "0.7603361344537816\n",
      "0.7603629845395474\n",
      "0.7603898286098353\n",
      "0.7604166666666666\n",
      "0.7603315040878038\n",
      "0.7602463605823069\n",
      "0.7602732056880529\n",
      "0.7603000447828034\n",
      "0.7603268778685772\n",
      "0.760353704947392\n",
      "0.7603805260212647\n",
      "0.7604073410922113\n",
      "0.7604341501622468\n",
      "0.7604609532333856\n",
      "0.7604877503076407\n",
      "0.7605145413870246\n",
      "0.7605413264735488\n",
      "0.7604562737642585\n",
      "0.7604830593760483\n",
      "0.7605098389982111\n",
      "0.760424818334265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7604515984797675\n",
      "0.7603666033307254\n",
      "0.7603933839964238\n",
      "0.7604201586769471\n",
      "0.7603351955307263\n",
      "0.7603619707295274\n",
      "0.7603887399463807\n",
      "0.7603038087791801\n",
      "0.7603305785123967\n",
      "0.76035734226689\n",
      "0.7603841000446628\n",
      "0.7602992073238808\n",
      "0.7602143335565975\n",
      "0.7602410983368679\n",
      "0.7602678571428572\n",
      "0.7602946099765651\n",
      "0.760321356839991\n",
      "0.7603480977351333\n",
      "0.7602632753235163\n",
      "0.7602900167317346\n",
      "0.7603167521748829\n",
      "0.760343481654957\n",
      "0.7603702051739518\n",
      "0.7603969227338611\n",
      "0.7604236343366778\n",
      "0.7604503399843942\n",
      "0.7603655818100757\n",
      "0.7603922879750362\n",
      "0.7604189881880989\n",
      "0.7604456824512534\n",
      "0.7604723707664884\n",
      "0.7604990531357915\n",
      "0.7605257295611495\n",
      "0.7605524000445484\n",
      "0.7605790645879733\n",
      "0.7604943770181495\n",
      "0.7605210420841684\n",
      "0.7605477012134031\n",
      "0.7604630454140695\n",
      "0.760378408458542\n",
      "0.7604050745604274\n",
      "0.7604317347279403\n",
      "0.760347129506008\n",
      "0.7603737901880075\n",
      "0.7602892102335929\n",
      "0.760204649093538\n",
      "0.7602313167259787\n",
      "0.760257978427666\n",
      "0.7602846342005781\n",
      "0.7603112840466926\n",
      "0.7603379279679857\n",
      "0.7602534178059354\n",
      "0.7602800622360525\n",
      "0.7603067007445272\n",
      "0.7603333333333333\n",
      "0.760359960004444\n",
      "0.7602754943345923\n",
      "0.7601910474286349\n",
      "0.760217681030653\n",
      "0.7602443087173792\n",
      "0.7602709304907839\n",
      "0.7602975463528366\n",
      "0.7603241563055062\n",
      "0.7603507603507603\n",
      "0.7603773584905661\n",
      "0.7604039507268894\n",
      "0.7604305370616955\n",
      "0.7604571174969489\n",
      "0.760372753494564\n",
      "0.7603993344425957\n",
      "0.7603149955634427\n",
      "0.7603415770211822\n",
      "0.7603681525837215\n",
      "0.7603947222530214\n",
      "0.7604212860310421\n",
      "0.7604478439197428\n",
      "0.7603635557526047\n",
      "0.7603901141527208\n",
      "0.7603058510638298\n",
      "0.7603324099722991\n",
      "0.76035896299579\n",
      "0.7603855101362579\n",
      "0.7604120513956579\n",
      "0.7604385867759442\n",
      "0.7604651162790698\n",
      "0.7604916399069871\n",
      "0.7605181576616474\n",
      "0.7605446695450017\n",
      "0.7605711755589993\n",
      "0.7605976757055893\n",
      "0.7605135015493582\n",
      "0.7605400022131238\n",
      "0.7605664970126134\n",
      "0.7605929859497732\n",
      "0.7605088495575221\n",
      "0.7604247317774583\n",
      "0.7603406326034063\n",
      "0.7603671348003981\n",
      "0.7603936311366651\n",
      "0.760309563294638\n",
      "0.7603360601370771\n",
      "0.7602520172432851\n",
      "0.7602785145888594\n",
      "0.7603050060780197\n",
      "0.7603314917127072\n",
      "0.7603579714948624\n",
      "0.7603844454264251\n",
      "0.7604109135093339\n",
      "0.7604373757455268\n",
      "0.7604638321369409\n",
      "0.7604902826855123\n",
      "0.7604063155570278\n",
      "0.7604327666151468\n",
      "0.7603488243735511\n",
      "0.7602649006622516\n",
      "0.7602913585696943\n",
      "0.7603178106378283\n",
      "0.7603442568685865\n",
      "0.7603706972639012\n",
      "0.7603971318257032\n",
      "0.7603132583278183\n",
      "0.7603396933936253\n",
      "0.7603661226290251\n",
      "0.7603925460359466\n",
      "0.7603087100330761\n",
      "0.7603351339433359\n",
      "0.7602513227513228\n",
      "0.7601675300341673\n",
      "0.7601939607670267\n",
      "0.7602203856749311\n",
      "0.7602468047598061\n",
      "0.7602732180235761\n",
      "0.7602996254681648\n",
      "0.7603260270954951\n",
      "0.760352422907489\n",
      "0.7603788129060676\n",
      "0.7602950891874036\n",
      "0.760321479687328\n",
      "0.7603478643769265\n",
      "0.7603742432581178\n",
      "0.7602905569007264\n",
      "0.7603169362826016\n",
      "0.7603433098591549\n",
      "0.7603696776323028\n",
      "0.7603960396039604\n",
      "0.7603123968760312\n",
      "0.7603387593488782\n",
      "0.7603651160233147\n",
      "0.7602815042885419\n",
      "0.760307861462342\n",
      "0.7602242744063324\n",
      "0.7601407057271627\n",
      "0.7600571554187734\n",
      "0.7600835256621606\n",
      "0.7601098901098902\n",
      "0.7601362487638721\n",
      "0.7601626016260162\n",
      "0.7601889486982314\n",
      "0.7602152899824253\n",
      "0.7602416254805052\n",
      "0.7602679551943773\n",
      "0.760294279125947\n",
      "0.760320597277119\n",
      "0.7603469096497969\n",
      "0.7603732162458836\n",
      "0.760289759631215\n",
      "0.7603160667251976\n",
      "0.760342368045649\n",
      "0.7602589422865921\n",
      "0.7602852441031267\n",
      "0.7603115401491882\n",
      "0.7603378304266755\n",
      "0.7602544417635446\n",
      "0.7602807325364623\n",
      "0.7603070175438597\n",
      "0.760333296787633\n",
      "0.7603595702696777\n",
      "0.7603858379918886\n",
      "0.7604120999561595\n",
      "0.7604383561643836\n",
      "0.7604646066184527\n",
      "0.7604908513202586\n",
      "0.7605170902716915\n",
      "0.7605433234746413\n",
      "0.7604600219058051\n",
      "0.7604862556127477\n",
      "0.7604029785370127\n",
      "0.7603197196977992\n",
      "0.7602364790891176\n",
      "0.7602627257799671\n",
      "0.7602889667250438\n",
      "0.7602057568129583\n",
      "0.7602319982490698\n",
      "0.7601488127803917\n",
      "0.7601750547045952\n",
      "0.7602012908872114\n",
      "0.760118136075257\n",
      "0.7601443727441759\n",
      "0.7601706036745407\n",
      "0.7600874794969928\n",
      "0.7601137109118741\n",
      "0.7601399365912321\n",
      "0.760056843025798\n",
      "0.7600830691878894\n",
      "0.7601092896174864\n",
      "0.7601355043164681\n",
      "0.7600524475524476\n",
      "0.7599694089369605\n",
      "0.7598863884640594\n",
      "0.759803386127799\n",
      "0.7597204019222368\n",
      "0.7596374358414327\n",
      "0.7596636820266434\n",
      "0.7596899224806202\n",
      "0.7597161572052402\n",
      "0.7597423862023797\n",
      "0.759768609473914\n",
      "0.7597948270217177\n",
      "0.7598210388476647\n",
      "0.759847244953628\n",
      "0.7598734453414794\n",
      "0.7598996400130904\n",
      "0.7599258289703316\n",
      "0.7599520122150726\n",
      "0.7599781897491821\n",
      "0.7600043615745284\n",
      "0.7599215002180549\n",
      "0.75994767251717\n",
      "0.7599738391105297\n",
      "0.76\n",
      "0.7600261551874455\n",
      "0.7599433366023755\n",
      "0.7599694922641098\n",
      "0.7599956422268221\n",
      "0.7600217864923747\n",
      "0.7599390044657445\n",
      "0.7599651492049663\n",
      "0.7599912882500273\n",
      "0.7599085365853658\n",
      "0.7599346761023408\n",
      "0.7599608099281515\n",
      "0.7599869380646566\n",
      "0.7600130605137135\n",
      "0.7600391772771793\n",
      "0.7600652883569097\n",
      "0.75998259166576\n",
      "0.7600087032201914\n",
      "0.759926030675514\n",
      "0.759952142701762\n",
      "0.7598694942903752\n",
      "0.7598956067855589\n",
      "0.7599217136022616\n",
      "0.7599478147423353\n",
      "0.7599739102076313\n",
      "0.76\n",
      "0.7600260841212911\n",
      "0.7600521625733536\n",
      "0.7600782353580354\n",
      "0.7601043024771839\n",
      "0.7601303639326453\n",
      "0.7601564197262655\n",
      "0.7600738568480504\n",
      "0.7600999131190269\n",
      "0.7600173743077424\n",
      "0.760043431053203\n",
      "0.7600694821409185\n",
      "0.7600955275727312\n",
      "0.760121567350483\n",
      "0.7601476014760148\n",
      "0.7601736299511666\n",
      "0.7601996527777778\n",
      "0.7602256699576869\n",
      "0.7602516814927316\n",
      "0.7602776873847489\n",
      "0.7603036876355749\n",
      "0.7603296822470448\n",
      "0.7602472348731295\n",
      "0.7602732299685568\n",
      "0.7601908065915004\n",
      "0.7601084010840108\n",
      "0.760134402774767\n",
      "0.760160398829522\n",
      "0.7601863892501084\n",
      "0.7602123740383574\n",
      "0.7602383531960997\n",
      "0.7602643267251652\n",
      "0.7601819757365684\n",
      "0.760099642586375\n",
      "0.7601256226987221\n",
      "0.7601515971846237\n",
      "0.7601775660459074\n",
      "0.7602035292843997\n",
      "0.7602294869019268\n",
      "0.7602554389003139\n",
      "0.7602813852813853\n",
      "0.7603073260469646\n",
      "0.7603332611988747\n",
      "0.7603591907389375\n",
      "0.7603851146689745\n",
      "0.7604110329908058\n",
      "0.7604369457062513\n",
      "0.7604628528171299\n",
      "0.7604887543252595\n",
      "0.7605146502324576\n",
      "0.7605405405405405\n",
      "0.7604583288293157\n",
      "0.7604842196281885\n",
      "0.7605101048308657\n",
      "0.7605359844391615\n",
      "0.7605618584548892\n",
      "0.7605877268798618\n",
      "0.7606135897158907\n",
      "0.7606394469647872\n",
      "0.7606652986283616\n",
      "0.7605831533477322\n",
      "0.7606090055069646\n",
      "0.7606348520837832\n",
      "0.7606606930799957\n",
      "0.7606865284974094\n",
      "0.7607123583378306\n",
      "0.7607381826030649\n",
      "0.7606560915074997\n",
      "0.7606819162710401\n",
      "0.7605998489588952\n",
      "0.7606256742179073\n",
      "0.7606514939057275\n",
      "0.7606773080241588\n",
      "0.7607031165750027\n",
      "0.7607289195600604\n",
      "0.7607547169811321\n",
      "0.7606727037516171\n",
      "0.7606985016707988\n",
      "0.7607242940288855\n",
      "0.760642310593814\n",
      "0.7606681034482758\n",
      "0.7606938907445319\n",
      "0.7607196724843783\n",
      "0.7607454486696111\n",
      "0.760771219302025\n",
      "0.7607969843834141\n",
      "0.7608227439155718\n",
      "0.7608484979002907\n",
      "0.7608742463393626\n",
      "0.7608999892345786\n",
      "0.7609257265877287\n",
      "0.7609514584006027\n",
      "0.7609771846749892\n",
      "0.7610029054126762\n",
      "0.7610286206154508\n",
      "0.7609467455621302\n",
      "0.7609724612736661\n",
      "0.760998171453157\n",
      "0.7610238761023876\n",
      "0.7610495752231423\n",
      "0.7610752688172043\n",
      "0.7611009568863563\n",
      "0.7611266394323801\n",
      "0.7611523164570568\n",
      "0.7610705073086844\n",
      "0.7609887157442236\n",
      "0.7610143993122717\n",
      "0.7609326313527452\n",
      "0.7608508809626128\n",
      "0.7608765710602643\n",
      "0.7609022556390977\n",
      "0.7609279347008914\n",
      "0.7609536082474226\n",
      "0.7609792762804681\n",
      "0.7610049388018038\n",
      "0.7610305958132045\n",
      "0.7609489051094891\n",
      "0.7609745626274552\n",
      "0.7610002146383344\n",
      "0.7610258611438996\n",
      "0.7610515021459228\n",
      "0.7610771376461754\n",
      "0.760995494529071\n",
      "0.7609138689263113\n",
      "0.7608322608322609\n",
      "0.760857908847185\n",
      "0.7608835513617842\n",
      "0.7609091883778278\n",
      "0.760934819897084\n",
      "0.7608532532961733\n",
      "0.7608788853161843\n",
      "0.7609045118422463\n",
      "0.760822974710673\n",
      "0.7607414550519661\n",
      "0.7606599528605099\n",
      "0.7606855918585966\n",
      "0.7606041131105399\n",
      "0.760629752597194\n",
      "0.760655386592418\n",
      "0.7605739372523825\n",
      "0.7605995717344753\n",
      "0.7606252007279735\n",
      "0.7606508242346393\n",
      "0.7606764422562347\n",
      "0.7607020547945206\n",
      "0.7606206527554842\n",
      "0.7606462657821528\n",
      "0.7606718733283406\n",
      "0.7606974753958066\n",
      "0.7606161086747246\n",
      "0.7606417112299465\n",
      "0.7606673083092718\n",
      "0.7606928999144568\n",
      "0.7607184860472576\n",
      "0.7607440667094291\n",
      "0.7606627471940139\n",
      "0.7606883283454468\n",
      "0.7606070321684301\n",
      "0.7606326138063689\n",
      "0.7606581899775617\n",
      "0.7606837606837606\n",
      "0.7606024997329345\n",
      "0.760628070925016\n",
      "0.7606536366549183\n",
      "0.7606791969243912\n",
      "0.7607047517351841\n",
      "0.7607303010890455\n",
      "0.7607558449877229\n",
      "0.7607813834329633\n",
      "0.760806916426513\n",
      "0.7608324439701174\n",
      "0.7608579660655213\n",
      "0.7607767819035425\n",
      "0.7608023044916249\n",
      "0.7608278216343076\n",
      "0.7607466666666667\n",
      "0.7607721843003413\n",
      "0.7606910525754506\n",
      "0.7607165706973769\n",
      "0.7607420833777588\n",
      "0.7607675906183369\n",
      "0.7606864939771879\n",
      "0.7607120017053933\n",
      "0.760630928274539\n",
      "0.7606564364876386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7606819392647842\n",
      "0.7607074366077136\n",
      "0.7607329285181634\n",
      "0.7607584149978697\n",
      "0.7607838960485674\n",
      "0.7608093716719915\n",
      "0.7607283569374933\n",
      "0.7607538330494037\n",
      "0.7607793037368253\n",
      "0.7608047690014903\n",
      "0.7607237892496008\n",
      "0.7607492550021285\n",
      "0.7607747153346813\n",
      "0.7606937646307725\n",
      "0.7607192254495159\n",
      "0.7606382978723404\n",
      "0.7606637591745559\n",
      "0.7606892150606254\n",
      "0.760714665532277\n",
      "0.7607401105912378\n",
      "0.7607655502392344\n",
      "0.7607909844779928\n",
      "0.7608164133092378\n",
      "0.7607355442176871\n",
      "0.7607609735359762\n",
      "0.7607863974495218\n",
      "0.7608118159600468\n",
      "0.7608372290692733\n",
      "0.7608626367789227\n",
      "0.7607818143190992\n",
      "0.7608072225172597\n",
      "0.7608326253186066\n",
      "0.7607518317935649\n",
      "0.7607772350817583\n",
      "0.7608026329758998\n",
      "0.7607218683651805\n",
      "0.760747266744507\n",
      "0.7607726597325408\n",
      "0.7607980473309987\n",
      "0.7608234295415959\n",
      "0.7608488063660478\n",
      "0.7608741778060684\n",
      "0.7608995438633712\n",
      "0.7609249045396691\n",
      "0.760844204051331\n",
      "0.760763520678685\n",
      "0.7607888877107412\n",
      "0.7608142493638677\n",
      "0.7608396056397753\n",
      "0.7608649565401738\n",
      "0.7607843137254902\n",
      "0.7608096651123357\n",
      "0.7608350111264173\n",
      "0.7608603517694427\n",
      "0.7607797436169086\n",
      "0.7608050847457627\n",
      "0.7608304205063023\n",
      "0.760855750900233\n",
      "0.7608810759292598\n",
      "0.7609063955950869\n",
      "0.7609317098994177\n",
      "0.7609570188439552\n",
      "0.7608764687202286\n",
      "0.7609017781541066\n",
      "0.7608212509260239\n",
      "0.7607407407407407\n",
      "0.7607660565019575\n",
      "0.7607913669064749\n",
      "0.7607108854331958\n",
      "0.7607361963190185\n",
      "0.7607615018508725\n",
      "0.7607868020304569\n",
      "0.7608120968594692\n",
      "0.7608373863396067\n",
      "0.7608626704725658\n",
      "0.7607822410147992\n",
      "0.76080752563154\n",
      "0.7608328049038259\n",
      "0.760858078833351\n",
      "0.760883347421809\n",
      "0.7609086106708928\n",
      "0.7609338685822945\n",
      "0.7609591211577057\n",
      "0.7609843683988171\n",
      "0.7609040025345866\n",
      "0.7609292502639915\n",
      "0.7609544926618097\n",
      "0.7608741554054054\n",
      "0.7608993982898765\n",
      "0.7609246358454718\n",
      "0.7609498680738787\n",
      "0.7609750949767835\n",
      "0.760894797931835\n",
      "0.7609200253217978\n",
      "0.7609452473889651\n",
      "0.7609704641350211\n",
      "0.7609956755616496\n",
      "0.7609154186880405\n",
      "0.7608351787409048\n",
      "0.7608603964571911\n",
      "0.7608856088560886\n",
      "0.7609108159392789\n",
      "0.7609360177084431\n",
      "0.7609612141652614\n",
      "0.76088102012857\n",
      "0.7609062170706006\n",
      "0.7609314087029818\n",
      "0.7609565950273914\n",
      "0.7609817760455072\n",
      "0.7609016220771013\n",
      "0.760926803580832\n",
      "0.7608466722830666\n",
      "0.7608718542697694\n",
      "0.7608970309538851\n",
      "0.7609222023370881\n",
      "0.7609473684210526\n",
      "0.7609725292074518\n",
      "0.7609976846979584\n",
      "0.7610228348942439\n",
      "0.7609427609427609\n",
      "0.7609679116254603\n",
      "0.7609930570166211\n",
      "0.7610181971179131\n",
      "0.7609381573411864\n",
      "0.7608581343989904\n",
      "0.7608832807570978\n",
      "0.7609084218273579\n",
      "0.7608284272497897\n",
      "0.7608535688005886\n",
      "0.7608787050662182\n",
      "0.7609038360483448\n",
      "0.7608238755779739\n",
      "0.7607439319113166\n",
      "0.7606640050430763\n",
      "0.76068914801975\n",
      "0.760609243697479\n",
      "0.7606343871442075\n",
      "0.7605545053560177\n",
      "0.760579649270188\n",
      "0.7606047879042419\n",
      "0.7605249343832021\n",
      "0.7605500734830989\n",
      "0.7604702424687729\n",
      "0.7604953820319059\n",
      "0.7605205163186064\n",
      "0.7604407135362015\n",
      "0.7604658482845452\n",
      "0.7604909777591271\n",
      "0.760516101961607\n",
      "0.7605412208936438\n",
      "0.7604614577871002\n",
      "0.7603817114093959\n",
      "0.760406836531404\n",
      "0.7604319563849864\n",
      "0.7604570709717999\n",
      "0.760482180293501\n",
      "0.7605072843517451\n",
      "0.7605323831481869\n",
      "0.7604526878340144\n",
      "0.7604777870913663\n",
      "0.7603981141959141\n",
      "0.760423213911586\n",
      "0.7604483083691211\n",
      "0.7604733975701717\n",
      "0.7604984815163891\n",
      "0.7605235602094241\n",
      "0.7605486336509266\n",
      "0.760573701842546\n",
      "0.7604940856275516\n",
      "0.7605191542809294\n",
      "0.7604395604395604\n",
      "0.7604646295521138\n",
      "0.7604896934184368\n",
      "0.7605147520401758\n",
      "0.7605398054189769\n",
      "0.7605648535564854\n",
      "0.7605898964543458\n",
      "0.760510353482535\n",
      "0.760430827146293\n",
      "0.7604558762024257\n",
      "0.7603763721902771\n",
      "0.7604014217018608\n",
      "0.7603219400020905\n",
      "0.7603469899665551\n",
      "0.7603720346953705\n",
      "0.7602925809822362\n",
      "0.7602131438721137\n",
      "0.7602381947346427\n",
      "0.7601587799018071\n",
      "0.7600793816586588\n",
      "0.7601044386422976\n",
      "0.7601294903926483\n",
      "0.7601545369113502\n",
      "0.7600751722697849\n",
      "0.7601002192295647\n",
      "0.7601252609603341\n",
      "0.7600459242250287\n",
      "0.7600709663953246\n",
      "0.7600960033392465\n",
      "0.7601210350584308\n",
      "0.7601460615545123\n",
      "0.7601710828291258\n",
      "0.7600917909669344\n",
      "0.7600125156445557\n",
      "0.7599332568568151\n",
      "0.7599582898852972\n",
      "0.7599833176936711\n",
      "0.7599040867389492\n",
      "0.7598248723027208\n",
      "0.7598499061913696\n",
      "0.7598749348619073\n",
      "0.759899958315965\n",
      "0.7599249765551734\n",
      "0.7599499895811628\n",
      "0.7599749973955621\n",
      "0.76\n",
      "0.7600249973961045\n",
      "0.7600499895855031\n",
      "0.7599708424450693\n",
      "0.7599958350687214\n",
      "0.7600208224882874\n",
      "0.7600458047053925\n",
      "0.7600707817216613\n",
      "0.7600957535387177\n",
      "0.760120720158185\n",
      "0.7600416233090531\n",
      "0.7599625429195713\n",
      "0.7599875156054932\n",
      "0.7600124830958077\n",
      "0.7600374453921365\n",
      "0.7600624024960998\n",
      "0.7600873544093179\n",
      "0.7601123011334096\n",
      "0.7601372426699937\n",
      "0.7601621790206882\n",
      "0.7601871101871102\n",
      "0.7602120361708762\n",
      "0.7602369569736022\n",
      "0.7602618725969033\n",
      "0.760286783042394\n",
      "0.7602077922077922\n",
      "0.7602327030957823\n",
      "0.7602576088085593\n",
      "0.7602825093477358\n",
      "0.7602035517706927\n",
      "0.7602284527518173\n",
      "0.7602533485619354\n",
      "0.7602782392026578\n",
      "0.7603031246755944\n",
      "0.7603280049823542\n",
      "0.7603528801245459\n",
      "0.7603777501037775\n",
      "0.7604026149216562\n",
      "0.7603237186138203\n",
      "0.7603485838779956\n",
      "0.7603734439834025\n",
      "0.7602945752515299\n",
      "0.7603194358017009\n",
      "0.760344291195686\n",
      "0.7602654500207383\n",
      "0.7602903058579575\n",
      "0.7603151565415717\n",
      "0.7603400020731834\n",
      "0.7603648424543947\n",
      "0.7603896776868069\n",
      "0.7604145077720207\n",
      "0.7604393327116361\n",
      "0.7603605470368835\n",
      "0.7602817776856936\n",
      "0.760306608659623\n",
      "0.7603314344899016\n",
      "0.7603562551781275\n",
      "0.7603810707258983\n",
      "0.7604058811348106\n",
      "0.7604306864064603\n",
      "0.7604554865424431\n",
      "0.7604802815443535\n",
      "0.760505071413786\n",
      "0.7605298561523336\n",
      "0.7605546357615894\n",
      "0.7605794102431453\n",
      "0.760604179598593\n",
      "0.76052549912072\n",
      "0.7605502689284237\n",
      "0.7604716103009619\n",
      "0.7604963805584282\n",
      "0.7605211456933099\n",
      "0.760545905707196\n",
      "0.7604672800578931\n",
      "0.7603886706636345\n",
      "0.7604134366925065\n",
      "0.760438197602315\n",
      "0.7604629533946471\n",
      "0.7604877040710891\n",
      "0.7605124496332266\n",
      "0.7605371900826446\n",
      "0.7605619254209276\n",
      "0.7604833712042967\n",
      "0.7605081069916348\n",
      "0.7605328376703842\n",
      "0.760557563242127\n",
      "0.7605822837084452\n",
      "0.7605037679364096\n",
      "0.7604252683732452\n",
      "0.7603467850139334\n",
      "0.7602683178534572\n",
      "0.7602930554122381\n",
      "0.7603177878662815\n",
      "0.760342515217167\n",
      "0.760367237466474\n",
      "0.7603919546157814\n",
      "0.7604166666666666\n",
      "0.7604413736207074\n",
      "0.7604660754794803\n",
      "0.7603876688318383\n",
      "0.7604123711340206\n",
      "0.7604370683434697\n",
      "0.7604617604617605\n",
      "0.7604864474904669\n",
      "0.7605111294311624\n",
      "0.7605358062854198\n",
      "0.7605604780548114\n",
      "0.7605851447409087\n",
      "0.7606098063452822\n",
      "0.7605314656504274\n",
      "0.7605561277033985\n",
      "0.7605807846771703\n",
      "0.7605024711696869\n",
      "0.7605271285905487\n",
      "0.7605517809347334\n",
      "0.7605764282038086\n",
      "0.7606010703993413\n",
      "0.760625707522898\n",
      "0.7606503395760444\n",
      "0.7605720753163906\n",
      "0.7605967078189301\n",
      "0.7606213352535748\n",
      "0.7605430981279572\n",
      "0.7605677260104906\n",
      "0.7605923488276429\n",
      "0.7606169665809769\n",
      "0.7606415792720543\n",
      "0.7606661869024365\n",
      "0.7606907894736842\n",
      "0.7607153869873574\n",
      "0.7607399794450154\n",
      "0.760764566848217\n",
      "0.7607891491985204\n",
      "0.7608137264974828\n",
      "0.7608382987466612\n",
      "0.7608628659476117\n",
      "0.7608874281018899\n",
      "0.7608092841737701\n",
      "0.7608338467857876\n",
      "0.7608584043536297\n",
      "0.7608829568788501\n",
      "0.7609075043630017\n",
      "0.760932046807637\n",
      "0.760853946423073\n",
      "0.7608784893267652\n",
      "0.7609030271934325\n",
      "0.7609275600246255\n",
      "0.7609520878218939\n",
      "0.760976610586787\n",
      "0.7610011283208534\n",
      "0.7610256410256411\n",
      "0.7610501487026972\n",
      "0.7609721082854799\n",
      "0.7609966164257151\n",
      "0.7610211195407013\n",
      "0.7610456176319836\n",
      "0.7609676096760968\n",
      "0.7608896177103618\n",
      "0.7609141217462595\n",
      "0.7609386207603238\n",
      "0.7609631147540984\n",
      "0.7609876037291261\n",
      "0.7609096496619545\n",
      "0.7608317115640684\n",
      "0.7607537894305613\n",
      "0.7606758832565285\n",
      "0.7607003891050583\n",
      "0.760724889935497\n",
      "0.7607493857493858\n",
      "0.7606715119254785\n",
      "0.7606960081883316\n",
      "0.7606181557670658\n",
      "0.7605403192795743\n",
      "0.7605648214468433\n",
      "0.7605893186003683\n",
      "0.760613810741688\n",
      "0.7606382978723404\n",
      "0.7605604991306126\n",
      "0.7605849867048476\n",
      "0.7606094692708866\n",
      "0.7605316973415133\n",
      "0.7605561803496576\n",
      "0.7605806583520752\n",
      "0.7606051313503015\n",
      "0.7606295993458708\n",
      "0.7606540623403168\n",
      "0.7605763335377069\n",
      "0.7606007969755798\n",
      "0.7606252554147936\n",
      "0.7606497088568802\n",
      "0.7606741573033707\n",
      "0.7606986007557961\n",
      "0.7607230392156863\n",
      "0.7607474726845707\n",
      "0.760771901163978\n",
      "0.7607963246554365\n",
      "0.7608207431604737\n",
      "0.7608451566806165\n",
      "0.7607675035721576\n",
      "0.760689866312889\n",
      "0.7607142857142857\n",
      "0.7607387001326396\n",
      "0.7607631095694756\n",
      "0.7607875140263185\n",
      "0.760811913504692\n",
      "0.7608363080061193\n",
      "0.7607587191515399\n",
      "0.7607831141021719\n",
      "0.7608075040783034\n",
      "0.7608318890814558\n",
      "0.7607543323139654\n",
      "0.7606767913566405\n",
      "0.7607011822258459\n",
      "0.7607255681239172\n",
      "0.7607499490523741\n",
      "0.7606724401426388\n",
      "0.7606968215158925\n",
      "0.7607211979219721\n",
      "0.7607455693623956\n",
      "0.7606680924737753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7606924643584522\n",
      "0.7607168312799104\n",
      "0.760741193239666\n",
      "0.7607655502392344\n",
      "0.7607899022801303\n",
      "0.7608142493638677\n",
      "0.7607368206798291\n",
      "0.7607611682100336\n",
      "0.7607855107855108\n",
      "0.7608098484077729\n",
      "0.7608341810783317\n",
      "0.760858508798698\n",
      "0.7607811228641171\n",
      "0.760703752669582\n",
      "0.760728086231442\n",
      "0.7606507371631926\n",
      "0.7606750711671411\n",
      "0.7606994002236455\n",
      "0.7607237243342143\n",
      "0.7606464071551987\n",
      "0.760670731707317\n",
      "0.7606950513159232\n",
      "0.7607193659825239\n",
      "0.7607436757086254\n",
      "0.7607679804957335\n",
      "0.760792280345353\n",
      "0.7608165752589884\n",
      "0.760739311465421\n",
      "0.7607636068237206\n",
      "0.7607878972484516\n",
      "0.7608121827411167\n",
      "0.760836463303218\n",
      "0.7608607389362566\n",
      "0.760783517710342\n",
      "0.7608077937893242\n",
      "0.760832064941654\n",
      "0.7608563311688312\n",
      "0.7608805924723546\n",
      "0.7609048488537229\n",
      "0.7609291003144335\n",
      "0.7609533468559838\n",
      "0.7609775884798702\n",
      "0.7610018251875887\n",
      "0.7609246679509277\n",
      "0.7609489051094891\n",
      "0.7609731373542828\n",
      "0.7609973646868031\n",
      "0.7610215871085436\n",
      "0.7610458046209971\n",
      "0.7610700172256561\n",
      "0.7610942249240121\n",
      "0.7611184277175564\n",
      "0.7610413290113452\n",
      "0.7610655322596982\n",
      "0.7610897306056309\n",
      "0.7611139240506329\n",
      "0.7611381125961928\n",
      "0.7610610509263946\n",
      "0.7610852399271107\n",
      "0.7611094240307723\n",
      "0.7611336032388664\n",
      "0.7610565732213339\n",
      "0.7610807528840315\n",
      "0.7611049276535465\n",
      "0.7610279239174423\n",
      "0.7610520991401113\n",
      "0.7610762694719806\n",
      "0.7611004349145343\n",
      "0.7610234627831716\n",
      "0.7609465062190313\n",
      "0.7609706774519717\n",
      "0.7609948437973916\n",
      "0.7610190052567731\n",
      "0.7610431618315981\n",
      "0.7609662421669698\n",
      "0.7609903991915109\n",
      "0.7610145513338723\n",
      "0.761038698595534\n",
      "0.7610628409779754\n",
      "0.761086978482675\n",
      "0.761010101010101\n",
      "0.7610342389657611\n",
      "0.7610583720460513\n",
      "0.7610825002524487\n",
      "0.7610056542810986\n",
      "0.7610297829379101\n",
      "0.7610539067231981\n",
      "0.7610780256384375\n",
      "0.7610012111425111\n",
      "0.7610253305076193\n",
      "0.7609485368314833\n",
      "0.7609726566441328\n",
      "0.7609967715899919\n",
      "0.7609200040351054\n",
      "0.7609441194270729\n",
      "0.7609682299546142\n",
      "0.7609923356192013\n",
      "0.7609155994756479\n",
      "0.7609397055858036\n",
      "0.7609638068353665\n",
      "0.7608870967741935\n",
      "0.7609111984678963\n",
      "0.7609352953033662\n",
      "0.7608586113070644\n",
      "0.7607819427650141\n",
      "0.7608060453400504\n",
      "0.760830143058634\n",
      "0.7607535005540446\n",
      "0.7606768734891217\n",
      "0.7607009769362474\n",
      "0.760725075528701\n",
      "0.7607491692679489\n",
      "0.7607732581554572\n",
      "0.760797342192691\n",
      "0.7608214213811154\n",
      "0.7608454957221943\n",
      "0.7608695652173914\n",
      "0.7608936298681694\n",
      "0.7609176896759912\n",
      "0.7609417446423181\n",
      "0.7608651911468813\n",
      "0.7608892465546726\n",
      "0.7609132971233152\n",
      "0.7609373428542693\n",
      "0.7609613837489944\n",
      "0.7609854198089492\n",
      "0.7610094510355921\n",
      "0.761033477430381\n",
      "0.7609569762766385\n",
      "0.760981003115891\n",
      "0.7609045226130653\n",
      "0.760928549894483\n",
      "0.7608520900321544\n",
      "0.7608761177534412\n",
      "0.7609001406469761\n",
      "0.7609241587142139\n",
      "0.7609481719566091\n",
      "0.7609721803756151\n",
      "0.7609961839726853\n",
      "0.761020182749272\n",
      "0.7610441767068273\n",
      "0.7610681658468025\n",
      "0.7610921501706485\n",
      "0.7611161296798153\n",
      "0.7610397430750703\n",
      "0.7610637230306071\n",
      "0.7610876981737908\n",
      "0.76111166850607\n",
      "0.7611356340288925\n",
      "0.7611595947437055\n",
      "0.7611835506519559\n",
      "0.7612075017550898\n",
      "0.7612314480545528\n",
      "0.7612553895517898\n",
      "0.7612793262482455\n",
      "0.7613032581453634\n",
      "0.761327185244587\n",
      "0.7613511075473589\n",
      "0.7613750250551212\n",
      "0.7612987273273875\n",
      "0.7613226452905811\n",
      "0.761346558461076\n",
      "0.7613704668403125\n",
      "0.7613943704297306\n",
      "0.7614182692307693\n",
      "0.7614421632448674\n",
      "0.7614660524734629\n",
      "0.7614899369179934\n",
      "0.7615138165798959\n",
      "0.7614375813394734\n",
      "0.7614614614614614\n",
      "0.7614853368031228\n",
      "0.7614091273018415\n",
      "0.7614330031021715\n",
      "0.7614568741244747\n",
      "0.7614807403701851\n",
      "0.7615046018407363\n",
      "0.7615284585375612\n",
      "0.7614522904580916\n",
      "0.7614761476147615\n",
      "0.7615\n",
      "0.7615238476152385\n",
      "0.7615476904619076\n",
      "0.7615715285414376\n",
      "0.7615953618552579\n",
      "0.7616191904047976\n",
      "0.7616430141914851\n",
      "0.7616668332167483\n",
      "0.7616906474820144\n",
      "0.7617144569887102\n",
      "0.7616383616383616\n",
      "0.7616621716112276\n",
      "0.7616859768278066\n",
      "0.7617097772895236\n",
      "0.7617335729978031\n",
      "0.7616575137294059\n",
      "0.7616813099041534\n",
      "0.7617051013277428\n",
      "0.7617288880015971\n",
      "0.7617526699271384\n",
      "0.7616766467065869\n",
      "0.7617004290988924\n",
      "0.7617242067451606\n",
      "0.7617479796468123\n",
      "0.7617717478052674\n",
      "0.7617955112219451\n",
      "0.7618192698982645\n",
      "0.7617432931086068\n",
      "0.7616673314718787\n",
      "0.7616910958221159\n",
      "0.7616151545363908\n",
      "0.7616389193500149\n",
      "0.7616626794258373\n",
      "0.7616864347652746\n",
      "0.7617101853697429\n",
      "0.7616342800199303\n",
      "0.7616580310880829\n",
      "0.7615821460595795\n",
      "0.7616058975891612\n",
      "0.7616296443868911\n",
      "0.7616533864541832\n",
      "0.761677123792451\n",
      "0.7617008564031069\n",
      "0.7617245842875635\n",
      "0.7617483074472322\n",
      "0.7617720258835241\n",
      "0.7617957395978499\n",
      "0.7618194485916194\n",
      "0.761843152866242\n",
      "0.7618668524231267\n",
      "0.7617910447761194\n",
      "0.7618147448015122\n",
      "0.7618384401114207\n",
      "0.7618621307072516\n",
      "0.7618858165904118\n",
      "0.7619094977623073\n",
      "0.7619331742243437\n",
      "0.7619568459779258\n",
      "0.7619805130244581\n",
      "0.7620041753653445\n",
      "0.7619284294234593\n",
      "0.7619520922373522\n",
      "0.7618763665275293\n",
      "0.7618006558680314\n",
      "0.761724960254372\n",
      "0.7617486338797814\n",
      "0.76177230280151\n",
      "0.7617959670209595\n",
      "0.7618196265395312\n",
      "0.7618432813586254\n",
      "0.7618669314796425\n",
      "0.7618905769039818\n",
      "0.7619142176330421\n",
      "0.761937853668222\n",
      "0.7619614850109192\n",
      "0.7618858560794045\n",
      "0.7619094878920206\n",
      "0.7619331150143892\n",
      "0.7619567374479064\n",
      "0.7619803551939677\n",
      "0.7620039682539682\n",
      "0.7620275766293026\n",
      "0.7620511803213648\n",
      "0.7620747793315481\n",
      "0.7620983736612456\n",
      "0.7620228061477442\n",
      "0.7619472536188776\n",
      "0.761970853573907\n",
      "0.7619944488501189\n",
      "0.7620180394489048\n",
      "0.7619425173439048\n",
      "0.7618670102071152\n",
      "0.7618906064209274\n",
      "0.7619141979589815\n",
      "0.7618387160689518\n",
      "0.7618623080733036\n",
      "0.7617868462757528\n",
      "0.761711399425572\n",
      "0.7617349970291146\n",
      "0.7616595702544806\n",
      "0.7616831683168317\n",
      "0.7617067617067617\n",
      "0.7617303504256583\n",
      "0.7616549539740671\n",
      "0.7616785431512272\n",
      "0.7617021276595745\n",
      "0.7617257075004947\n",
      "0.7617492826753735\n",
      "0.7617728531855956\n",
      "0.7617964190325452\n",
      "0.7618199802176063\n",
      "0.7617446345564237\n",
      "0.7616693037974683\n",
      "0.7616928705626421\n",
      "0.761617559818074\n",
      "0.761641127039051\n",
      "0.7615658362989324\n",
      "0.7615894039735099\n",
      "0.7616129669895236\n",
      "0.7615377013538888\n",
      "0.7615612648221344\n",
      "0.7615848236340282\n",
      "0.7615095830863465\n",
      "0.7614343574039316\n",
      "0.7614579217700513\n",
      "0.7614814814814815\n",
      "0.761505036539601\n",
      "0.7615285869457885\n",
      "0.7615521327014217\n",
      "0.7614769473788133\n",
      "0.7615004935834156\n",
      "0.7615240351396703\n",
      "0.7615475720489538\n",
      "0.7615711043126419\n",
      "0.7615946319321097\n",
      "0.7616181549087321\n",
      "0.7616416732438832\n",
      "0.7616651869389366\n",
      "0.7616886959952653\n",
      "0.7616135713581221\n",
      "0.7616370808678501\n",
      "0.7616605857410512\n",
      "0.7616840859790969\n",
      "0.7617075815833579\n",
      "0.7616324921135647\n",
      "0.761655988171513\n",
      "0.7616794795978711\n",
      "0.761702966394008\n",
      "0.7617264485612929\n",
      "0.7617499261010937\n",
      "0.7616748768472906\n",
      "0.7616983548418875\n",
      "0.7617218282111899\n",
      "0.761646803900325\n",
      "0.761571794366752\n",
      "0.7615952732644018\n",
      "0.761618747538401\n",
      "0.7615437629221227\n",
      "0.7615672376452057\n",
      "0.7615907077468255\n",
      "0.7616141732283465\n",
      "0.7615392185808484\n",
      "0.761562684510923\n",
      "0.7615861458230837\n",
      "0.7616096025186935\n",
      "0.7615346778160355\n",
      "0.7614597678536298\n",
      "0.7614832300580309\n",
      "0.7614083398898505\n",
      "0.7614318025371226\n",
      "0.7614552605703048\n",
      "0.7614787139907581\n",
      "0.7615021627998427\n",
      "0.7615256069989187\n",
      "0.7615490465893454\n",
      "0.7615724815724816\n",
      "0.761497641509434\n",
      "0.7615210769381939\n",
      "0.7614462566319513\n",
      "0.7613714510266234\n",
      "0.7612966601178782\n",
      "0.7613201060799528\n",
      "0.7613435474366529\n",
      "0.7613669841893351\n",
      "0.7613904163393559\n",
      "0.7614138438880707\n",
      "0.7614372668368349\n",
      "0.761460685187003\n",
      "0.7614840989399293\n",
      "0.7615075080969673\n",
      "0.76153091265947\n",
      "0.7614561868315181\n",
      "0.7613814756671899\n",
      "0.7614048857058766\n",
      "0.7614282911516579\n",
      "0.7614516920058853\n",
      "0.7614750882699097\n",
      "0.7614984799450819\n",
      "0.7615218670327515\n",
      "0.761545249534268\n",
      "0.7615686274509804\n",
      "0.7615920007842368\n",
      "0.761517349539306\n",
      "0.7615407233166716\n",
      "0.7614660917287338\n",
      "0.761391474767271\n",
      "0.7614148540074466\n",
      "0.7614382286666014\n",
      "0.7614615987460815\n",
      "0.7614849642472329\n",
      "0.7615083251714005\n",
      "0.7615316815199294\n",
      "0.7614571092831962\n",
      "0.7614804660726525\n",
      "0.7615038182886235\n",
      "0.7615271659324523\n",
      "0.7614526233359437\n",
      "0.7614759714201821\n",
      "0.7614993149344295\n",
      "0.7615226538800274\n",
      "0.761545988258317\n",
      "0.7615693180706389\n",
      "0.761592643318333\n",
      "0.7616159640027389\n",
      "0.7616392801251957\n",
      "0.7616625916870415\n",
      "0.7615881087424213\n",
      "0.7616114207489978\n",
      "0.7615369573719202\n",
      "0.7615602698210969\n",
      "0.76158357771261\n",
      "0.761606881047796\n",
      "0.7616301798279906\n",
      "0.7616534740545294\n",
      "0.7616767637287473\n",
      "0.7616023448949683\n",
      "0.7615279406017976\n",
      "0.7614535508449741\n",
      "0.7613791756202384\n",
      "0.7613048149233324\n",
      "0.761328125\n",
      "0.7613514305243628\n",
      "0.7613747314977544\n",
      "0.7613980279215073\n",
      "0.7614213197969543\n",
      "0.761444607125427\n",
      "0.7614678899082569\n",
      "0.7614911681467746\n",
      "0.7615144418423107\n",
      "0.7615377109961947\n",
      "0.7615609756097561\n",
      "0.7615842356843234\n",
      "0.7615099492781896\n",
      "0.7615332097922559\n",
      "0.7614589428515701\n",
      "0.7614822038030229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7615054602184087\n",
      "0.7615287120990543\n",
      "0.7615519594462858\n",
      "0.7615752022614289\n",
      "0.7615984405458089\n",
      "0.7616216743007505\n",
      "0.7616449035275775\n",
      "0.7616681282276138\n",
      "0.7615939204988309\n",
      "0.7616171456405261\n",
      "0.7616403662575492\n",
      "0.7616635823512223\n",
      "0.7616867939228672\n",
      "0.761612620508326\n",
      "0.7616358325219085\n",
      "0.7616590400155778\n",
      "0.7616822429906542\n",
      "0.7616080989000292\n",
      "0.7616313023165272\n",
      "0.761654501216545\n",
      "0.7616776956014013\n",
      "0.7617008854724141\n",
      "0.761724070830901\n",
      "0.7617472516781788\n",
      "0.7617704280155642\n",
      "0.7617935998443731\n",
      "0.7618167671659211\n",
      "0.7617426820966644\n",
      "0.7616686114352392\n",
      "0.7615945551774429\n",
      "0.7615205133190744\n",
      "0.7615436959268981\n",
      "0.7615668740279938\n",
      "0.7615900476236758\n",
      "0.7615160349854228\n",
      "0.7615392090175882\n",
      "0.7615623785464438\n",
      "0.7614883901680753\n",
      "0.7615115601321159\n",
      "0.7615347255949491\n",
      "0.7614607614607615\n",
      "0.7614839273574827\n",
      "0.7615070887550981\n",
      "0.761530245654918\n",
      "0.7614563106796116\n",
      "0.7614794680128143\n",
      "0.7615026208503203\n",
      "0.7615257691934388\n",
      "0.7615489130434783\n",
      "0.7615720524017467\n",
      "0.7614981564137395\n",
      "0.761424274764723\n",
      "0.7613504074505238\n",
      "0.7613735570860414\n",
      "0.7612997090203686\n",
      "0.7613228590825332\n",
      "0.7613460046547711\n",
      "0.7612721807427518\n",
      "0.7612953267403529\n",
      "0.7613184682501212\n",
      "0.7612446684761536\n",
      "0.7611708830086266\n",
      "0.7611940298507462\n",
      "0.7612171722066091\n",
      "0.7611434108527132\n",
      "0.7611665536285244\n",
      "0.7610928114706452\n",
      "0.7611159546643418\n",
      "0.761139093374661\n",
      "0.7611622276029055\n",
      "0.7611853573503777\n",
      "0.761208482618379\n",
      "0.7612316034082107\n",
      "0.7612547197211734\n",
      "0.7612778315585673\n",
      "0.761300938921692\n",
      "0.7613240418118467\n",
      "0.76134714023033\n",
      "0.7612734662279853\n",
      "0.76129656507015\n",
      "0.7612229102167183\n",
      "0.7612460094805069\n",
      "0.7612691042754884\n",
      "0.7612921946029597\n",
      "0.7612185686653772\n",
      "0.7612416594139831\n",
      "0.7612647456971572\n",
      "0.7611911437687324\n",
      "0.7612142304717711\n",
      "0.7612373127114548\n",
      "0.7611637347767253\n",
      "0.7610901710640765\n",
      "0.7611132586006958\n",
      "0.7611363416755242\n",
      "0.7611594202898551\n",
      "0.7611824944449812\n",
      "0.7611089644513137\n",
      "0.7610354486622235\n",
      "0.7610585281050801\n",
      "0.7610816030902945\n",
      "0.7610081112398609\n",
      "0.7609346335811529\n",
      "0.7609577138443715\n",
      "0.7609807896515107\n",
      "0.760907335907336\n",
      "0.760930412122382\n",
      "0.7609534838834202\n",
      "0.7608800540384059\n",
      "0.760903126206098\n",
      "0.7608297153883261\n",
      "0.7608527879606406\n",
      "0.760875856081798\n",
      "0.7608024691358025\n",
      "0.7608255376603337\n",
      "0.7608486017357763\n",
      "0.7608716613634172\n",
      "0.760894716544543\n",
      "0.7609177672804396\n",
      "0.7609408135723925\n",
      "0.760867469879518\n",
      "0.7608905165767155\n",
      "0.7609135588320324\n",
      "0.7609365966467527\n",
      "0.7609596300221602\n",
      "0.7609826589595375\n",
      "0.7610056834601676\n",
      "0.7610287035253323\n",
      "0.7610517191563132\n",
      "0.7610747303543913\n",
      "0.7610014443909485\n",
      "0.7609281725399576\n",
      "0.7608549147973428\n",
      "0.7608779360800925\n",
      "0.7609009529309847\n",
      "0.7609239653512994\n",
      "0.7609469733423154\n",
      "0.7609699769053118\n",
      "0.7609929760415665\n",
      "0.7610159707523572\n",
      "0.7610389610389611\n",
      "0.760965756060023\n",
      "0.7609887467538713\n",
      "0.7609155606847471\n",
      "0.7608423886912203\n",
      "0.7608653846153847\n",
      "0.7607922315162003\n",
      "0.7608152278407998\n",
      "0.7608382197443045\n",
      "0.7608612072279892\n",
      "0.7608841902931283\n",
      "0.7609071689409955\n",
      "0.7609301431728644\n",
      "0.7609531129900077\n",
      "0.7609760783936977\n",
      "0.7609990393852065\n",
      "0.7610219959658054\n",
      "0.7610449481367653\n",
      "0.7610678958993565\n",
      "0.7609948146725561\n",
      "0.7610177628420547\n",
      "0.7610407066052227\n",
      "0.7610636459633292\n",
      "0.7609905932040699\n",
      "0.7610135329686151\n",
      "0.7610364683301344\n",
      "0.7610593992898954\n",
      "0.7610823258491652\n",
      "0.7610093063417442\n",
      "0.7609363008442057\n",
      "0.760863309352518\n",
      "0.7608862459236524\n",
      "0.7609091780953294\n",
      "0.7609321058688148\n",
      "0.7609550292453735\n",
      "0.7609779482262704\n",
      "0.7610008628127696\n",
      "0.7609279141104295\n",
      "0.7609508290999712\n",
      "0.760973739697144\n",
      "0.7609966459032104\n",
      "0.7609237255653507\n",
      "0.7608508192009198\n",
      "0.7608737305997317\n",
      "0.7608966376089664\n",
      "0.7609195402298851\n",
      "0.7609424384637486\n",
      "0.7609653323118176\n",
      "0.7609882217753519\n",
      "0.7610111068556109\n",
      "0.7610339875538535\n",
      "0.7609611334482098\n",
      "0.7609840145496315\n",
      "0.7610068912710567\n",
      "0.7609340606756627\n",
      "0.760956937799043\n",
      "0.7609798105444455\n",
      "0.7610026789131267\n",
      "0.7609298765904525\n",
      "0.7608570881959059\n",
      "0.7608799617407939\n",
      "0.760902830910482\n",
      "0.7609256957062255\n",
      "0.760948556129279\n",
      "0.7609714121808968\n",
      "0.7609942638623327\n",
      "0.7610171111748398\n",
      "0.7610399541196712\n",
      "0.761062792698079\n",
      "0.761085626911315\n",
      "0.7610129001433349\n",
      "0.7610357347601758\n",
      "0.761058565013853\n",
      "0.7610813909056171\n",
      "0.7610086923297354\n",
      "0.7610315186246418\n",
      "0.7609588386973546\n",
      "0.7609816653934302\n",
      "0.7610044877303542\n",
      "0.7609318312010693\n",
      "0.7609546539379475\n",
      "0.7609774723176785\n",
      "0.76100028634151\n",
      "0.7610230960106891\n",
      "0.760950472373318\n",
      "0.7609732824427481\n",
      "0.7609960881595268\n",
      "0.7610188895248998\n",
      "0.7610416865401126\n",
      "0.7609690957649752\n",
      "0.7609918931807343\n",
      "0.7610146862483311\n",
      "0.7609421188137694\n",
      "0.7609649122807017\n",
      "0.7609877014014682\n",
      "0.7609151572926597\n",
      "0.7609379468115528\n",
      "0.7609607319862752\n",
      "0.7609835128180692\n",
      "0.7609109967600534\n",
      "0.7609337779895188\n",
      "0.7609565548780488\n",
      "0.7609793274268839\n",
      "0.7610020956372643\n",
      "0.7610248595104295\n",
      "0.7610476190476191\n",
      "0.7609751452242643\n",
      "0.7609979051609217\n",
      "0.7609254498714653\n",
      "0.760948210205636\n",
      "0.7608757734412185\n",
      "0.7608985341709499\n",
      "0.7609212905681927\n",
      "0.7608488770460602\n",
      "0.760776477305167\n",
      "0.7607992388201713\n",
      "0.7607268575777757\n",
      "0.7607496194824962\n",
      "0.7607723770569771\n",
      "0.7607951303024538\n",
      "0.7608178792201616\n",
      "0.7608406238113351\n",
      "0.7608633640772083\n",
      "0.7607910249096786\n",
      "0.7607186994961498\n",
      "0.7607414448669202\n",
      "0.7607641859138865\n",
      "0.7607869226382817\n",
      "0.7607146251069087\n",
      "0.7607373622196884\n",
      "0.7607600950118765\n",
      "0.760687820634619\n",
      "0.7606155599886008\n",
      "0.7606382978723404\n",
      "0.7606610314369836\n",
      "0.7606837606837606\n",
      "0.7607064856139019\n",
      "0.7607292062286365\n",
      "0.760751922529194\n",
      "0.7607746345168027\n",
      "0.760797342192691\n",
      "0.7608200455580866\n",
      "0.7608427446142165\n",
      "0.7608654393623079\n",
      "0.7607932441408103\n",
      "0.7608159392789374\n",
      "0.7608386301109952\n",
      "0.7608613166382091\n",
      "0.760883998861804\n",
      "0.7609066767830045\n",
      "0.7609293504030347\n",
      "0.7609520197231178\n",
      "0.760974684744477\n",
      "0.760902540766022\n",
      "0.7609252061806806\n",
      "0.7609478672985782\n",
      "0.7608757463747512\n",
      "0.7608984078847612\n",
      "0.7609210650999716\n",
      "0.7608489672162213\n",
      "0.760871624822359\n",
      "0.7608942781356575\n",
      "0.7608222032774462\n",
      "0.7608448569804888\n",
      "0.7608675063926508\n",
      "0.7607954545454545\n",
      "0.7608181043461794\n",
      "0.7607460708199205\n",
      "0.7606740509325003\n",
      "0.7606967057932601\n",
      "0.7606247042120208\n",
      "0.7606473594548552\n",
      "0.7606700104097662\n",
      "0.7606926570779712\n",
      "0.760715299460687\n",
      "0.7607379375591297\n",
      "0.7606659729448492\n",
      "0.7605940219447598\n",
      "0.7606166650903244\n",
      "0.7606393039530925\n",
      "0.7605673758865248\n",
      "0.760590015128593\n",
      "0.7606126500898175\n",
      "0.7606352807714124\n",
      "0.7606579071745911\n",
      "0.7606805293005671\n",
      "0.7607031471505529\n",
      "0.7607257607257607\n",
      "0.7607483700274025\n",
      "0.7607709750566893\n",
      "0.7607935758148323\n",
      "0.7608161723030418\n",
      "0.7607443090582791\n",
      "0.760766905931243\n",
      "0.7607894985362168\n",
      "0.7608120868744098\n",
      "0.7608346709470305\n",
      "0.760857250755287\n",
      "0.7608798263003871\n",
      "0.7609023975835378\n",
      "0.7608305804624823\n",
      "0.7607587768969423\n",
      "0.7607813532131735\n",
      "0.7607095678429893\n",
      "0.7607321445419379\n",
      "0.7607547169811321\n",
      "0.7607772851617772\n",
      "0.7607998490850783\n",
      "0.7608224087522399\n",
      "0.7607506601282535\n",
      "0.7606789250353607\n",
      "0.7607014897227984\n",
      "0.7607240501555577\n",
      "0.7607466063348416\n",
      "0.7606748986709397\n",
      "0.760603204524034\n",
      "0.7606257657148242\n",
      "0.7606483226535997\n",
      "0.7605766512767361\n",
      "0.7605992085924251\n",
      "0.7606217616580311\n",
      "0.7605501130369254\n",
      "0.7604784779127813\n",
      "0.7604068562817856\n",
      "0.7603352481401262\n",
      "0.7603578154425612\n",
      "0.7603803784954336\n",
      "0.7604029372999435\n",
      "0.7603313564906335\n",
      "0.7603539156626506\n",
      "0.7603764705882353\n",
      "0.7603990212685865\n",
      "0.7603274677707725\n",
      "0.7603500188182161\n",
      "0.7603725656223539\n",
      "0.7603951081843838\n",
      "0.7604176465055028\n",
      "0.7603461249059443\n",
      "0.7603686635944701\n",
      "0.7602971600526612\n",
      "0.760319699106723\n",
      "0.7602482136141406\n",
      "0.7602707530318699\n",
      "0.76029328821207\n",
      "0.7602218253595263\n",
      "0.7602443609022557\n",
      "0.7602668922093788\n",
      "0.7602894192820898\n",
      "0.7602179836512262\n",
      "0.7602405110860578\n",
      "0.7601690934711132\n",
      "0.7600976892729664\n",
      "0.7601202216586832\n",
      "0.7601427498121713\n",
      "0.760165273734623\n",
      "0.7601877934272301\n",
      "0.7602103088911839\n",
      "0.7601389410439354\n",
      "0.7601614568666103\n",
      "0.7601839684625493\n",
      "0.7601126231816049\n",
      "0.7601351351351351\n",
      "0.7601576428638454\n",
      "0.7601801463689247\n",
      "0.7601088282202834\n",
      "0.7601313320825516\n",
      "0.7600600318919426\n",
      "0.7599887450759707\n",
      "0.7600112538685173\n",
      "0.7600337584396099\n",
      "0.760056258790436\n",
      "0.7600787549221827\n",
      "0.7600074997656323\n",
      "0.759936257967754\n",
      "0.759958759021464\n",
      "0.7599812558575445\n",
      "0.7599100365476525\n",
      "0.7598388305847077\n",
      "0.7598613323339267\n",
      "0.7598838298669665\n",
      "0.7599063231850117\n",
      "0.7599288122892469\n",
      "0.7598576379132715\n",
      "0.759880127364675\n",
      "0.7598089708774229\n",
      "0.7597378277153558\n",
      "0.7597603220672222\n",
      "0.7597828122074518\n",
      "0.7598052981372274\n",
      "0.7597341819543242\n",
      "0.7597566682264857\n",
      "0.7597791502900992\n",
      "0.759801628146346\n",
      "0.7597305389221557\n",
      "0.7597530171204041\n",
      "0.7597754911131899\n",
      "0.759797960901693\n",
      "0.7598204264870931\n",
      "0.7598428878705695\n",
      "0.759865345053301\n",
      "0.7598877980364657\n",
      "0.7599102468212415\n",
      "0.7599326914088063\n",
      "0.7599551318003365\n",
      "0.7599775679970091\n",
      "0.76\n",
      "0.760022427810485\n",
      "0.7600448514296393\n",
      "0.7600672708586378\n",
      "0.7600896860986547\n",
      "0.7600186828584774\n",
      "0.7600410984494675\n",
      "0.7600635098533669\n",
      "0.7599925289503175\n",
      "0.7600149407040807\n",
      "0.7600373482726424\n",
      "0.7600597516571749\n",
      "0.7600821508588499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7601045458788388\n",
      "0.7601269367183124\n",
      "0.7601493233784414\n",
      "0.7600783874580067\n",
      "0.7600074647755902\n",
      "0.7600298563164769\n",
      "0.7600522436794477\n",
      "0.7600746268656716\n",
      "0.760003730995243\n",
      "0.7600261145308711\n",
      "0.7600484938916348\n",
      "0.760070869078702\n",
      "0.76\n",
      "0.7600223755360805\n",
      "0.760044746900345\n",
      "0.7599739000745712\n",
      "0.7599962717867462\n",
      "0.7600186393289842\n",
      "0.7600410027024509\n",
      "0.7600633619083116\n",
      "0.7600857169477313\n",
      "0.7601080678218745\n",
      "0.76003726129483\n",
      "0.7600596125186289\n",
      "0.7600819595790258\n",
      "0.7601043024771839\n",
      "0.7601266412142658\n",
      "0.7601489757914339\n",
      "0.7601713062098501\n",
      "0.7601936324706758\n",
      "0.7602159545750722\n",
      "0.7602382725241995\n",
      "0.7602605863192182\n",
      "0.760282895961288\n",
      "0.7603052014515679\n",
      "0.760327502791217\n",
      "0.7603497999813936\n",
      "0.7603720930232558\n",
      "0.7603943819179612\n",
      "0.7603236607142857\n",
      "0.760345949967451\n",
      "0.7603682350753208\n",
      "0.7602975360297536\n",
      "0.7603198214949796\n",
      "0.7603421028167705\n",
      "0.7602714259155977\n",
      "0.7602937075936426\n",
      "0.7603159851301116\n",
      "0.7603382585261593\n",
      "0.7602676082512544\n",
      "0.760289882003159\n",
      "0.7602192493496841\n",
      "0.7601486298188574\n",
      "0.7601709084153817\n",
      "0.7601003064920591\n",
      "0.7601225854383358\n",
      "0.7601448602470053\n",
      "0.7600742804085423\n",
      "0.7600037136756105\n",
      "0.7600259933160045\n",
      "0.7600482688201986\n",
      "0.7600705401893447\n",
      "0.760092807424594\n",
      "0.7601150705270973\n",
      "0.7600445392966503\n",
      "0.7600668027463351\n",
      "0.7599962890806197\n",
      "0.7600185528756958\n",
      "0.7599480567665338\n",
      "0.7598775737340011\n",
      "0.7598998423444311\n",
      "0.7599221068249258\n",
      "0.7599443671766342\n",
      "0.7599666234007046\n",
      "0.759988875498285\n",
      "0.7600111234705228\n",
      "0.7600333673185652\n",
      "0.7599629286376274\n",
      "0.7599851728292095\n",
      "0.7600074128984433\n",
      "0.7600296488464746\n",
      "0.7600518806744487\n",
      "0.7600741083835109\n",
      "0.760003705075954\n",
      "0.760025933129573\n",
      "0.7600481570661234\n",
      "0.7600703768867487\n",
      "0.76\n",
      "0.7600222201647996\n",
      "0.7600444362155157\n",
      "0.7600666481532907\n",
      "0.7599962976675305\n",
      "0.7600185099490976\n",
      "0.7600407181195632\n",
      "0.7600629221800684\n",
      "0.7600851221317543\n",
      "0.7601073179757609\n",
      "0.7601295097132285\n",
      "0.7601516973452964\n",
      "0.760173880873104\n",
      "0.7601035790252474\n",
      "0.7601257628999445\n",
      "0.7601479426722145\n",
      "0.7600776627218935\n",
      "0.7600998428399741\n",
      "0.7601220188574598\n",
      "0.7601441907754876\n",
      "0.7601663585951941\n",
      "0.7601885223177155\n",
      "0.7602106819441877\n",
      "0.7602328374757461\n",
      "0.7602549889135255\n",
      "0.7601847575057736\n",
      "0.7602069092924442\n",
      "0.7602290569871617\n",
      "0.7602512005910602\n",
      "0.7601809954751131\n",
      "0.7602031394275162\n",
      "0.7601329517126766\n",
      "0.7601550960118169\n",
      "0.760177236222653\n",
      "0.7601993723463172\n",
      "0.760221504383941\n",
      "0.7602436323366556\n",
      "0.760173479745317\n",
      "0.7601033400996494\n",
      "0.7601254728295969\n",
      "0.7601476014760148\n",
      "0.7601697260400332\n",
      "0.7601918465227818\n",
      "0.7601217375265148\n",
      "0.7601438583548507\n",
      "0.7601659751037344\n",
      "0.7601880877742947\n",
      "0.7602101963676593\n",
      "0.7602323008849557\n",
      "0.7602544013273113\n",
      "0.760184331797235\n",
      "0.7602064325868584\n",
      "0.7602285293033543\n",
      "0.7602506219478485\n",
      "0.7601805785885388\n",
      "0.760202671579917\n",
      "0.7602247605011054\n",
      "0.7602468453532283\n",
      "0.7602689261374102\n",
      "0.7602910028547748\n",
      "0.7603130755064457\n",
      "0.7602430715403738\n",
      "0.7602651445406002\n",
      "0.7601951578753567\n",
      "0.7601251840942562\n",
      "0.7601472618499769\n",
      "0.7601693355420578\n",
      "0.7601914051716205\n",
      "0.7602134707397865\n",
      "0.7601435274634281\n",
      "0.760165593376265\n",
      "0.7601876552295097\n",
      "0.7602097130242825\n",
      "0.7601397958245194\n",
      "0.7601618539635828\n",
      "0.760183908045977\n",
      "0.7602059580728209\n",
      "0.760228004045233\n",
      "0.7602500459643317\n",
      "0.7602720838312345\n",
      "0.7602941176470588\n",
      "0.7603161474129216\n",
      "0.7603381731299393\n",
      "0.7603601947992281\n",
      "0.7603822124219037\n",
      "0.7604042259990813\n",
      "0.760334374425868\n",
      "0.7603563883530816\n",
      "0.7603783982365907\n",
      "0.7603085682799156\n",
      "0.7603305785123967\n",
      "0.7603525847029657\n",
      "0.760374586852736\n",
      "0.7603965849628201\n",
      "0.7604185790343309\n",
      "0.76044056906838\n",
      "0.7603707782672541\n",
      "0.7603010002753051\n",
      "0.7603229950449624\n",
      "0.7603449857785118\n",
      "0.7603669724770642\n",
      "0.7603889551417301\n",
      "0.7604109337736196\n",
      "0.760432908373842\n",
      "0.7604548789435069\n",
      "0.7604768454837231\n",
      "0.7604988079955988\n",
      "0.760520766480242\n",
      "0.7604510451045104\n",
      "0.7604730039416995\n",
      "0.7604949587534372\n",
      "0.7604252589130236\n",
      "0.7604472140762464\n",
      "0.7604691652157977\n",
      "0.7603994868975628\n",
      "0.7604214383875401\n",
      "0.7604433858556248\n",
      "0.7604653293029221\n",
      "0.7604872687305367\n",
      "0.7605092041395732\n",
      "0.7605311355311355\n",
      "0.7605530629063273\n",
      "0.7605749862662516\n",
      "0.7605969056120113\n",
      "0.7605272793848408\n",
      "0.7605491990846682\n",
      "0.7605711147721033\n",
      "0.7605930264482474\n",
      "0.760614934114202\n",
      "0.7606368377710678\n",
      "0.7605672461116194\n",
      "0.7605891501235019\n",
      "0.7605195755579949\n",
      "0.7605414799231683\n",
      "0.7605633802816901\n",
      "0.7604938271604939\n",
      "0.7604242867593269\n",
      "0.7603547590747005\n",
      "0.7603766684951545\n",
      "0.7603985739098638\n",
      "0.7604204753199268\n",
      "0.7603509734027968\n",
      "0.7602814841893621\n",
      "0.76021200767614\n",
      "0.7602339181286549\n",
      "0.7602558245774326\n",
      "0.7601863694500274\n",
      "0.7602082762400658\n",
      "0.760230179028133\n",
      "0.7602520778153256\n",
      "0.7602739726027398\n",
      "0.7602958633914711\n",
      "0.760317750182615\n",
      "0.7603396329772665\n",
      "0.76036151177652\n",
      "0.7602921040620721\n",
      "0.7603139832055494\n",
      "0.7603358583553892\n",
      "0.7603577295126848\n",
      "0.7602883474769596\n",
      "0.7603102189781021\n",
      "0.7603320864884591\n",
      "0.7602627257799671\n",
      "0.7602845936331296\n",
      "0.7602152499087924\n",
      "0.7602371181030552\n",
      "0.760258982308955\n",
      "0.7602808425275828\n",
      "0.7603026987600292\n",
      "0.7603245510073845\n",
      "0.7602552415679125\n",
      "0.7602770941573238\n",
      "0.7602989427633977\n",
      "0.7603207873872232\n",
      "0.7603426280298888\n",
      "0.7603644646924829\n",
      "0.7603862973760933\n",
      "0.7604081260818074\n",
      "0.7604299508107123\n",
      "0.7604517715638948\n",
      "0.7604735883424408\n",
      "0.7604954011474365\n",
      "0.7604261518849026\n",
      "0.7603569152326323\n",
      "0.7603787327021122\n",
      "0.7603095129722348\n",
      "0.760331330784635\n",
      "0.7603531446254664\n",
      "0.7603749544958136\n",
      "0.7603967603967604\n",
      "0.7604185623293903\n",
      "0.7603493767628059\n",
      "0.7603711790393013\n",
      "0.7603929773492223\n",
      "0.7604147716936511\n",
      "0.7604365620736698\n",
      "0.7604583484903601\n",
      "0.760389197053742\n",
      "0.7603200581923987\n",
      "0.7602509319029003\n",
      "0.7601818181818182\n",
      "0.7602036178529225\n",
      "0.7602254135611707\n",
      "0.7602472053076433\n",
      "0.760178117048346\n",
      "0.7601999091322126\n",
      "0.7602216972560422\n",
      "0.7602434814209139\n",
      "0.760265261627907\n",
      "0.7602870378780997\n",
      "0.7603088101725703\n",
      "0.7602397602397603\n",
      "0.7602615328732292\n",
      "0.7602833015527104\n",
      "0.760305066279281\n",
      "0.7603268270540172\n",
      "0.7603485838779956\n",
      "0.7603703367522919\n",
      "0.7603920856779814\n",
      "0.7604138306561394\n",
      "0.7604355716878403\n",
      "0.7604573087741584\n",
      "0.7603883142805299\n",
      "0.7604100517100608\n",
      "0.7604317851959361\n",
      "0.7604535147392291\n",
      "0.7604752403410121\n",
      "0.7604969620023578\n",
      "0.7605186797243381\n",
      "0.7604497234563423\n",
      "0.7603807796917498\n",
      "0.7603118484271598\n",
      "0.7602429296591733\n",
      "0.7602646605637633\n",
      "0.7602863875294544\n",
      "0.7603081105573176\n",
      "0.7603298296484233\n",
      "0.7603515448038416\n",
      "0.7603732560246421\n",
      "0.7603949633118942\n",
      "0.7604166666666666\n",
      "0.7604383660900281\n",
      "0.7604600615830466\n",
      "0.7604817531467898\n",
      "0.7605034407823252\n",
      "0.7605251244907197\n",
      "0.76054680427304\n",
      "0.7604779578166018\n",
      "0.7604996379435192\n",
      "0.7605213141460766\n",
      "0.7605429864253394\n",
      "0.7605646547823727\n",
      "0.760586319218241\n",
      "0.7606079797340088\n",
      "0.76062963633074\n",
      "0.760651289009498\n",
      "0.7606729377713459\n",
      "0.7606945826173465\n",
      "0.7607162235485622\n",
      "0.7607378605660547\n",
      "0.7606690777576853\n",
      "0.7606907151252147\n",
      "0.7606219490146447\n",
      "0.7606435867305432\n",
      "0.7606652205350687\n",
      "0.7605964753727971\n",
      "0.7606181095246701\n",
      "0.7606397397668745\n",
      "0.7606613661004699\n",
      "0.7606829885265155\n",
      "0.7607046070460705\n",
      "0.7606358955830548\n",
      "0.760657514450867\n",
      "0.7605888196514043\n",
      "0.7606104388658118\n",
      "0.7606320541760723\n",
      "0.7605633802816901\n",
      "0.7605849959375283\n",
      "0.7606066076909189\n",
      "0.7605379546890514\n",
      "0.7604693140794224\n",
      "0.7604909304214421\n",
      "0.7605125428622992\n",
      "0.7605341514030497\n",
      "0.7605557560447492\n",
      "0.7605773567884528\n",
      "0.7605087497744903\n",
      "0.7605303508613692\n",
      "0.760551948051948\n",
      "0.7604833618901614\n",
      "0.760414788097385\n",
      "0.7604363898656569\n",
      "0.7604579877389109\n",
      "0.7603894347786893\n",
      "0.7603208941770326\n",
      "0.7603424966200991\n",
      "0.7603640951694304\n",
      "0.7602955753807336\n",
      "0.7603171742656335\n",
      "0.7602486710514461\n",
      "0.7601801801801802\n",
      "0.7601117016485002\n",
      "0.7600432354530715\n",
      "0.7599747815905611\n",
      "0.7599963976945245\n",
      "0.760018009905448\n",
      "0.7600396182243833\n",
      "0.7599711893400558\n",
      "0.7599927979834353\n",
      "0.76001440273652\n",
      "0.76003600360036\n",
      "0.7600576005760058\n",
      "0.7600791936645068\n",
      "0.7601007828669126\n",
      "0.7600323915781897\n",
      "0.7600539811066127\n",
      "0.7600755667506297\n",
      "0.7600971485112891\n",
      "0.7601187263896384\n",
      "0.7600503642413886\n",
      "0.7600719424460431\n",
      "0.7600935167700746\n",
      "0.7601150872145298\n",
      "0.7601366537804549\n",
      "0.760158216468896\n",
      "0.7601797752808989\n",
      "0.7601114506561208\n",
      "0.7601330097959917\n",
      "0.7601545650611071\n",
      "0.7601761164525115\n",
      "0.7601976639712489\n",
      "0.7602192076183631\n",
      "0.7602407473948976\n",
      "0.7602622833018953\n",
      "0.7602838153403988\n",
      "0.7602155365963179\n",
      "0.7602370689655172\n",
      "0.7602585974678998\n",
      "0.7602801221045071\n",
      "0.7603016428763802\n",
      "0.7602333931777379\n",
      "0.7602549142805852\n",
      "0.7602764315203734\n",
      "0.7602979448981423\n",
      "0.7603194544149318\n",
      "0.7602512337371018\n",
      "0.7601830253005563\n",
      "0.7602045393379384\n",
      "0.7602260495156081\n",
      "0.760247555834604\n",
      "0.7602690582959641\n",
      "0.7602905569007264\n",
      "0.7603120516499282\n",
      "0.7602438805702502\n",
      "0.7602653756499911\n",
      "0.7602868668758405\n",
      "0.7602187163858014\n",
      "0.7602402079412028\n",
      "0.7601720738483599\n",
      "0.7601935657316964\n",
      "0.7601254480286739\n",
      "0.7601469402383298\n",
      "0.7600788389177566\n",
      "0.7600107497984413\n",
      "0.7600322465066285\n",
      "0.7600537393640842\n",
      "0.760075228371843\n",
      "0.7600967135309393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7600286532951289\n",
      "0.760050138776972\n",
      "0.7599820948970457\n",
      "0.7600035807000268\n",
      "0.7599355531686359\n",
      "0.7598675378143739\n",
      "0.7598890281009486\n",
      "0.759910514541387\n",
      "0.7599319971367215\n",
      "0.7598640064418001\n",
      "0.75979602791197\n",
      "0.7598175149834511\n",
      "0.7598389982110912\n",
      "0.7597710401574099\n",
      "0.7597925236988017\n",
      "0.7598140033980149\n",
      "0.7598354792560801\n",
      "0.7598569512740277\n",
      "0.7597890219917754\n",
      "0.7597211048538483\n",
      "0.7597425813371469\n",
      "0.7597640539815891\n",
      "0.7596961572832887\n",
      "0.7597176302385846\n",
      "0.7597390993566834\n",
      "0.7597605646386134\n",
      "0.7597820260854029\n",
      "0.7598034836980795\n",
      "0.7598249374776705\n",
      "0.7597570777886934\n",
      "0.759778531880693\n",
      "0.7597999821412627\n",
      "0.7598214285714285\n",
      "0.7597535934291582\n",
      "0.7596857703981432\n",
      "0.7597072212800143\n",
      "0.7597286683327383\n",
      "0.7597501115573405\n",
      "0.7597715509548456\n",
      "0.7597929865262782\n",
      "0.7597251962883654\n",
      "0.7597466321705773\n",
      "0.759678858162355\n",
      "0.7597002943537597\n",
      "0.7596325365679629\n",
      "0.7596539730669758\n",
      "0.7596754057428214\n",
      "0.7596968345965225\n",
      "0.7597182596291013\n",
      "0.7597396808415797\n",
      "0.7597610982349795\n",
      "0.7597825118103217\n",
      "0.7598039215686274\n",
      "0.759825327510917\n",
      "0.7598467296382106\n",
      "0.7597790252160741\n",
      "0.759800427655025\n",
      "0.7598218262806236\n",
      "0.7598432210938891\n",
      "0.7597755411062617\n",
      "0.7597078731742073\n",
      "0.7597292724196277\n",
      "0.7597506678539626\n",
      "0.7597720594782299\n",
      "0.7597934472934473\n",
      "0.7598148313006321\n",
      "0.7598362115008012\n",
      "0.759857587894971\n",
      "0.759878960484158\n",
      "0.7599003292693779\n",
      "0.7599216942516462\n",
      "0.7599430554319779\n",
      "0.7598754448398577\n",
      "0.7598968063339561\n",
      "0.759918164027753\n",
      "0.7599395179222628\n",
      "0.7599608680184987\n",
      "0.7599822143174745\n",
      "0.7600035568202027\n",
      "0.7600248955276963\n",
      "0.7600462304409673\n",
      "0.7600675615610276\n",
      "0.7600888888888889\n",
      "0.7600213314372056\n",
      "0.7599537859936011\n",
      "0.7599751177463787\n",
      "0.7599964457081927\n",
      "0.7600177698800533\n",
      "0.7600390902629709\n",
      "0.7599715732433153\n",
      "0.7599928939420856\n",
      "0.7599253930189181\n",
      "0.7599467140319716\n",
      "0.7599680312583252\n",
      "0.7599893446989877\n",
      "0.7600106543549676\n",
      "0.7600319602272727\n",
      "0.7600532623169108\n",
      "0.7599857979762116\n",
      "0.7600071003816455\n",
      "0.7600283990060348\n",
      "0.760049693850386\n",
      "0.7600709849157055\n",
      "0.7600922722029988\n",
      "0.7601135557132718\n",
      "0.7600461279162601\n",
      "0.7600674117438354\n",
      "0.7600886917960089\n",
      "0.7601099680737851\n",
      "0.7601312405781679\n",
      "0.7600638411065792\n",
      "0.7599964535863108\n",
      "0.7599290780141844\n",
      "0.759950359010726\n",
      "0.7599716362347102\n",
      "0.75999290968714\n",
      "0.760014179369018\n",
      "0.7599468320779796\n",
      "0.7598794967216019\n",
      "0.759812173296713\n",
      "0.7598334514528703\n",
      "0.7598547258393126\n",
      "0.7598759964570416\n",
      "0.7598972633070588\n",
      "0.7599185263903648\n",
      "0.7599397857079607\n",
      "0.7599610412608465\n",
      "0.7599822930500222\n",
      "0.7600035410764873\n",
      "0.7600247853412411\n",
      "0.7600460258452824\n",
      "0.7600672625896097\n",
      "0.76\n",
      "0.75993274931422\n",
      "0.7599539904441692\n",
      "0.7599752278156242\n",
      "0.7599964614295824\n",
      "0.7600176912870411\n",
      "0.7599504687776402\n",
      "0.7599716989475546\n",
      "0.7599929253625751\n",
      "0.7600141480236979\n",
      "0.7600353669319186\n",
      "0.7600565820882327\n",
      "0.760077793493635\n",
      "0.7600990011491204\n",
      "0.7601202050556832\n",
      "0.7601414052143173\n",
      "0.7601626016260162\n",
      "0.7601837942917734\n",
      "0.7602049832125817\n",
      "0.7602261683894337\n",
      "0.7602473498233215\n",
      "0.7601801960957513\n",
      "0.7601130542307013\n",
      "0.7601342400423916\n",
      "0.7601554221123278\n",
      "0.7601766004415011\n",
      "0.7601977750309024\n",
      "0.760130661251876\n",
      "0.760151836158192\n",
      "0.7600847382822844\n",
      "0.760017652250662\n",
      "0.7600388315241373\n",
      "0.760060007059654\n",
      "0.7600811788582017\n",
      "0.7600141168166579\n",
      "0.7599470666078518\n",
      "0.7598800282286521\n",
      "0.7599012084325659\n",
      "0.7599223849003351\n",
      "0.7599435576329482\n",
      "0.7599647266313933\n",
      "0.7599858918966581\n",
      "0.7600070534297302\n",
      "0.7600282112315966\n",
      "0.760049365303244\n",
      "0.7600705156456589\n",
      "0.7600916622598273\n",
      "0.7600246761258482\n",
      "0.7600458230525202\n",
      "0.7600669662525332\n",
      "0.7600881057268722\n",
      "0.7601092414765219\n",
      "0.7601303735024665\n",
      "0.7601515018056901\n",
      "0.7601726263871763\n",
      "0.7601937472479084\n",
      "0.7601268052131032\n",
      "0.7601479263890112\n",
      "0.7601690438457475\n",
      "0.7601901575842944\n",
      "0.7602112676056338\n",
      "0.7602323739107473\n",
      "0.7602534765006161\n",
      "0.7602745753762211\n",
      "0.7602956705385427\n",
      "0.7603167619885614\n",
      "0.7603378497272567\n",
      "0.7603589337556084\n",
      "0.7603800140745953\n",
      "0.7604010906851966\n",
      "0.7604221635883905\n",
      "0.7604432327851552\n",
      "0.7604642982764686\n",
      "0.7603974325156071\n",
      "0.7604184983295235\n",
      "0.7604395604395604\n",
      "0.7603727144866386\n",
      "0.7603058802847851\n",
      "0.7603269467393216\n",
      "0.760348009491168\n",
      "0.7603690685413005\n",
      "0.760390123890695\n",
      "0.7603233175188895\n",
      "0.760256522885004\n",
      "0.7602775825720309\n",
      "0.7602986385595081\n",
      "0.7603196908484103\n",
      "0.7602529199964873\n",
      "0.7602739726027398\n",
      "0.7602950215119852\n",
      "0.7603160667251976\n",
      "0.76033710824335\n",
      "0.7603581460674157\n",
      "0.7603791801983674\n",
      "0.7604002106371774\n",
      "0.7604212373848179\n",
      "0.7604422604422605\n",
      "0.7604632798104765\n",
      "0.760484295490437\n",
      "0.7605053074831125\n",
      "0.7605263157894737\n",
      "0.7605473204104903\n",
      "0.760568321347132\n",
      "0.7605893186003683\n",
      "0.760610312171168\n",
      "0.7606313020604998\n",
      "0.7605646151148518\n",
      "0.7605856053300605\n",
      "0.7606065918653576\n",
      "0.7605399246209134\n",
      "0.7605609114811569\n",
      "0.7605818946630444\n",
      "0.7606028741675429\n",
      "0.7605362306142118\n",
      "0.7605572104433153\n",
      "0.7605781865965835\n",
      "0.7605115627189909\n",
      "0.7605325391959359\n",
      "0.7605535119985987\n",
      "0.7605744811279447\n",
      "0.7605954465849387\n",
      "0.7605288503633657\n",
      "0.760549816144283\n",
      "0.760570778254399\n",
      "0.7605917366946778\n",
      "0.7605251641137856\n",
      "0.7605461228776474\n",
      "0.7605670779732213\n",
      "0.7605005250262513\n",
      "0.7604339837256103\n",
      "0.7604549431321085\n",
      "0.7603884174612895\n",
      "0.760409377186844\n",
      "0.7604303332458672\n",
      "0.7604512856393213\n",
      "0.7603847835592479\n",
      "0.7604057362714236\n",
      "0.7604266853195768\n",
      "0.7604476307046687\n",
      "0.7604685724276598\n",
      "0.7604895104895105\n",
      "0.7605104448911808\n",
      "0.7604439783254676\n",
      "0.7604649130472778\n",
      "0.7604858441104508\n",
      "0.7604193971166449\n",
      "0.7603529617333566\n",
      "0.7603738970909408\n",
      "0.7603074772886094\n",
      "0.7603284129618307\n",
      "0.760349344978166\n",
      "0.7602829447209851\n",
      "0.7603038770520433\n",
      "0.760324805727757\n",
      "0.7603457307490833\n",
      "0.7603666521169795\n",
      "0.7603875698324022\n",
      "0.7604084838963079\n",
      "0.760342119043463\n",
      "0.7603630334235099\n",
      "0.7603839441535777\n",
      "0.760317598813367\n",
      "0.7602512650497295\n",
      "0.7602721800575766\n",
      "0.7602930914166085\n",
      "0.7603139991277802\n",
      "0.760334903192046\n",
      "0.7603558036103601\n",
      "0.7603767003836763\n",
      "0.760397593512948\n",
      "0.7603312990409764\n",
      "0.760352192485398\n",
      "0.7603730822873083\n",
      "0.7603939684476597\n",
      "0.7604148509674046\n",
      "0.7604357298474945\n",
      "0.7604566050888811\n",
      "0.7603903459092097\n",
      "0.7604112214671546\n",
      "0.7604320933879257\n",
      "0.7604529616724739\n",
      "0.760473826321749\n",
      "0.7604946873367009\n",
      "0.7605155447182792\n",
      "0.7604493207941484\n",
      "0.7604701784936874\n",
      "0.7604910325613791\n",
      "0.7605118829981719\n",
      "0.760532729805014\n",
      "0.7605535729828532\n",
      "0.760574412532637\n",
      "0.7605952484553129\n",
      "0.7606160807518274\n",
      "0.7606369094231271\n",
      "0.7606577344701584\n",
      "0.7606785558938669\n",
      "0.7606993736951984\n",
      "0.7607201878750979\n",
      "0.7607409984345104\n",
      "0.7607618053743804\n",
      "0.7607826086956522\n",
      "0.7607164594383097\n",
      "0.7607372630846809\n",
      "0.7606711292706251\n",
      "0.7606919332406119\n",
      "0.7607127335940895\n",
      "0.7607335303320006\n",
      "0.760754323455288\n",
      "0.760775112964894\n",
      "0.7607090103397341\n",
      "0.7607298001737619\n",
      "0.7607505863956215\n",
      "0.7607713690062543\n",
      "0.760705289672544\n",
      "0.7607260726072608\n",
      "0.7607468519322622\n",
      "0.760767627648489\n",
      "0.7607883997568812\n",
      "0.7608091682583782\n",
      "0.7608299331539196\n",
      "0.7608506944444444\n",
      "0.7608714521308915\n",
      "0.760892206214199\n",
      "0.760912956695305\n",
      "0.7609337035751476\n",
      "0.7609544468546637\n",
      "0.760888426166927\n",
      "0.7609091697753101\n",
      "0.7609299097848716\n",
      "0.7609506461965478\n",
      "0.760971379011275\n",
      "0.7609921082299888\n",
      "0.7610128338536247\n",
      "0.7609468481748027\n",
      "0.7609675741286631\n",
      "0.7609882964889467\n",
      "0.761009015256588\n",
      "0.7609430527866863\n",
      "0.7608771017507368\n",
      "0.7608978247681775\n",
      "0.7609185441941074\n",
      "0.7609392600294602\n",
      "0.760959972275169\n",
      "0.7609806809321666\n",
      "0.761001386001386\n",
      "0.7609354699003897\n",
      "0.7608695652173914\n",
      "0.7608036719494241\n",
      "0.760824385174922\n",
      "0.7608450948134038\n",
      "0.7607792207792208\n",
      "0.7607999307419271\n",
      "0.7608206371191135\n",
      "0.7608413399117112\n",
      "0.7608620391206509\n",
      "0.7608827347468629\n",
      "0.7609034267912772\n",
      "0.760924115254824\n",
      "0.7609448001384322\n",
      "0.7609654814430314\n",
      "0.7608996539792388\n",
      "0.7609203356111063\n",
      "0.7609410136654559\n",
      "0.7609616881432154\n",
      "0.7608958837772397\n",
      "0.7608300907911802\n",
      "0.7607643091820854\n",
      "0.7607849917869802\n",
      "0.7608056708160442\n",
      "0.7608263462702048\n",
      "0.7608470181503889\n",
      "0.7608676864575231\n",
      "0.7608883511925337\n",
      "0.7609090123563467\n",
      "0.7608432693969241\n",
      "0.7608639308855292\n",
      "0.7608845888044229\n",
      "0.7609052431545306\n",
      "0.7609258939367767\n",
      "0.7609465411520857\n",
      "0.7609671848013817\n",
      "0.7609878248855885\n",
      "0.7610084614056294\n",
      "0.7610290943624277\n",
      "0.761049723756906\n",
      "0.7610703495899871\n",
      "0.7610046607975143\n",
      "0.7610252869595237\n",
      "0.7610459095616154\n",
      "0.7609802398826473\n",
      "0.7610008628127696\n",
      "0.760935208351307\n",
      "0.7608695652173914\n",
      "0.7608901923574571\n",
      "0.7609108159392789\n",
      "0.7609314359637774\n",
      "0.7608658157985512\n",
      "0.7608864361472795\n",
      "0.7609070529401621\n",
      "0.7609276661781188\n",
      "0.7609482758620689\n",
      "0.7609688819929317\n",
      "0.7609894845716256\n",
      "0.7610100835990692\n",
      "0.7610306790761806\n",
      "0.7610512710038776\n",
      "0.7610718593830778\n",
      "0.761092444214698\n",
      "0.7611130254996554\n",
      "0.7610474631751227\n",
      "0.761068044788975\n",
      "0.7610024976315563\n",
      "0.7609369617636927\n",
      "0.760871437182468\n",
      "0.7608920268641295\n",
      "0.7609126130004304\n",
      "0.7609331955922864\n",
      "0.7609537746406129\n",
      "0.7609743501463246\n",
      "0.7609949221103365\n",
      "0.7610154905335629\n",
      "0.7610360554169177\n",
      "0.7610566167613148\n",
      "0.7609911382603458\n",
      "0.7610116999311769\n",
      "0.7610322580645161\n",
      "0.760966798554963\n",
      "0.7609873570138471\n",
      "0.760921912624699\n",
      "0.7609424714076877\n",
      "0.7609630266552021\n",
      "0.7609835783681541\n",
      "0.7610041265474553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7609387088455256\n",
      "0.760959257349149\n",
      "0.7609798023205845\n",
      "0.7609144035751118\n",
      "0.7609349488699837\n",
      "0.7609554906341296\n",
      "0.7608901108342642\n",
      "0.7609106529209622\n",
      "0.7609311914783953\n",
      "0.7609517265074729\n",
      "0.7609722580091042\n",
      "0.7609927859841978\n",
      "0.7610133104336625\n",
      "0.7610338313584063\n",
      "0.7610543487593372\n",
      "0.7610748626373627\n",
      "0.76109537299339\n",
      "0.7611158798283262\n",
      "0.7610505536005493\n",
      "0.761071060762101\n",
      "0.7610057495923797\n",
      "0.7610262570791144\n",
      "0.7610467610467611\n",
      "0.7610672614962252\n",
      "0.7610877584284121\n",
      "0.7611082518442271\n",
      "0.761128741744575\n",
      "0.7611492281303602\n",
      "0.7611697110024869\n",
      "0.761190190361859\n",
      "0.76121066620938\n",
      "0.7612311385459534\n",
      "0.7612516073724818\n",
      "0.761272072689868\n",
      "0.7612925344990144\n",
      "0.7612272883099075\n",
      "0.76124775044991\n",
      "0.7612682090831191\n",
      "0.7612886642104362\n",
      "0.7613091158327622\n",
      "0.761329563950998\n",
      "0.7613500085660442\n",
      "0.7613704496788009\n",
      "0.7613908872901679\n",
      "0.7614113214010448\n",
      "0.7614317520123309\n",
      "0.7613665553557668\n",
      "0.7613869863013699\n",
      "0.7613218046400136\n",
      "0.76125663413799\n",
      "0.7611914747924334\n",
      "0.7612119137281753\n",
      "0.7611467693624304\n",
      "0.7610816361458155\n",
      "0.7611020792333362\n",
      "0.7610369609856262\n",
      "0.7610574043972966\n",
      "0.7609923011120616\n",
      "0.7610127448464631\n",
      "0.761033185083818\n",
      "0.7610536218250236\n",
      "0.7610740550709766\n",
      "0.7610944848225738\n",
      "0.7611149110807114\n",
      "0.7611353338462854\n",
      "0.7611557531201915\n",
      "0.7611761689033251\n",
      "0.7611965811965812\n",
      "0.7611315272198957\n",
      "0.7611519398393437\n",
      "0.7611723489703495\n",
      "0.7611927546138072\n",
      "0.7612131567706109\n",
      "0.761148129164531\n",
      "0.7611685316477321\n",
      "0.7611889306457124\n",
      "0.7612093261593645\n",
      "0.7611443210930828\n",
      "0.7611647169327982\n",
      "0.7610997267759563\n",
      "0.7611201229403227\n",
      "0.7611405156223322\n",
      "0.7611609048228767\n",
      "0.7611812905428473\n",
      "0.7612016727831357\n",
      "0.7612220515446322\n",
      "0.7612424268282276\n",
      "0.7611774744027304\n",
      "0.7611978500127975\n",
      "0.7612182221463915\n",
      "0.7612385908044016\n",
      "0.7612589559877175\n",
      "0.7612793176972281\n",
      "0.7612996759338223\n",
      "0.7612347573974588\n",
      "0.7612551159618008\n",
      "0.7612754710546509\n",
      "0.7612958226768969\n",
      "0.7613161708294263\n",
      "0.7613365155131265\n",
      "0.7613568567288843\n",
      "0.7613771944775864\n",
      "0.7613975287601193\n",
      "0.7614178595773687\n",
      "0.7614381869302207\n",
      "0.7614585108195604\n",
      "0.7614788312462731\n",
      "0.7614139693356048\n",
      "0.7613491184737246\n",
      "0.7613694430250383\n",
      "0.7613046069999149\n",
      "0.761324931880109\n",
      "0.7613452532992763\n",
      "0.7613655712583007\n",
      "0.7613858857580659\n",
      "0.7613210759278175\n",
      "0.7612562771299685\n",
      "0.7612765957446809\n",
      "0.7612118117607012\n",
      "0.7612321307011573\n",
      "0.761252446183953\n",
      "0.7612727582099711\n",
      "0.7612930667800936\n",
      "0.761228308948622\n",
      "0.7611635621331972\n",
      "0.7611838748086409\n",
      "0.7612041840292542\n",
      "0.7612244897959184\n",
      "0.7612447921095145\n",
      "0.7612650909709233\n",
      "0.7612853863810253\n",
      "0.7613056783407004\n",
      "0.7612409689757756\n",
      "0.7612612612612613\n",
      "0.7611965666694994\n",
      "0.7612168592794017\n",
      "0.7612371484408191\n",
      "0.7612574341546304\n",
      "0.7612777164217144\n",
      "0.7612979952429494\n",
      "0.7613182706192134\n",
      "0.7613385425513844\n",
      "0.7612738853503185\n",
      "0.7612941576086957\n",
      "0.7613144264243865\n",
      "0.761249787739854\n",
      "0.7612700568808897\n",
      "0.7612903225806451\n",
      "0.7613105848399966\n",
      "0.7613308436598201\n",
      "0.7612662310107783\n",
      "0.7612864901561439\n",
      "0.7612218922358931\n",
      "0.7612421517054132\n",
      "0.7612624077373378\n",
      "0.7612826603325415\n",
      "0.761218084655187\n",
      "0.7612383375742154\n",
      "0.7611737766092782\n",
      "0.7611940298507462\n",
      "0.7612142796574239\n",
      "0.7612345260301848\n",
      "0.7612547689699025\n",
      "0.7611902339776195\n",
      "0.7612104772399763\n",
      "0.7612307170706899\n",
      "0.7612509534706331\n",
      "0.7612711864406779\n",
      "0.7612914159816965\n",
      "0.7612269106931029\n",
      "0.7612471405574854\n",
      "0.7612673669942392\n",
      "0.7612875900042355\n",
      "0.7613078095883449\n",
      "0.761328025747438\n",
      "0.7613482384823849\n",
      "0.7613684477940554\n",
      "0.7613886536833192\n",
      "0.7614088561510456\n",
      "0.7614290551981037\n",
      "0.7613645983238805\n",
      "0.7613847976976469\n",
      "0.7614049936521371\n",
      "0.7614251861882194\n",
      "0.7614453753067615\n",
      "0.7613809443222204\n",
      "0.7614011337676623\n",
      "0.7614213197969543\n",
      "0.7614415024109635\n",
      "0.7614616816105566\n",
      "0.7614818573965998\n",
      "0.7615020297699594\n",
      "0.7615221987315011\n",
      "0.7615423642820903\n",
      "0.7615625264225924\n",
      "0.7614981400067636\n",
      "0.7615183024769634\n",
      "0.7614539306846999\n",
      "0.761474093483222\n",
      "0.7614097363083164\n",
      "0.7614298994337869\n",
      "0.7613655568700355\n",
      "0.7613857203210815\n",
      "0.7614058803649881\n",
      "0.7614260370026189\n",
      "0.7613617165061666\n",
      "0.761381873469043\n",
      "0.761402027027027\n",
      "0.7613377248543197\n",
      "0.7613578787366999\n",
      "0.7612935911508908\n",
      "0.7612293144208038\n",
      "0.761249472351203\n",
      "0.761185210197535\n",
      "0.7611209588925466\n",
      "0.7611411208642809\n",
      "0.7611612794328635\n",
      "0.7611814345991561\n",
      "0.7611172052991309\n",
      "0.7611373607829902\n",
      "0.7611575128659411\n",
      "0.7611776615488443\n",
      "0.7611134542387178\n",
      "0.7611336032388664\n",
      "0.7611537488403475\n",
      "0.7611738910440209\n",
      "0.7611097057087444\n",
      "0.7610455311973019\n",
      "0.7610656774302336\n",
      "0.7610858202663969\n",
      "0.761105959706651\n",
      "0.7610418071476737\n",
      "0.7610619469026548\n",
      "0.7610820832631047\n",
      "0.7611022162298812\n",
      "0.7610380856083586\n",
      "0.7610582188895442\n",
      "0.7610783487784331\n",
      "0.7610984752758824\n",
      "0.7610343665768194\n",
      "0.7610544933883602\n",
      "0.7609903991915109\n",
      "0.7610105263157895\n",
      "0.7610306500505221\n",
      "0.7610507703965648\n",
      "0.7609866980973228\n",
      "0.7610068187557876\n",
      "0.761026936026936\n",
      "0.7610470499116236\n",
      "0.7610671604107053\n",
      "0.7610872675250357\n",
      "0.7610232245035342\n",
      "0.7609591922591502\n",
      "0.7608951707891637\n",
      "0.7608311600908556\n",
      "0.7607671601615074\n",
      "0.7607872823618471\n",
      "0.76080740117746\n",
      "0.7607434193928181\n",
      "0.7607635385132863\n",
      "0.7606995711763223\n",
      "0.7607196906003026\n",
      "0.760739806641446\n",
      "0.7607599193006053\n",
      "0.7607800285786332\n",
      "0.7608001344763826\n",
      "0.7608202369947055\n",
      "0.7608403361344538\n",
      "0.7608604318964793\n",
      "0.7607965047891111\n",
      "0.7607325884230867\n",
      "0.760752688172043\n",
      "0.7607727845443091\n",
      "0.7607928775407358\n",
      "0.7607289829512052\n",
      "0.7607490762512596\n",
      "0.7607691661768411\n",
      "0.7607892527287994\n",
      "0.7608093359079842\n",
      "0.7608294157152451\n",
      "0.7607655502392344\n",
      "0.7607016954843042\n",
      "0.760637851447755\n",
      "0.760657938905673\n",
      "0.7606780229923639\n",
      "0.760698103708676\n",
      "0.7607181810554576\n",
      "0.7607382550335571\n",
      "0.7606744400637531\n",
      "0.7606106357993625\n",
      "0.7606307137465403\n",
      "0.760650788326065\n",
      "0.7606708595387841\n",
      "0.7606909273855442\n",
      "0.760710991867192\n",
      "0.7607310529845741\n",
      "0.7607511107385363\n",
      "0.7606873428331936\n",
      "0.7607074008884419\n",
      "0.7607274555816292\n",
      "0.7606637056901031\n",
      "0.7605999664823194\n",
      "0.7606200251361541\n",
      "0.7606400804289544\n",
      "0.7606601323615649\n",
      "0.7605964148098509\n",
      "0.7605327079319876\n",
      "0.7605527638190954\n",
      "0.7605728163470397\n",
      "0.7605928655166638\n",
      "0.7606129113288118\n",
      "0.7606329537843268\n",
      "0.7606529928840519\n",
      "0.7606730286288297\n",
      "0.7606930610195028\n",
      "0.7607130900569133\n",
      "0.760733115741903\n",
      "0.7607531380753139\n",
      "0.7606894820517112\n",
      "0.7607095046854083\n",
      "0.7606458629632729\n",
      "0.7606658858959344\n",
      "0.7606859054788792\n",
      "0.760622281699565\n",
      "0.760642301580664\n",
      "0.7606623181133969\n",
      "0.7606823312986035\n",
      "0.76061872909699\n",
      "0.7606387425800518\n",
      "0.760658752716937\n",
      "0.7606787595084845\n",
      "0.7606987629555333\n",
      "0.7606351859590472\n",
      "0.7606551897041618\n",
      "0.7606751901061252\n",
      "0.7606951871657754\n",
      "0.7607151808839502\n",
      "0.760735171261487\n",
      "0.7607551582992231\n",
      "0.7607751419979953\n",
      "0.7607951223586402\n",
      "0.7608150993819943\n",
      "0.7608350730688935\n",
      "0.7607715430861723\n",
      "0.7607915170743926\n",
      "0.7608114877275004\n",
      "0.7607479756240086\n",
      "0.7607679465776294\n",
      "0.7607879141974794\n",
      "0.7608078784843932\n",
      "0.7608278394392055\n",
      "0.7608477970627503\n",
      "0.7607843137254902\n",
      "0.7608042716502587\n",
      "0.7607408025360808\n",
      "0.7606773440106773\n",
      "0.7606973058637084\n",
      "0.7606338615512928\n",
      "0.7606538237011091\n",
      "0.7606737825216812\n",
      "0.7606103560410239\n",
      "0.7605469401367351\n",
      "0.7605669028761984\n",
      "0.7605868622874291\n",
      "0.7606068183712594\n",
      "0.7606267711285214\n",
      "0.7606467205600467\n",
      "0.7606666666666667\n",
      "0.760603283059745\n",
      "0.7605399100149975\n",
      "0.7605598600349912\n",
      "0.7604965011662779\n",
      "0.760433152852978\n",
      "0.7604531067799434\n",
      "0.760389772632631\n",
      "0.7604097268487675\n",
      "0.7603464068615206\n",
      "0.7602830974188176\n",
      "0.7603030555324286\n",
      "0.7603230103230103\n",
      "0.7603429617913927\n",
      "0.7603629099384052\n",
      "0.7602996254681648\n",
      "0.7603195739014648\n",
      "0.7603395190147292\n",
      "0.7603594608087868\n",
      "0.7602961976869956\n",
      "0.7602329450915142\n",
      "0.7601697030197155\n",
      "0.7601896523041092\n",
      "0.7601264243533228\n",
      "0.760146373918829\n",
      "0.7600831600831601\n",
      "0.7600199567603526\n",
      "0.7599567639477841\n",
      "0.7599767209843699\n",
      "0.7599966747028015\n",
      "0.7600166251039069\n",
      "0.7600365721885131\n",
      "0.7600565159574468\n",
      "0.760076456411535\n",
      "0.7600963935516037\n",
      "0.7601163273784795\n",
      "0.7601362578929877\n",
      "0.7601561850959542\n",
      "0.760176108988204\n",
      "0.7601960295705623\n",
      "0.7602159468438539\n",
      "0.7601528112283032\n",
      "0.7601727287825942\n",
      "0.7601926430291456\n",
      "0.760129525074726\n",
      "0.7601494396014944\n",
      "0.7601693508218496\n",
      "0.7601892587366149\n",
      "0.7601261620185923\n",
      "0.7600630757739232\n",
      "0.7600829875518672\n",
      "0.7600199153597211\n",
      "0.760039827414537\n",
      "0.7599767692690617\n",
      "0.7599966815994691\n",
      "0.7600165906262961\n",
      "0.760036496350365\n",
      "0.7600563987724973\n",
      "0.7600762978935147\n",
      "0.7600961937142383\n",
      "0.7601160862354892\n",
      "0.7601359754580881\n",
      "0.7600729563919748\n",
      "0.7600928458923982\n",
      "0.7601127320954907\n",
      "0.7600497306257771\n",
      "0.7600696171059175\n",
      "0.7600066296511147\n",
      "0.7600265164070269\n",
      "0.760046399867429\n",
      "0.76006628003314\n",
      "0.7600861569049788\n",
      "0.7601060304837641\n",
      "0.760125900770314\n",
      "0.7601457677654464\n",
      "0.7600828157349897\n",
      "0.7601026830076184\n",
      "0.7601225469901466\n",
      "0.7601424076833913\n",
      "0.7601622650881695\n",
      "0.760182119205298\n",
      "0.7602019700355931\n",
      "0.7602218175798708\n",
      "0.7601589009351982\n",
      "0.7600959947037405\n",
      "0.7601158460901944\n",
      "0.7601356941916266\n",
      "0.7600728054935054\n",
      "0.7600926538716082\n",
      "0.7601124989660022\n",
      "0.760132340777502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7601521793069225\n",
      "0.7600893152497519\n",
      "0.7601091540560655\n",
      "0.7601289895816107\n",
      "0.7601488218272013\n",
      "0.7601686507936508\n",
      "0.7601884764817723\n",
      "0.760208298892379\n",
      "0.7601454665674849\n",
      "0.7601652892561983\n",
      "0.760185108668705\n",
      "0.7602049248058173\n",
      "0.7602247376683466\n",
      "0.7601619299405156\n",
      "0.7601817430813713\n",
      "0.760201552948951\n",
      "0.7601387626992648\n",
      "0.7601585728444004\n",
      "0.7600957965149888\n",
      "0.7601156069364162\n",
      "0.7601354140863678\n",
      "0.7600726552179656\n",
      "0.7600924626434409\n",
      "0.7601122667987452\n",
      "0.7600495253817582\n",
      "0.7600693298118191\n",
      "0.7600891309730131\n",
      "0.7601089288661496\n",
      "0.7601287234920373\n",
      "0.7601485148514852\n",
      "0.7601683029453016\n",
      "0.7601880877742947\n",
      "0.7602078693392724\n",
      "0.7602276476410426\n",
      "0.7601649484536083\n",
      "0.760184727032822\n",
      "0.7602045023501278\n",
      "0.7602242744063324\n",
      "0.7602440432022426\n",
      "0.7602638087386645\n",
      "0.7602835710164042\n",
      "0.7603033300362677\n",
      "0.7603230857990604\n",
      "0.7602604252513598\n",
      "0.7602801812937783\n",
      "0.7602175346077785\n",
      "0.7602372909285655\n",
      "0.7602570439940682\n",
      "0.7602767938050911\n",
      "0.7602965403624382\n",
      "0.7602339181286549\n",
      "0.7602536649645857\n",
      "0.7602734085481347\n",
      "0.7602931488801054\n",
      "0.760312885961301\n",
      "0.7602502881607114\n",
      "0.7602700255207047\n",
      "0.760289759631215\n",
      "0.7603094904930447\n",
      "0.7603292181069958\n",
      "0.7603489424738704\n",
      "0.7602863726135616\n",
      "0.7603060972599358\n",
      "0.7603258186605233\n",
      "0.760345536816125\n",
      "0.760365251727542\n",
      "0.7603849633955746\n",
      "0.7603224214508966\n",
      "0.7603421333991283\n",
      "0.7602796052631579\n",
      "0.760299317490338\n",
      "0.7603190264759085\n",
      "0.7603387322206693\n",
      "0.7603584347254193\n",
      "0.7602959309494451\n",
      "0.7603156337333552\n",
      "0.7602531437494863\n",
      "0.7602728468113084\n",
      "0.760292546634892\n",
      "0.7603122432210353\n",
      "0.760249774053077\n",
      "0.7602694709168584\n",
      "0.7602891645444837\n",
      "0.7603088549367505\n",
      "0.7602464065708419\n",
      "0.7601839684625493\n",
      "0.760203662642687\n",
      "0.7601412382985712\n",
      "0.7600788242055998\n",
      "0.7600985221674876\n",
      "0.7601182168951646\n",
      "0.7601379083894271\n",
      "0.7601575966510712\n",
      "0.7601772816808929\n",
      "0.7601148953631515\n",
      "0.7601345806663384\n",
      "0.7601542627389841\n",
      "0.7600918936659009\n",
      "0.7601115760111576\n",
      "0.7601312551271534\n",
      "0.760150931014683\n",
      "0.7601706036745407\n",
      "0.7601902731075207\n",
      "0.7602099393144169\n",
      "0.760229602296023\n",
      "0.7602492620531321\n",
      "0.7602689185865377\n",
      "0.7602885718970323\n",
      "0.7602262480531191\n",
      "0.7602459016393442\n",
      "0.7601835915088927\n",
      "0.7602032453696116\n",
      "0.7601409489469803\n",
      "0.760160603080957\n",
      "0.7601802539942647\n",
      "0.7601999016876946\n",
      "0.7602195461620381\n",
      "0.7602391874180865\n",
      "0.7602588254566304\n",
      "0.7602784602784602\n",
      "0.7602980918843666\n",
      "0.7603177202751392\n",
      "0.760337345451568\n",
      "0.7603569674144425\n",
      "0.7603765861645517\n",
      "0.760396201702685\n",
      "0.7604158140296309\n",
      "0.7604354231461777\n",
      "0.760455029053114\n",
      "0.760392798690671\n",
      "0.7604124048768514\n",
      "0.7603501881852397\n",
      "0.7603697946494314\n",
      "0.7603893979057592\n",
      "0.7604089979550103\n",
      "0.7604285947979715\n",
      "0.7604481884354298\n",
      "0.7603859993457638\n",
      "0.7604055932619184\n",
      "0.7604251839738349\n",
      "0.760444771482299\n",
      "0.7604643557880968\n",
      "0.7604839368920134\n",
      "0.760503514794834\n",
      "0.7605230894973437\n",
      "0.7605426610003269\n",
      "0.7605622293045681\n",
      "0.7605000817126981\n",
      "0.760519650298227\n",
      "0.7605392156862745\n",
      "0.7604770852054571\n",
      "0.7604149648750205\n",
      "0.7604345340194397\n",
      "0.7604540999673309\n",
      "0.7604736627194774\n",
      "0.7604115629593337\n",
      "0.7604311259900384\n",
      "0.7603690398432397\n",
      "0.7603069638337824\n",
      "0.7603265306122449\n",
      "0.760264468206677\n",
      "0.7602024159320927\n",
      "0.7602219864522974\n",
      "0.7602415537783581\n",
      "0.7602611179110567\n",
      "0.7601990861618799\n",
      "0.760218650567023\n",
      "0.760238211780062\n",
      "0.7602577698017783\n",
      "0.7601957585644372\n",
      "0.7601337574422967\n",
      "0.7601533191975208\n",
      "0.7601728777623746\n",
      "0.7601924331376386\n",
      "0.7601304525071341\n",
      "0.760150008152617\n",
      "0.760169560609766\n",
      "0.760107597000326\n",
      "0.7601271497269542\n",
      "0.7601466992665037\n",
      "0.7601662456197539\n",
      "0.7601857887874837\n",
      "0.7602053287704718\n",
      "0.7602248655694965\n",
      "0.760244399185336\n",
      "0.7602639296187683\n",
      "0.760283456870571\n",
      "0.7603029809415214\n",
      "0.7602410619757309\n",
      "0.7602605863192182\n",
      "0.7602801074831039\n",
      "0.7602182055039896\n",
      "0.7602377269396727\n",
      "0.7602572451970042\n",
      "0.7602767602767603\n",
      "0.7602962721797167\n",
      "0.7603157809066493\n",
      "0.7603352864583334\n",
      "0.760354788835544\n",
      "0.7603742880390562\n",
      "0.7603937840696444\n",
      "0.7604132769280834\n",
      "0.7603514195070366\n",
      "0.7603709126403123\n",
      "0.760390402602684\n",
      "0.7604098893949252\n",
      "0.7604293730178092\n",
      "0.7603675394373068\n",
      "0.7603870233352306\n",
      "0.7603252032520326\n",
      "0.7602633932200634\n",
      "0.7602828808323849\n",
      "0.760221084288385\n",
      "0.7601592977893368\n",
      "0.7600975213327915\n",
      "0.760035754916301\n",
      "0.7600552531079873\n",
      "0.7600747481312967\n",
      "0.7600942399870014\n",
      "0.7601137286758732\n",
      "0.7601332141986841\n",
      "0.7601526965562053\n",
      "0.7601721757492081\n",
      "0.7601916517784635\n",
      "0.7602111246447422\n",
      "0.7602305943488146\n",
      "0.7601688722903305\n",
      "0.7601883422633544\n",
      "0.760207809075412\n",
      "0.7602272727272728\n",
      "0.7601655709763818\n",
      "0.760103879240383\n",
      "0.7601233465876815\n",
      "0.7601428107757222\n",
      "0.7600811359026369\n",
      "0.760100600356969\n",
      "0.7601200616532814\n",
      "0.7601395197923426\n",
      "0.7601589747749209\n",
      "0.7601784266017843\n",
      "0.7601167788500527\n",
      "0.7601362309438858\n",
      "0.7601556798832401\n",
      "0.7601751256688828\n",
      "0.7601945683015808\n",
      "0.7602140077821011\n",
      "0.7602334441112102\n",
      "0.760171826876317\n",
      "0.7601912634735392\n",
      "0.7602106969205835\n",
      "0.7602301272182157\n",
      "0.7602495543672014\n",
      "0.7601879607874908\n",
      "0.7602073882047958\n",
      "0.7602268124746862\n",
      "0.7602462335979264\n",
      "0.7601846602413542\n",
      "0.7602040816326531\n",
      "0.7602234998785327\n",
      "0.7601619433198381\n",
      "0.7601003967290098\n",
      "0.7601198186528497\n",
      "0.7601392374322027\n",
      "0.7600777076250607\n",
      "0.7600161877782274\n",
      "0.7600356102298479\n",
      "0.7600550295379137\n",
      "0.7600744457031883\n",
      "0.7600938587264342\n",
      "0.7601132686084142\n",
      "0.7601326753498908\n",
      "0.760152078951626\n",
      "0.7601714794143816\n",
      "0.7601908767389195\n",
      "0.7602102709260008\n",
      "0.7601487950832929\n",
      "0.7601681895366702\n",
      "0.7601067270375161\n",
      "0.7601261217560029\n",
      "0.7601455133387227\n",
      "0.760164901786436\n",
      "0.760184287099903\n",
      "0.7601228481370726\n",
      "0.7601422337158559\n",
      "0.7601616161616161\n",
      "0.7601001939237233\n",
      "0.7601195766340794\n",
      "0.7601389562126353\n",
      "0.7601583326601502\n",
      "0.7601777059773829\n",
      "0.7601970761650917\n",
      "0.7602164432240349\n",
      "0.7602358071549705\n",
      "0.7601744186046512\n",
      "0.7601937828017763\n",
      "0.7602131438721137\n",
      "0.7602325018164204\n",
      "0.7601711333548595\n",
      "0.7601904915650981\n",
      "0.7602098466505246\n",
      "0.7602291986118958\n",
      "0.7601678502259522\n",
      "0.7601065117404987\n",
      "0.7601258673551718\n",
      "0.7600645421540944\n",
      "0.7600032268473701\n",
      "0.7600225861095427\n",
      "0.7600419422487498\n",
      "0.7600612952657473\n",
      "0.7600806451612904\n",
      "0.7600999919361342\n",
      "0.7600387034349299\n",
      "0.7600580504716601\n",
      "0.7600773943889068\n",
      "0.7600161225312374\n",
      "0.7600354667096566\n",
      "0.7600548077698074\n",
      "0.7600741457124436\n",
      "0.7600934805383189\n",
      "0.7601128122481869\n",
      "0.7600515671581661\n",
      "0.7600708991298744\n",
      "0.7600096672842988\n",
      "0.7600289995166747\n",
      "0.760048328634716\n",
      "0.7600676546391752\n",
      "0.7600869775308046\n",
      "0.760106297310356\n",
      "0.7601256139785812\n",
      "0.7601449275362319\n",
      "0.7601642379840593\n",
      "0.7601835453228144\n",
      "0.760202849553248\n",
      "0.7602221506761108\n",
      "0.7602414486921529\n",
      "0.7602607436021246\n",
      "0.7601995654622998\n",
      "0.7601383971676858\n",
      "0.7600772387159064\n",
      "0.7600965406275141\n",
      "0.7601158394336739\n",
      "0.7600546975546976\n",
      "0.7600739966218933\n",
      "0.760093292584848\n",
      "0.7601125854443104\n",
      "0.7601318752010293\n",
      "0.7600707566133312\n",
      "0.7600900466312912\n",
      "0.7601093335477128\n",
      "0.760128617363344\n",
      "0.7600675186882083\n",
      "0.7600868027648288\n",
      "0.760106083741863\n",
      "0.7600450016072002\n",
      "0.7600642828445159\n",
      "0.7600835609834485\n",
      "0.7601028360247449\n",
      "0.7601221079691517\n",
      "0.7600610490802474\n",
      "0.76\n",
      "0.759938960726046\n",
      "0.7599582396402185\n",
      "0.7599775154581225\n",
      "0.7599967881805042\n",
      "0.7599357687675632\n",
      "0.7599550417469493\n",
      "0.7599743116320141\n",
      "0.7599133087172901\n",
      "0.7599325788586564\n",
      "0.7598715890850722\n",
      "0.7598908594815825\n",
      "0.7599101267854277\n",
      "0.7598491534943432\n",
      "0.7598684210526315\n",
      "0.7598074608904933\n",
      "0.7597465105085833\n",
      "0.7597657816635919\n",
      "0.7597048444016683\n",
      "0.7597241158072019\n",
      "0.7597433841218926\n",
      "0.7596824633148905\n",
      "0.7596215522771007\n",
      "0.7596408241802293\n",
      "0.7596600929934263\n",
      "0.7596793587174349\n",
      "0.7596986213529977\n",
      "0.7597178809008576\n",
      "0.7596569963135118\n",
      "0.7596762561102652\n",
      "0.7596153846153846\n",
      "0.7596346446598831\n",
      "0.7596539016183304\n",
      "0.7595930465432988\n",
      "0.7596123037487985\n",
      "0.7596315578694434\n",
      "0.7596508089059747\n",
      "0.7596700568591335\n",
      "0.7596893017296604\n",
      "0.7597085435182961\n",
      "0.7597277822257806\n",
      "0.7596669602113522\n",
      "0.7596861991674672\n",
      "0.7597054350436244\n",
      "0.7597246678405635\n",
      "0.7596638655462185\n",
      "0.7596830985915493\n",
      "0.759622309354245\n",
      "0.7596415426468235\n",
      "0.7595807664613169\n",
      "0.75952\n",
      "0.7595392368610511\n",
      "0.7595584706446968\n",
      "0.7595777013516756\n",
      "0.7595169545745362\n",
      "0.7595361855257897\n",
      "0.7594754517831441\n",
      "0.7594147277524587\n",
      "0.7593540134314039\n",
      "0.7593732512590935\n",
      "0.759392486011191\n",
      "0.7593317880265367\n",
      "0.7592710997442456\n",
      "0.7592903380484296\n",
      "0.7593095732779287\n",
      "0.7593288054334798\n",
      "0.7593480345158198\n",
      "0.759367260525685\n",
      "0.7593065984981626\n",
      "0.7593258247463855\n",
      "0.7593450479233227\n",
      "0.7593642680297101\n",
      "0.7593834850662834\n",
      "0.7594026990337779\n",
      "0.7594219099329288\n",
      "0.7594411177644711\n",
      "0.7594603225291394\n",
      "0.7594795242276683\n",
      "0.7594987228607918\n",
      "0.7595179184292441\n",
      "0.759537110933759\n",
      "0.7595563003750698\n",
      "0.7594956910309607\n",
      "0.7594350913588127\n",
      "0.7594542843465774\n",
      "0.7594734742720383\n",
      "0.7594128908742821\n",
      "0.7594320810401213\n",
      "0.7594512681448397\n",
      "0.7593907010128399\n",
      "0.7593301435406699\n",
      "0.7593493341838768\n",
      "0.7593685217668633\n",
      "0.7593877062903611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.759406887755102\n",
      "0.7594260661618174\n",
      "0.7594452415112386\n",
      "0.7594644138040966\n",
      "0.7594038890659867\n",
      "0.7593433739740219\n",
      "0.7592828685258964\n",
      "0.7593020476456059\n",
      "0.759321223709369\n",
      "0.7592607344857802\n",
      "0.759279910785407\n",
      "0.7592990840302668\n",
      "0.7593182542210896\n",
      "0.7592577845026678\n",
      "0.7592769549291288\n",
      "0.759216498128832\n",
      "0.759156050955414\n",
      "0.7591752249024759\n",
      "0.7591147906384333\n",
      "0.7591339648173207\n",
      "0.7591531359439669\n",
      "0.7591723040191006\n",
      "0.7591914690434506\n",
      "0.7591310575316305\n",
      "0.7591502227880331\n",
      "0.7591693849948286\n",
      "0.7591885441527446\n",
      "0.7591281520960942\n",
      "0.7590677696468342\n",
      "0.7590073968026724\n",
      "0.7590265627485288\n",
      "0.7589662027833002\n",
      "0.758985368956743\n",
      "0.7590045320823726\n",
      "0.7590236921609159\n",
      "0.7590428491930996\n",
      "0.7590620031796502\n",
      "0.7590811541212941\n",
      "0.759100302018757\n",
      "0.7590399745688627\n",
      "0.7590591226954864\n",
      "0.7590782677791021\n",
      "0.7590179564595583\n",
      "0.7590371017716692\n",
      "0.7590562440419447\n",
      "0.7590753832711097\n",
      "0.7590945194598888\n",
      "0.7591136526090064\n",
      "0.7591327827191868\n",
      "0.7591519097911538\n",
      "0.7591710338256312\n",
      "0.7591107582373958\n",
      "0.7591298825023817\n",
      "0.7590696197507343\n",
      "0.7590093665661216\n",
      "0.7590284943249465\n",
      "0.7590476190476191\n",
      "0.7590667407348624\n",
      "0.7590858593873988\n",
      "0.759104975005951\n",
      "0.7591240875912408\n",
      "0.7591431971439905\n",
      "0.7591623036649214\n",
      "0.7591814071547553\n",
      "0.7592005076142132\n",
      "0.7592196050440162\n",
      "0.759238699444885\n",
      "0.7592577908175402\n",
      "0.7592768791627021\n",
      "0.7592959644810909\n",
      "0.7593150467734263\n",
      "0.759334126040428\n",
      "0.7593532022828154\n",
      "0.7592930173575335\n",
      "0.7593120938342051\n",
      "0.7593311672874238\n",
      "0.759270998415214\n",
      "0.7592900721020521\n",
      "0.759309142766598\n",
      "0.7592489899390003\n",
      "0.7592680608365019\n",
      "0.7592871287128713\n",
      "0.7593061935688262\n",
      "0.7593252554050843\n",
      "0.759344314222363\n",
      "0.7593633700213793\n",
      "0.7593824228028504\n",
      "0.7594014725674927\n",
      "0.7594205193160228\n",
      "0.7594395630491569\n",
      "0.7594586037676112\n",
      "0.7594776414721013\n",
      "0.759417537195315\n",
      "0.7594365751365039\n",
      "0.7594556100648837\n",
      "0.7594746419811694\n",
      "0.7594145569620253\n",
      "0.7594335891147852\n",
      "0.7594526182566049\n",
      "0.759471644388199\n",
      "0.7594906675102816\n",
      "0.7595096876235666\n",
      "0.7594496283409774\n",
      "0.7594686486913893\n",
      "0.7594876660341556\n",
      "0.7595066803699897\n",
      "0.7595256916996047\n",
      "0.7594656548889416\n",
      "0.7594056275687638\n",
      "0.7594246423773018\n",
      "0.7593646277856804\n",
      "0.7593836428289213\n",
      "0.7593236409608091\n",
      "0.7593426562376551\n",
      "0.7593616685100332\n",
      "0.7593016825973615\n",
      "0.7592417061611374\n",
      "0.7591817391991154\n",
      "0.7592007581740642\n",
      "0.7592197741451473\n",
      "0.7592387871130765\n",
      "0.759257797078563\n",
      "0.759276804042318\n",
      "0.7592168627141391\n",
      "0.7591569308493843\n",
      "0.7591759412739758\n",
      "0.7591949486977111\n",
      "0.7592139531213006\n",
      "0.7592329545454546\n",
      "0.759251952970883\n",
      "0.7592709483982957\n",
      "0.7592110453648915\n",
      "0.7591511517828968\n",
      "0.7591701506665615\n",
      "0.7591891465530841\n",
      "0.7592081394431738\n",
      "0.7592271293375394\n",
      "0.7591672581026733\n",
      "0.7591862482258319\n",
      "0.7591263896554443\n",
      "0.7591453800063072\n",
      "0.7591643673630272\n",
      "0.7591833517263125\n",
      "0.7592023330968708\n",
      "0.7592213114754098\n",
      "0.7592402868626369\n",
      "0.7592592592592593\n",
      "0.7592782286659838\n",
      "0.7592971950835172\n",
      "0.759316158512566\n",
      "0.7593351189538364\n",
      "0.7593540764080347\n",
      "0.7593730308758664\n",
      "0.7593132235961251\n",
      "0.7592534257363365\n",
      "0.7592723836522561\n",
      "0.7592913385826772\n",
      "12700\n",
      "tensor(9643)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12700, tensor(9643))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_per_example_mask(model_syn, mult_mask) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
