{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 \n",
    "import itertools\n",
    "\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "from vgg16_2 import VGG_16_2\n",
    "\n",
    "\n",
    "from data_loader_withid_single_label_hair import get_loader\n",
    "from data_loader_withid_single_label_hair import get_dataset\n",
    "\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import time\n",
    "import shutil\n",
    "import os\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "celeba_image_dir = '/home/name/celeba/stargan/data/celeba/images'\n",
    "attr_path = '/home/name/celeba/stargan/data/celeba/list_attr_celeba.txt'\n",
    "id_path = '/home/name/celeba/stargan/data/celeba/identity_CelebA.txt'\n",
    "selected_attrs = ['Black_Hair']\n",
    "celeba_crop_size = 224\n",
    "image_size = 224 \n",
    "batch_size = 30\n",
    "num_workers = 0\n",
    "mode1 = 'train' \n",
    "mode2 = 'test'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #\"cuda:0\" if torch.cuda.is_available() else \n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preprocessing the CelebA dataset...\n",
      "Finished preprocessing the CelebA dataset...\n"
     ]
    }
   ],
   "source": [
    "celeba_loader_train, data_train = get_loader(celeba_image_dir, attr_path, id_path, selected_attrs,celeba_crop_size, image_size, batch_size,'CelebA', mode1, num_workers)\n",
    "celeba_loader_test, data_test = get_loader(celeba_image_dir, attr_path, id_path, selected_attrs,celeba_crop_size, image_size, batch_size,'CelebA', mode2, num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preprocessing the CelebA dataset...\n"
     ]
    }
   ],
   "source": [
    "celeba_loader_test2, data_test2 = get_loader(celeba_image_dir, attr_path, id_path, selected_attrs,celeba_crop_size, image_size, 1,'CelebA', mode2, num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87293\n",
      "9650\n"
     ]
    }
   ],
   "source": [
    "print(len(data_train))\n",
    "print(len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_single_label(model):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    size =0\n",
    "    for im, labels in celeba_loader_test:\n",
    "        im, labels = im.to(device), labels.to(device)\n",
    "        labels = labels.long().squeeze(1)\n",
    "        preds, _ = model(im)\n",
    "        values, indices = preds.max(-1)\n",
    "        correct += indices.eq(labels).sum()\n",
    "\n",
    "        \n",
    "        del im\n",
    "        del preds\n",
    "        \n",
    "        del values\n",
    "        \n",
    "        print(correct)\n",
    "        size += len(labels)\n",
    "        del labels\n",
    "        print(float(correct)/float(size))\n",
    "        \n",
    "    print(size)\n",
    "    print(correct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_single_label_stoch(model):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    size =0\n",
    "    for im, labels in celeba_loader_test2:\n",
    "        im, labels = im.to(device), labels.to(device)\n",
    "        labels = labels.long().squeeze(1)\n",
    "        preds, _ = model(im)\n",
    "        values, indices = preds.max(-1)\n",
    "        correct += indices.eq(labels).sum()\n",
    "\n",
    "        \n",
    "        del im\n",
    "        del preds\n",
    "        \n",
    "        del values\n",
    "        \n",
    "        #print(correct)\n",
    "        size += len(labels)\n",
    "        del labels\n",
    "        print(float(correct)/float(size))\n",
    "        \n",
    "    print(size)\n",
    "    print(correct)\n",
    "    return size,correct\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noisy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NoisyActivation(nn.Module):\n",
    "    def __init__(self,  given_locs, given_scales, min_scale, max_scale):\n",
    "        super(NoisyActivation, self).__init__()\n",
    "        size = given_scales.shape\n",
    "        self.min_scale = min_scale\n",
    "        self.max_scale = max_scale\n",
    "        self.given_locs = given_locs \n",
    "        self.given_scales = given_scales\n",
    "        self.locs = nn.Parameter(torch.Tensor(size).copy_(self.given_locs))         \n",
    "        self.rhos = nn.Parameter(torch.ones(size)-5) #-inf\n",
    "\n",
    "\n",
    "        self.normal = torch.distributions.normal.Normal(0,1)\n",
    "        self.rhos.requires_grad = True\n",
    "        self.locs.requires_grad = True\n",
    "        \n",
    "    def scales(self):\n",
    "        return (1.0 +torch.tanh(self.rhos))/2*(self.max_scale-self.min_scale) +self.min_scale             \n",
    "    \n",
    "    def sample_noise(self, mask):\n",
    "        epsilon = self.normal.sample(self.rhos.shape)*mask\n",
    "        return  self.locs + self.scales()*epsilon\n",
    "                                 \n",
    "                               \n",
    "                            \n",
    "    def forward(self, input, mask):\n",
    "        noise = self.sample_noise(mask)\n",
    "        return (input)*mask + noise\n",
    "\n",
    "\n",
    "\n",
    "class vgg_syn(nn.Module):\n",
    "\n",
    "    def __init__(self, model_features, model_classifier, min_scale,max_scale, given_locs, given_scale):\n",
    "        super(vgg_syn, self).__init__()\n",
    "        \n",
    "\n",
    "        self.intermed = NoisyActivation( given_locs, given_scale, min_scale, max_scale)\n",
    "        self.model_pt2 =  torch.nn.Sequential(*(list(model_features)))\n",
    "        self.model_pt3 = model_classifier\n",
    "        #self.components = components\n",
    "        for child in itertools.chain(self.model_pt2, self.model_pt3): #self.model_pt2 #(self.model_pt2, \n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "            if isinstance(child, nn.modules.batchnorm._BatchNorm):\n",
    "                child.eval()\n",
    "                child.affine = False\n",
    "                child.track_running_stats = False\n",
    "                \n",
    "        self.intermed.rhos.reuires_grad = True\n",
    "        self.intermed.locs.reuires_grad = True\n",
    "                                 \n",
    "    def forward(self, img, mask):\n",
    "                                 \n",
    "        img = img\n",
    "        x = self.intermed(img, mask)\n",
    "        noisy = x.detach()\n",
    "       \n",
    "        x = self.model_pt2(x)                    \n",
    "        x = x.view(img.size(0), -1)\n",
    "        x = self.model_pt3(x)                                 \n",
    "\n",
    "        return x, noisy\n",
    "    \n",
    "                                 \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_adversary(model, model_syn):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    size =0\n",
    "    for im, labels in celeba_loader_test2:\n",
    "        im, labels = im.to(device), labels.to(device)\n",
    "        labels = labels.long().squeeze(1)\n",
    "        _, noisy = model_syn(im)\n",
    "        preds = model(noisy)\n",
    "        values, indices = preds.max(-1)\n",
    "        correct += indices.eq(labels).sum()\n",
    "\n",
    "        del im\n",
    "        del preds\n",
    "\n",
    "        del values\n",
    "\n",
    "        #print(correct)\n",
    "        size += len(labels)\n",
    "        del labels\n",
    "        print(float(correct)/float(size))\n",
    "\n",
    "    print(size)\n",
    "    print(correct)\n",
    "    return size,correct\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_mask(model, mask):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    size =0\n",
    "    for im, labels in celeba_loader_test:\n",
    "        im, labels = im.to(device), labels.to(device)\n",
    "        labels = labels.long().squeeze(1)\n",
    "        im = im*mask\n",
    "        preds = model(im)\n",
    "        values, indices = preds.max(-1)\n",
    "        correct += indices.eq(labels).sum()\n",
    "\n",
    "        \n",
    "        del im\n",
    "        del preds\n",
    "        \n",
    "        del values\n",
    "        \n",
    "        print(correct)\n",
    "        size += len(labels)\n",
    "        del labels\n",
    "        print(float(correct)/float(size))\n",
    "        \n",
    "    print(size)\n",
    "    print(correct)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_per_example_mask(model, mult_mask):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    size =0\n",
    "    for im, labels in celeba_loader_test2:\n",
    "        im, labels = im.to(device), labels.to(device)\n",
    "        labels = labels.long().squeeze(1)\n",
    "        im_2 = im*mult_mask\n",
    "        preds = model(im_2)\n",
    "        values, indices = preds.max(-1)\n",
    "        correct += indices.eq(labels).sum()\n",
    "\n",
    "        \n",
    "        del im\n",
    "        del preds\n",
    "        #del im_2\n",
    "        del values\n",
    "        \n",
    "        size += len(labels)\n",
    "        del labels\n",
    "        print(float(correct)/float(size))\n",
    "        \n",
    "    print(size)\n",
    "    print(correct)\n",
    "    return size,correct\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Train   noisy adversary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adversary(model, model_syn, criterion, optimizer):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    size = 0\n",
    "    correcta = 0\n",
    "    model_syn.eval()\n",
    "    for i, (im, labels) in enumerate(celeba_loader_train):\n",
    "        #if (i ==1):\n",
    "         #   print (labels)\n",
    "        print(\"iteration no\", i)\n",
    "        im, labels = im.to(device), labels.to(device)\n",
    "        labels = labels.long().squeeze(1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        _, noisy = model_syn(im)\n",
    "        output = model(noisy)\n",
    "        \n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        del im\n",
    "        del noisy\n",
    "        del _\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print (loss, \"loss\")\n",
    "        #preds = model(im)\n",
    "        values, indices = output.max(-1)\n",
    "        size += len(labels)\n",
    "        correcta += indices.eq(labels).sum()\n",
    "        correct = indices.eq(labels).sum()\n",
    "\n",
    "        \n",
    "        print(float(correcta)/float(size))\n",
    "            \n",
    "            \n",
    "    print(correcta, \"correct out of all\")\n",
    "    print(size)\n",
    "    return correct, size\n",
    "        #print(list(filter(lambda p: p.requires_grad, model.parameters()))[0][0])\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adversary_mask(model, model_syn, criterion, optimizer, mask):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    size = 0\n",
    "    correcta = 0\n",
    "    model_syn.eval()\n",
    "    for i, (im, labels) in enumerate(celeba_loader_train):\n",
    "  \n",
    "        print(\"iteration no\", i)\n",
    "        im, labels = im.to(device), labels.to(device)\n",
    "        labels = labels.long().squeeze(1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        _, noisy = model_syn(im, mask)\n",
    "        output = model(noisy)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        del im\n",
    "        del noisy\n",
    "        del _\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print (loss, \"loss\")\n",
    "        #preds = model(im)\n",
    "        values, indices = output.max(-1)\n",
    "        size += len(labels)\n",
    "        correcta += indices.eq(labels).sum()\n",
    "        correct = indices.eq(labels).sum()\n",
    "\n",
    "        \n",
    "        print(float(correcta)/float(size))\n",
    "            \n",
    "            \n",
    "    print(correcta, \"correct out of all\")\n",
    "    print(size)\n",
    "    return correct, size\n",
    "        #print(list(filter(lambda p: p.requires_grad, model.parameters()))[0][0])\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_hair = VGG_16_2()\n",
    "\n",
    "model_hair.load_state_dict(torch.load(\"celeba-black-hair-2-class-84l4\",  map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VGG_16_2()\n",
    "\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "mus = torch.zeros((3,224,224))\n",
    "scale = torch.ones((3,224,224))*0.001\n",
    "model_syn = vgg_syn(model.convnet, model.fc ,0, 5 ,mus, scale )\n",
    "model_syn.load_state_dict(torch.load(\"celeba-smiling-avg-cloak\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=4096, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for child in itertools.chain(model_hair.fc, model_hair.convnet): #self.model_pt2 #(self.model_pt2, \n",
    "    for param in child.parameters():\n",
    "        param.requires_grad = True\n",
    "print(model_hair.fc[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "d = list(filter(lambda p: p.requires_grad, model_hair.parameters()))\n",
    "print(len(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mult_mask = torch.where(model_syn.intermed.scales()>4.5, torch.zeros(model_syn.intermed.scales().shape), torch.ones(model_syn.intermed.scales().shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration no 0\n",
      "tensor(0.9223, grad_fn=<NllLossBackward>) loss\n",
      "0.5333333333333333\n",
      "iteration no 1\n",
      "tensor(0.6953, grad_fn=<NllLossBackward>) loss\n",
      "0.5166666666666667\n",
      "iteration no 2\n",
      "tensor(1.1526, grad_fn=<NllLossBackward>) loss\n",
      "0.5111111111111111\n",
      "iteration no 3\n",
      "tensor(0.6658, grad_fn=<NllLossBackward>) loss\n",
      "0.5416666666666666\n",
      "iteration no 4\n",
      "tensor(0.6889, grad_fn=<NllLossBackward>) loss\n",
      "0.5466666666666666\n",
      "iteration no 5\n",
      "tensor(0.6768, grad_fn=<NllLossBackward>) loss\n",
      "0.5555555555555556\n",
      "iteration no 6\n",
      "tensor(0.6848, grad_fn=<NllLossBackward>) loss\n",
      "0.5571428571428572\n",
      "iteration no 7\n",
      "tensor(0.7169, grad_fn=<NllLossBackward>) loss\n",
      "0.5458333333333333\n",
      "iteration no 8\n",
      "tensor(0.7029, grad_fn=<NllLossBackward>) loss\n",
      "0.5407407407407407\n",
      "iteration no 9\n",
      "tensor(0.7386, grad_fn=<NllLossBackward>) loss\n",
      "0.52\n",
      "iteration no 10\n",
      "tensor(0.6814, grad_fn=<NllLossBackward>) loss\n",
      "0.5272727272727272\n",
      "iteration no 11\n",
      "tensor(0.7049, grad_fn=<NllLossBackward>) loss\n",
      "0.5138888888888888\n",
      "iteration no 12\n",
      "tensor(0.6933, grad_fn=<NllLossBackward>) loss\n",
      "0.5128205128205128\n",
      "iteration no 13\n",
      "tensor(0.6844, grad_fn=<NllLossBackward>) loss\n",
      "0.5285714285714286\n",
      "iteration no 14\n",
      "tensor(0.7065, grad_fn=<NllLossBackward>) loss\n",
      "0.52\n",
      "iteration no 15\n",
      "tensor(0.6760, grad_fn=<NllLossBackward>) loss\n",
      "0.5270833333333333\n",
      "iteration no 16\n",
      "tensor(0.7425, grad_fn=<NllLossBackward>) loss\n",
      "0.5117647058823529\n",
      "iteration no 17\n",
      "tensor(0.6618, grad_fn=<NllLossBackward>) loss\n",
      "0.5222222222222223\n",
      "iteration no 18\n",
      "tensor(0.7219, grad_fn=<NllLossBackward>) loss\n",
      "0.5140350877192983\n",
      "iteration no 19\n",
      "tensor(0.7021, grad_fn=<NllLossBackward>) loss\n",
      "0.5116666666666667\n",
      "iteration no 20\n",
      "tensor(0.6957, grad_fn=<NllLossBackward>) loss\n",
      "0.5111111111111111\n",
      "iteration no 21\n",
      "tensor(0.6871, grad_fn=<NllLossBackward>) loss\n",
      "0.5136363636363637\n",
      "iteration no 22\n",
      "tensor(0.6910, grad_fn=<NllLossBackward>) loss\n",
      "0.5144927536231884\n",
      "iteration no 23\n",
      "tensor(0.7081, grad_fn=<NllLossBackward>) loss\n",
      "0.5069444444444444\n",
      "iteration no 24\n",
      "tensor(0.6865, grad_fn=<NllLossBackward>) loss\n",
      "0.512\n",
      "iteration no 25\n",
      "tensor(0.6973, grad_fn=<NllLossBackward>) loss\n",
      "0.5076923076923077\n",
      "iteration no 26\n",
      "tensor(0.6937, grad_fn=<NllLossBackward>) loss\n",
      "0.5061728395061729\n",
      "iteration no 27\n",
      "tensor(0.6928, grad_fn=<NllLossBackward>) loss\n",
      "0.5083333333333333\n",
      "iteration no 28\n",
      "tensor(0.6909, grad_fn=<NllLossBackward>) loss\n",
      "0.5103448275862069\n",
      "iteration no 29\n",
      "tensor(0.7031, grad_fn=<NllLossBackward>) loss\n",
      "0.5055555555555555\n",
      "iteration no 30\n",
      "tensor(0.7014, grad_fn=<NllLossBackward>) loss\n",
      "0.5021505376344086\n",
      "iteration no 31\n",
      "tensor(0.7005, grad_fn=<NllLossBackward>) loss\n",
      "0.49895833333333334\n",
      "iteration no 32\n",
      "tensor(0.7017, grad_fn=<NllLossBackward>) loss\n",
      "0.49393939393939396\n",
      "iteration no 33\n",
      "tensor(0.6900, grad_fn=<NllLossBackward>) loss\n",
      "0.49901960784313726\n",
      "iteration no 34\n",
      "tensor(0.6934, grad_fn=<NllLossBackward>) loss\n",
      "0.4980952380952381\n",
      "iteration no 35\n",
      "tensor(0.6935, grad_fn=<NllLossBackward>) loss\n",
      "0.4962962962962963\n",
      "iteration no 36\n",
      "tensor(0.6932, grad_fn=<NllLossBackward>) loss\n",
      "0.4963963963963964\n",
      "iteration no 37\n",
      "tensor(0.6945, grad_fn=<NllLossBackward>) loss\n",
      "0.493859649122807\n",
      "iteration no 38\n",
      "tensor(0.6919, grad_fn=<NllLossBackward>) loss\n",
      "0.49743589743589745\n",
      "iteration no 39\n",
      "tensor(0.6947, grad_fn=<NllLossBackward>) loss\n",
      "0.495\n",
      "iteration no 40\n",
      "tensor(0.6919, grad_fn=<NllLossBackward>) loss\n",
      "0.4975609756097561\n",
      "iteration no 41\n",
      "tensor(0.6907, grad_fn=<NllLossBackward>) loss\n",
      "0.5007936507936508\n",
      "iteration no 42\n",
      "tensor(0.6954, grad_fn=<NllLossBackward>) loss\n",
      "0.4992248062015504\n",
      "iteration no 43\n",
      "tensor(0.6933, grad_fn=<NllLossBackward>) loss\n",
      "0.49924242424242427\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-972c6a077d72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_hair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mcor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_adversary_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_hair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_syn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmult_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-da91a2cf1cf4>\u001b[0m in \u001b[0;36mtrain_adversary_mask\u001b[0;34m(model, model_syn, criterion, optimizer, mask)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcorrecta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel_syn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mceleba_loader_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iteration no\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-environments/py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-environments/py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-environments/py36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-environments/py36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code-neurips20/exp2-adversary/data_loader_withid_single_label_hair.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m#print(filename, label)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-environments/py36/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2842\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2843\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2844\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr =  0.0001  \n",
    "for epoch in range (1): # real\n",
    "   \n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model_hair.parameters()), lr=lr, weight_decay=0)\n",
    "    cor, size = train_adversary_mask(model_hair,model_syn, criterion, optimizer, mult_mask)\n",
    "    print (cor,size)\n",
    " \n",
    "    test_adversary_mask(model_hair, mult_mask)\n",
    "    \n",
    "    #wd helps\n",
    "    if (epoch>  5):\n",
    "        lr  = 0.00001\n",
    "        print(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
