{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from lenet_5 import LeNet5_5\n",
    "from lenet import LeNet5\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "#from tqdm import tqdm, trange\n",
    "import math\n",
    "import csv\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import scipy.stats as st\n",
    "\n",
    "\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.utils import check_array, check_X_y\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "BATCH_TEST_SIZE = 1024\n",
    "data_train = MNIST('../../data/mnist',\n",
    "                   download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.Resize((32, 32)),\n",
    "                       transforms.ToTensor()]))\n",
    "data_test = MNIST('../../data/mnist',\n",
    "                  train=False,\n",
    "                  download=True,\n",
    "                  transform=transforms.Compose([\n",
    "                      transforms.Resize((32, 32)),\n",
    "                      transforms.ToTensor()]))\n",
    "data_train_loader = DataLoader(data_train, batch_size = BATCH_SIZE , shuffle=True, num_workers=8)\n",
    "data_test_loader = DataLoader(data_test,  batch_size = BATCH_TEST_SIZE, num_workers=8)\n",
    "data_test_loader2 = DataLoader(data_test,  batch_size = 1, num_workers=0)\n",
    "\n",
    "TRAIN_SIZE = len(data_train_loader.dataset)\n",
    "TEST_SIZE = len(data_test_loader.dataset)\n",
    "NUM_BATCHES = len(data_train_loader)\n",
    "NUM_TEST_BATCHES = len(data_test_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model and load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_original = LeNet5()\n",
    "model_original.load_state_dict(torch.load(\"LeNet-saved\"))\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(net, criterion):\n",
    "    net.eval()\n",
    "    total_correct = 0\n",
    "    avg_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(data_test_loader2):\n",
    "    \n",
    "        output = net(images)\n",
    "        avg_loss += criterion(output, labels).sum()\n",
    "        pred = output.detach().max(1)[1]\n",
    "        total_correct += pred.eq(labels.view_as(pred)).sum()\n",
    "\n",
    "    avg_loss /= len(data_test)\n",
    "    print('Test Avg. Loss: %f, Accuracy: %f' % (avg_loss.detach().cpu().item(), float(total_correct) / len(data_test)))\n",
    "    return float(total_correct) / len(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_noisy (net, criterion):\n",
    "    net.eval()\n",
    "    total_correct = 0\n",
    "    avg_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(data_test_loader2):\n",
    "        \n",
    "        output, _ = net(images)\n",
    "        avg_loss += criterion(output, labels).sum()\n",
    "        pred = output.detach().max(1)[1]\n",
    "        total_correct += pred.eq(labels.view_as(pred)).sum()\n",
    "\n",
    "    avg_loss /= len(data_test)\n",
    "    print('Test Avg. Loss: %f, Accuracy: %f' % (avg_loss.detach().cpu().item(), float(total_correct) / len(data_test)))\n",
    "    return float(total_correct) / len(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_noisy_mask (net, criterion, mask):\n",
    "    net.eval()\n",
    "    total_correct = 0\n",
    "    avg_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(data_test_loader2):\n",
    "        \n",
    "        output, _ = net(images, mask)\n",
    "        avg_loss += criterion(output, labels).sum()\n",
    "        pred = output.detach().max(1)[1]\n",
    "        total_correct += pred.eq(labels.view_as(pred)).sum()\n",
    "\n",
    "    avg_loss /= len(data_test)\n",
    "    print('Test Avg. Loss: %f, Accuracy: %f' % (avg_loss.detach().cpu().item(), float(total_correct) / len(data_test)))\n",
    "    return float(total_correct) / len(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_above_5(net, criterion, optimizer,coef, increase = False):\n",
    "    \n",
    "    net.train()\n",
    "    avg_loss = 0\n",
    "    total_correct=0\n",
    "    for i, (images, labels) in enumerate(data_train_loader):\n",
    "        net.zero_grad()\n",
    "        labels = (labels > 5).long()\n",
    "        output = net(images)\n",
    "        if increase:\n",
    "            loss = criterion(output, labels) + coef* 1/(torch.sum(net.intermed.scales()) )\n",
    "        else:\n",
    "            loss = criterion(output, labels) #+ 1/(torch.std(net.intermed.noise) )\n",
    "        avg_loss += loss \n",
    "        pred = output.detach().max(1)[1]\n",
    "        total_correct += pred.eq(labels.view_as(pred)).sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #print(\"here\")\n",
    "    avg_loss /= len(data_train)\n",
    "    print('Train Avg. Loss: %f, Accuracy: %f' % (avg_loss.detach().cpu().item(), float(total_correct) / len(data_train)))\n",
    "    return float(float(total_correct) / len(data_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_noisy(net, criterion, optimizer,coef=1, increase = False):\n",
    "    SNR_decoded = []\n",
    "    cnt = 0\n",
    "    \n",
    "    \n",
    "    net.train()\n",
    "    avg_loss = 0\n",
    "    total_correct=0\n",
    "    for i, (images, labels) in enumerate(data_train_loader):\n",
    "        net.zero_grad()\n",
    "        \n",
    "        output,noisy = net(images)\n",
    "        \n",
    "        SNR_decoded.append((((images**2).mean())/(abs(images-noisy)).var()).item())\n",
    "        if increase:\n",
    "            loss1 =  criterion(output, labels)\n",
    "            loss2 = torch.log(torch.mean(net.intermed.scales()) )\n",
    "            \n",
    "            loss = loss1 - coef*loss2\n",
    "\n",
    "        else:\n",
    "            loss = criterion(output, labels) #+ 1/(torch.std(net.intermed.noise) )\n",
    "        \n",
    "        avg_loss += loss \n",
    "        pred = output.detach().max(1)[1]\n",
    "        total_correct += pred.eq(labels.view_as(pred)).sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        cnt =+ 1\n",
    "        del noisy\n",
    "        del output\n",
    "        \n",
    "        \n",
    "        #print(\"here\")\n",
    "    avg_loss /= len(data_train)\n",
    "    print(sum(SNR_decoded)/cnt)\n",
    "    print('Train Avg. Loss: %f, Accuracy: %f' % (avg_loss.detach().cpu().item(), float(total_correct) / len(data_train)))\n",
    "    return float(float(total_correct) / len(data_train))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_noisy_mask(net, criterion, optimizer, mask,coef=1, increase = False):\n",
    "    SNR_decoded = []\n",
    "    cnt = 0\n",
    "    \n",
    "    \n",
    "    net.train()\n",
    "    avg_loss = 0\n",
    "    total_correct=0\n",
    "    for i, (images, labels) in enumerate(data_train_loader):\n",
    "        net.zero_grad()\n",
    "        \n",
    "        output,noisy = net(images, mask)\n",
    "        \n",
    "        SNR_decoded.append((((images**2).mean())/(abs(images-noisy)).var()).item())\n",
    "        if increase:\n",
    "            loss1 =  criterion(output, labels)\n",
    "            loss2 = torch.log(torch.mean(net.intermed.scales()) )\n",
    "            \n",
    "            loss = loss1 - coef* loss2\n",
    "\n",
    "        else:\n",
    "            loss = criterion(output, labels) #+ 1/(torch.std(net.intermed.noise) )\n",
    "        \n",
    "        avg_loss += loss \n",
    "        pred = output.detach().max(1)[1]\n",
    "        total_correct += pred.eq(labels.view_as(pred)).sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        cnt =+ 1\n",
    "        del noisy\n",
    "        del output\n",
    "        \n",
    "        \n",
    "        #print(\"here\")\n",
    "    avg_loss /= len(data_train)\n",
    "    print(sum(SNR_decoded)/cnt)\n",
    "    print('Train Avg. Loss: %f, Accuracy: %f' % (avg_loss.detach().cpu().item(), float(total_correct) / len(data_train)))\n",
    "    return float(float(total_correct) / len(data_train))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_stochastic(net, criterion, times=100):\n",
    "    sum = 0\n",
    "    lis = []\n",
    "    for i in range (times):\n",
    "        acc = validate(net, criterion)\n",
    "        sum += acc\n",
    "        lis.append(acc)\n",
    "    print(\"avg is:\", sum/times)\n",
    "    return(np.array(lis).std)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_stochastic_noisy(net, criterion, times=100):\n",
    "    sum = 0\n",
    "    lis = []\n",
    "    for i in range (times):\n",
    "        acc = validate_noisy(net, criterion)\n",
    "        sum += acc\n",
    "        lis.append(acc)\n",
    "    print(\"avg is:\", sum/times)\n",
    "    return(np.array(lis).std)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_stochastic_noisy_mask(net, criterion, mask, times=100):\n",
    "    sum = 0\n",
    "    lis = []\n",
    "    for i in range (times):\n",
    "        acc = validate_noisy_mask(net, criterion, mask)\n",
    "        sum += acc\n",
    "        lis.append(acc)\n",
    "    print(\"avg is:\", sum/times)\n",
    "    return(np.array(lis).std)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Avg. Loss: 0.034585, Accuracy: 0.990500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9905"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#validate (model_original, criterion)\n",
    "validate (model_original, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get conv shapes and layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (c1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (relu1): ReLU()\n",
      "  (s2): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (c3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (relu3): ReLU()\n",
      "  (s4): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (c5): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (relu5): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_original.convnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_indices(net):\n",
    "    conv_layers = []\n",
    "    fc_layers=[]\n",
    "    for i, layer in enumerate(net.convnet):\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            if ((i is not 0 )):\n",
    "                conv_layers.append(i)\n",
    "    conv_layers.append(len(net.convnet))\n",
    "    for i, layer in enumerate(net.fc):\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            fc_layers.append(i)\n",
    "\n",
    "    #conv_layers.append(-2)\n",
    "    print (conv_layers, fc_layers)\n",
    "    \n",
    "    return conv_layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_shapes(net):\n",
    "    conv_shapes=[]\n",
    "    for cnt2, (data, target) in enumerate(data_test_loader):\n",
    "        for cnt,i in enumerate(conv_layers):\n",
    "            #newmodel = torch.nn.Sequential(*(list(model_test.features)[0:i]))\n",
    "            newmodel_original =  torch.nn.Sequential(*(list(net.convnet)[0:i]))\n",
    "\n",
    "            output_original = newmodel_original(data)\n",
    "            conv_shapes.append(output_original.shape[1:])\n",
    "            print (output_original.shape[1:])\n",
    "        if (cnt2==0):\n",
    "            conv_shapes.append(data.shape[1:])\n",
    "            print(data.shape[1:])\n",
    "            break\n",
    "\n",
    "    return conv_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 6, 8] [0, 2]\n",
      "torch.Size([6, 14, 14])\n",
      "torch.Size([16, 5, 5])\n",
      "torch.Size([120, 1, 1])\n",
      "torch.Size([1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "conv_layers = get_conv_indices(model_original)\n",
    "conv_shapes = get_conv_shapes(model_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noisy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NoisyActivation(nn.Module):\n",
    "    def __init__(self,  given_locs, given_scales, min_scale, max_scale):\n",
    "        super(NoisyActivation, self).__init__()\n",
    "        size = given_scales.shape\n",
    "        self.min_scale = min_scale\n",
    "        self.max_scale = max_scale\n",
    "        self.given_locs = given_locs \n",
    "        self.given_scales = given_scales\n",
    "        self.locs = nn.Parameter(torch.Tensor(size).copy_(self.given_locs))         \n",
    "        self.rhos = nn.Parameter(torch.ones(size)-10) #-inf\n",
    "\n",
    "        #self.noise = nn.Parameter(torch.Tensor(size).normal_(mean=prior_mus, std=prior_sigmas))\n",
    "        self.normal = torch.distributions.normal.Normal(0,1)\n",
    "\n",
    "        \n",
    "    def scales(self):\n",
    "        return (1.0 +torch.tanh(self.rhos))/2*(self.max_scale-self.min_scale) +self.min_scale             \n",
    "    \n",
    "    def sample_noise(self, mask):\n",
    "        epsilon = self.normal.sample(self.rhos.shape)*mask\n",
    "        return self.locs + self.scales() * epsilon\n",
    "                                 \n",
    "                            \n",
    "                            \n",
    "    def forward(self, input, mask):\n",
    "        noise = self.sample_noise(mask)\n",
    "        return (input)*mask + noise\n",
    "\n",
    "\n",
    "# In[61]:\n",
    "\n",
    "\n",
    "class LeNet_syn(nn.Module):\n",
    "\n",
    "    def __init__(self, model_features, model_classifier,index, min_scale, max_scale, given_locs, given_scales):\n",
    "        super(LeNet_syn, self).__init__()\n",
    "        \n",
    "                                \n",
    "        self.model_pt1 =  torch.nn.Sequential(*(list(model_features)[0:conv_layers[index]]))\n",
    "        self.intermed = NoisyActivation(given_locs, given_scales, min_scale, max_scale)\n",
    "        self.model_pt2 =  torch.nn.Sequential(*(list(model_features)[conv_layers[index]:]))\n",
    "        self.model_pt3 = model_classifier\n",
    "\n",
    "        for child in itertools.chain(self.model_pt1, self.model_pt2, self.model_pt3): #self.model_pt2 #(self.model_pt2, \n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "            if isinstance(child, nn.modules.batchnorm._BatchNorm):\n",
    "                child.eval()\n",
    "                child.affine = False\n",
    "                child.track_running_stats = False\n",
    "                \n",
    "        #self.intermed.rhos.reuires_grad = True\n",
    "        #self.intermed.locs.reuires_grad = True\n",
    "                                 \n",
    "    def forward(self, img, mask):\n",
    "                                 \n",
    "        x = self.model_pt1(img)\n",
    "        x = self.intermed(x, mask)\n",
    "        noisy = x.detach()\n",
    "\n",
    "        x = self.model_pt2(x)                    \n",
    "        x = x.view(img.size(0), -1)\n",
    "        x = self.model_pt3(x)                                 \n",
    "\n",
    "        return x, noisy\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_original = LeNet5()\n",
    "model_original.load_state_dict(torch.load(\"LeNet-saved\"))\n",
    "criterion = nn.NLLLoss()\n",
    "mus = torch.zeros((1,120,1,1))\n",
    "scale = torch.ones((1,120,1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_syn = LeNet_syn(model_original.convnet, model_original.fc,2 ,0.0001, 3 ,mus, scale )\n",
    "model_syn.load_state_dict(torch.load(\"lenet-last-layer-cloak\",  map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_syn.intermed.rhos.requires_grad = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(list(filter(lambda p: p.requires_grad, model_syn.parameters()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model_syn.parameters()), lr=0.0001, weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mult_mask = torch.ones(model_syn.intermed.rhos.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(5): #45 0.01 10 -finess 0.1 coef lr 0.001/ 15 lr 0.01 coef 1 / 20 coef1 lr 0.01/\n",
    "    print(torch.mean(model_syn.intermed.scales()))\n",
    "    print(torch.max(model_syn.intermed.scales()))\n",
    "    print(torch.min(model_syn.intermed.scales()))\n",
    "    train_noisy_mask(model_syn, criterion, optimizer,mult_mask, 1,True)\n",
    "    print(\"*******************************************************\")\n",
    "    #print(list(filter(lambda p: p.requires_grad, model_syn.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Avg. Loss: 0.181763, Accuracy: 0.950400\n",
      "Test Avg. Loss: 0.184854, Accuracy: 0.950000\n",
      "Test Avg. Loss: 0.179758, Accuracy: 0.950400\n",
      "Test Avg. Loss: 0.186991, Accuracy: 0.949800\n",
      "Test Avg. Loss: 0.176966, Accuracy: 0.949200\n",
      "Test Avg. Loss: 0.174475, Accuracy: 0.951900\n",
      "Test Avg. Loss: 0.183070, Accuracy: 0.947800\n",
      "Test Avg. Loss: 0.192468, Accuracy: 0.947800\n",
      "Test Avg. Loss: 0.184152, Accuracy: 0.953500\n",
      "Test Avg. Loss: 0.186166, Accuracy: 0.948300\n",
      "Test Avg. Loss: 0.165158, Accuracy: 0.954200\n",
      "Test Avg. Loss: 0.190570, Accuracy: 0.950800\n",
      "Test Avg. Loss: 0.178145, Accuracy: 0.949800\n",
      "Test Avg. Loss: 0.182152, Accuracy: 0.948300\n",
      "Test Avg. Loss: 0.173242, Accuracy: 0.951600\n",
      "Test Avg. Loss: 0.188251, Accuracy: 0.947900\n",
      "Test Avg. Loss: 0.175125, Accuracy: 0.947900\n",
      "Test Avg. Loss: 0.185535, Accuracy: 0.949500\n",
      "Test Avg. Loss: 0.182943, Accuracy: 0.951200\n",
      "Test Avg. Loss: 0.185052, Accuracy: 0.950700\n",
      "avg is: 0.9500500000000001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function ndarray.std>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_stochastic_noisy_mask(model_syn, criterion,mult_mask, 20) #40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save for MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (images, labels) in enumerate(data_test_loader2):\n",
    "    x= images\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Avg. Loss: 2.000000, Accuracy: 0.948600\n",
      "0.9486\n"
     ]
    }
   ],
   "source": [
    "imgs =np.reshape(np.squeeze(images.detach().numpy()), (1,-1))\n",
    "lbls = np.reshape(np.squeeze(labels.detach().numpy()), (1,-1))\n",
    "encoded_noise = model_syn.intermed.rhos.detach().numpy()\n",
    "encoded_original = model_syn.intermed.rhos.detach().numpy()\n",
    "image_noisy =np.reshape(np.squeeze(model_syn.intermed.rhos.detach().numpy()), (1,-1))\n",
    "total_correct = 0\n",
    "\n",
    "model_syn.eval()\n",
    "for i, (images, labels) in enumerate(data_test_loader2):\n",
    "    \n",
    "    #labels = (labels > 5).long()\n",
    "    imgs=np.concatenate((imgs,np.reshape(np.squeeze(images.detach().numpy()), (1,-1)) ))\n",
    "    np.save(\"original-image-mutual_info-2class-input-noise-last-layer-cloak\", imgs)\n",
    "    lbls=np.concatenate((lbls,np.reshape(np.squeeze(labels.detach().numpy()), (1,-1)) ))\n",
    "    np.save(\"original-labels-mutual_info-2class-input-noise-last-layer-cloak\", lbls)\n",
    "    \n",
    "    \n",
    "    x= model_syn.model_pt1(images)\n",
    "    x = model_syn.intermed(x,mult_mask)\n",
    "   \n",
    "    image_noisy=np.concatenate((image_noisy,np.reshape(np.squeeze(x.detach().numpy()), (1,-1)) ))\n",
    "    np.save(\"noisy-image-mutual_info-2class-input-noise-last-layer-cloak\", image_noisy)\n",
    "\n",
    "    #print(imgs.shape, lbls.shape, encoded_original.shape, encoded_noise.shape, image_noisy.shape)\n",
    "    \n",
    "    x = model_syn.model_pt2(x)                    \n",
    "    x = x.view(images.size(0), -1)\n",
    "    output = model_syn.model_pt3(x)                                 \n",
    "\n",
    "\n",
    "    \n",
    "    #print(imgs.shape, image_noisy.shape)\n",
    "\n",
    "    pred = output.detach().max(1)[1]\n",
    "    total_correct += pred.eq(labels.view_as(pred)).sum()\n",
    "\n",
    "\n",
    "print('Test Avg. Loss: %f, Accuracy: %f' % (2, float(total_correct) / len(data_test)))\n",
    "print (float(total_correct) / len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
